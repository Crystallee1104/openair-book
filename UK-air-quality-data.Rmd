# Accessing UK Air Quality Data

The UK has a surprisingly large amount of air quality data that is publicly accessible. The main UK AURN archive and regional (England, Scotland, Wales and Northern Ireland) together with
Imperial College London's London Air Quality Network (LAQN) are 
important and large databases of information that allow free public
access. Storing and managing data in this way has many advantages
including consistent data format, and underlying high quality methods
to process and store the data.

## Acessing data

**openair** has a family of functions that provide users with extensive
access to UK air quality data. Ricardo Energy \& Environment have
provided .RData files (R workspaces) for several important air quality
networks in the UK. These files are updated on a daily basis. This
approach requires a link to the Internet to work. The networks include:

-  `importAURN` For importing data from the UK national
  network called
  [Automatic Urban and Rural Network}](https://uk-air.defra.gov.uk/networks/network-info?view=aurn). This is the main UK network.
  
- `importSAQN` For accessing data from
  [Air Quality Scotland](http://www.scottishairquality.scot/)
  network.
  
-  `importWAQN` For accessing data from the
  [Air Quality Wales](https://airquality.gov.wales/) network.
  
-  `importAQE` For accessing data from the
  [Air Quality England](https://www.airqualityengland.co.uk/)
  network of sites.

-  `importNI` For accessing data from the
  [Northern Ireland](https://www.airqualityni.co.uk)
  network of sites.

- `importEurope` A simplified version of a function to give
  basic access to hourly European data based on Stuart Grange's
  \cd{saqgetr} package --- see
  https://github.com/skgrange/saqgetr. The **openair** function has
  a similar approach to other **openair** import functions i.e. requires
  a site code(s) and year(s) to be supplied.
  
-  `importKCL` For accessing data from the sites operated by
  King's College London, primarily including the
  [The London Air Quality Network](https://www.londonair.org.uk/LondonAir/Default.aspx).


Many users download hourly data from the air quality archive at
http://www.airquality.co.uk. Most commonly, the data are emailed
to the user as .csv files and have a fixed format as shown below. This
is a useful facility but does have some limitations and frustrations,
many of which have been overcome using a new way of storing and
downloading the data described below.

There are several advantages over the web portal approach where .csv
files are downloaded. First, it is quick to select a range of sites,
pollutants and periods (see examples below). Second, storing the data
as .RData objects is very efficient as they are about four times
smaller than .csv files (which are already small) --- which means the
data downloads quickly and saves bandwidth. Third, the function
completely avoids any need for data manipulation or setting time
formats, time zones etc. Finally, it is easy to import many years of
data. The final point makes it possible to download several long time
series in one go.

The site codes and pollutant names can be upper or lower case. The
function will issue a warning when data less than six months old is
downloaded, which may not be ratified. Type ?importAURN for a full
listing of sites and their codes.

Some examples of usage are shown below. First load the packages we need.

```{r warning=FALSE,message=FALSE}
library(openair)
library(tidyverse)
```


```{r eval=FALSE}
## import all pollutants from Marylebone Rd from 1990:2009
mary <- importAURN(site = "my1", year = 2000:2009)

## import nox, no2, o3 from Marylebone Road and Nottingham Centre for 2000
thedata <- importAURN(site = c("my1", "nott"), year = 2000,
                      pollutant = c("nox", "no2", "o3"))

## import over 20 years of Mace Head O3 data!
o3 <- importAURN(site = "mh", year = 1987:2009)
## import hydrocarbon data from Marylebone Road
hc <- importAURN(site = "my1", year = 2008, hc = TRUE)
```

By default, the function returns data where each pollutant is in a
separate column. However, it is possible to return the data in a
*tidy* format (column for pollutant name, column for value) by
using the option `to_narrow`:

```{r eval=FALSE}
my1 <- importAURN("my1", year = 2018, to_narrow = TRUE)
``` 


## Site Meta Data

Users can access the details of air pollution monitoring sites using
the `importMeta` function. The user only needs to provide the
network name and (optionally) whether all data should be returned. By
default only site type, latitude and longitude are returned.

```{r importMeta,warning=FALSE,message=FALSE}
library(tidyverse) 
aurn_meta <- importMeta(source = "aurn")
aurn_meta
```

Or return much more detailed data:

```{r importMetaAll,eval=FALSE}
aurn_meta <- importMeta(source = "aurn", all = TRUE) 
aurn_meta
```

And to include basic meta data when importing air pollution data:

```{r importWithMeta}
kc1 <- importAURN(site = "kc1", year = 2018, meta = TRUE)

glimpse(kc1)
``` 

## Plot Sites on a Map

The example below uses sites on the AURN that measure NO~2~, but can easily be extended to the other data sources.

```{r warning=FALSE,message=FALSE}
aurn_detailed <- importMeta(source = "aurn", all = TRUE)

no2_sites <- filter(
  aurn_detailed,
  variable == "NO2",
  site_type == "Urban Traffic"
)

nrow(no2_sites)
```

In the example below the unique sites are selected from
`aurn_detailed` because the `site` repeats the number of pollutants
that are measured. Information is also collected for the map popups
and then the map is plotted.

```{r}
library(leaflet)

aurn_unique <- distinct(aurn_detailed, site, .keep_all = TRUE)

# information for map markers
content <- paste(
  paste(
    aurn_unique$site,
    paste("Code:", aurn_unique$code),
    paste("Start:", aurn_unique$start_date),
    paste("End:", aurn_unique$end_date),
    paste("Site Type:", aurn_unique$site_type),
    sep = "<br/>"
  )
)


# plot map
leaflet(aurn_unique) %>%
  addTiles() %>%
  addMarkers(~ longitude, ~ latitude, popup = content,
             clusterOptions = markerClusterOptions())
```

