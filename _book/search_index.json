[
["index.html", "openair Section1 Prerequisites", " openair David Carslaw 2020-07-21 Section1 Prerequisites To be written "],
["intro.html", "Section2 Introduction", " Section2 Introduction To be written "],
["sec-importAURN.html", "Section3 Accessing UK Air Quality Data 3.1 Accessing data 3.2 Site Meta Data 3.3 Plot Sites on a Map", " Section3 Accessing UK Air Quality Data The UK has a surprisingly large amount of air quality data that is publicly accessible. The main UK AURN archive and regional (England, Scotland, Wales and Northern Ireland) together with Imperial College London’s London Air Quality Network (LAQN) are important and large databases of information that allow free public access. Storing and managing data in this way has many advantages including consistent data format, and underlying high quality methods to process and store the data. 3.1 Accessing data openair has a family of functions that provide users with extensive access to UK air quality data. Ricardo Energy &amp; Environment have provided .RData files (R workspaces) for several important air quality networks in the UK. These files are updated on a daily basis. This approach requires a link to the Internet to work. The networks include: importAURN For importing data from the UK national network called Automatic Urban and Rural Network}. This is the main UK network. importSAQN For accessing data from Air Quality Scotland network. importWAQN For accessing data from the Air Quality Wales network. importAQE For accessing data from the Air Quality England network of sites. importNI For accessing data from the Northern Ireland network of sites. importEurope A simplified version of a function to give basic access to hourly European data based on Stuart Grange’s package — see https://github.com/skgrange/saqgetr. The openair function has a similar approach to other openair import functions i.e. requires a site code(s) and year(s) to be supplied. importKCL For accessing data from the sites operated by King’s College London, primarily including the The London Air Quality Network. Many users download hourly data from the air quality archive at http://www.airquality.co.uk. Most commonly, the data are emailed to the user as .csv files and have a fixed format as shown below. This is a useful facility but does have some limitations and frustrations, many of which have been overcome using a new way of storing and downloading the data described below. There are several advantages over the web portal approach where .csv files are downloaded. First, it is quick to select a range of sites, pollutants and periods (see examples below). Second, storing the data as .RData objects is very efficient as they are about four times smaller than .csv files (which are already small) — which means the data downloads quickly and saves bandwidth. Third, the function completely avoids any need for data manipulation or setting time formats, time zones etc. Finally, it is easy to import many years of data. The final point makes it possible to download several long time series in one go. The site codes and pollutant names can be upper or lower case. The function will issue a warning when data less than six months old is downloaded, which may not be ratified. Type ?importAURN for a full listing of sites and their codes. Some examples of usage are shown below. First load the packages we need. library(openair) library(tidyverse) ## import all pollutants from Marylebone Rd from 1990:2009 mary &lt;- importAURN(site = &quot;my1&quot;, year = 2000:2009) ## import nox, no2, o3 from Marylebone Road and Nottingham Centre for 2000 thedata &lt;- importAURN(site = c(&quot;my1&quot;, &quot;nott&quot;), year = 2000, pollutant = c(&quot;nox&quot;, &quot;no2&quot;, &quot;o3&quot;)) ## import over 20 years of Mace Head O3 data! o3 &lt;- importAURN(site = &quot;mh&quot;, year = 1987:2009) ## import hydrocarbon data from Marylebone Road hc &lt;- importAURN(site = &quot;my1&quot;, year = 2008, hc = TRUE) By default, the function returns data where each pollutant is in a separate column. However, it is possible to return the data in a tidy format (column for pollutant name, column for value) by using the option to_narrow: my1 &lt;- importAURN(&quot;my1&quot;, year = 2018, to_narrow = TRUE) 3.2 Site Meta Data Users can access the details of air pollution monitoring sites using the importMeta function. The user only needs to provide the network name and (optionally) whether all data should be returned. By default only site type, latitude and longitude are returned. library(tidyverse) aurn_meta &lt;- importMeta(source = &quot;aurn&quot;) aurn_meta ## # A tibble: 273 x 5 ## site code latitude longitude site_type ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Aberdeen ABD 57.2 -2.09 Urban Background ## 2 Aberdeen Union Street Roadside ABD7 57.1 -2.11 Urban Traffic ## 3 Aberdeen Wellington Road ABD8 57.1 -2.09 Urban Traffic ## 4 Armagh Roadside ARM6 54.4 -6.65 Urban Traffic ## 5 Aston Hill AH 52.5 -3.03 Rural Background ## 6 Auchencorth Moss ACTH 55.8 -3.24 Rural Background ## 7 Ballymena Antrim Road BAAR 54.9 -6.27 Urban Traffic ## 8 Ballymena Ballykeel BALM 54.9 -6.25 Urban Background ## 9 Barnsley BARN 53.6 -1.48 Urban Background ## 10 Barnsley 12 BAR2 53.6 -1.49 Urban Background ## # … with 263 more rows Or return much more detailed data: aurn_meta &lt;- importMeta(source = &quot;aurn&quot;, all = TRUE) aurn_meta And to include basic meta data when importing air pollution data: kc1 &lt;- importAURN(site = &quot;kc1&quot;, year = 2018, meta = TRUE) glimpse(kc1) ## Rows: 8,760 ## Columns: 17 ## $ site &lt;chr&gt; &quot;London N. Kensington&quot;, &quot;London N. Kensington&quot;, &quot;London N. … ## $ code &lt;chr&gt; &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC1&quot;, &quot;KC… ## $ date &lt;dttm&gt; 2018-01-01 00:00:00, 2018-01-01 01:00:00, 2018-01-01 02:00… ## $ co &lt;dbl&gt; 0.114872, 0.111043, 0.112000, 0.100512, 0.091897, 0.100512,… ## $ nox &lt;dbl&gt; 8.32519, 8.89934, 9.41967, 9.36584, 7.21277, 7.64339, 10.17… ## $ no2 &lt;dbl&gt; 8.11153, 8.54325, 8.99235, 8.93852, 6.94570, 7.26948, 10.01… ## $ no &lt;dbl&gt; 0.13935, 0.23224, 0.27869, 0.27869, 0.17418, 0.24386, 0.104… ## $ o3 &lt;dbl&gt; 70.98040, 67.52118, 69.69982, 70.49810, 71.74542, 70.49810,… ## $ so2 &lt;dbl&gt; NA, 2.40953, 2.49812, 2.12606, 2.39181, 2.28551, 2.23236, 2… ## $ pm10 &lt;dbl&gt; 12.425, 7.375, 5.625, 3.200, 3.875, 5.050, 9.400, 12.400, 1… ## $ pm2.5 &lt;dbl&gt; 8.892, 4.363, 3.137, 1.792, 2.146, 2.618, 4.575, 6.109, 7.0… ## $ ws &lt;dbl&gt; 5.5, 5.0, 4.8, 4.8, 5.3, 5.3, 4.4, 3.0, 2.6, 1.6, 1.6, 1.1,… ## $ wd &lt;dbl&gt; 263.3, 256.4, 251.0, 246.8, 248.4, 248.0, 245.8, 239.5, 232… ## $ air_temp &lt;dbl&gt; 5.5, 5.1, 4.9, 4.7, 4.9, 5.0, 5.0, 4.6, 4.2, 3.7, 5.4, 5.7,… ## $ latitude &lt;dbl&gt; 51.52105, 51.52105, 51.52105, 51.52105, 51.52105, 51.52105,… ## $ longitude &lt;dbl&gt; -0.213492, -0.213492, -0.213492, -0.213492, -0.213492, -0.2… ## $ site_type &lt;chr&gt; &quot;Urban Background&quot;, &quot;Urban Background&quot;, &quot;Urban Background&quot;,… 3.3 Plot Sites on a Map The example below uses sites on the AURN that measure NO2, but can easily be extended to the other data sources. aurn_detailed &lt;- importMeta(source = &quot;aurn&quot;, all = TRUE) no2_sites &lt;- filter( aurn_detailed, variable == &quot;NO2&quot;, site_type == &quot;Urban Traffic&quot; ) nrow(no2_sites) ## [1] 89 In the example below the unique sites are selected from aurn_detailed because the site repeats the number of pollutants that are measured. Information is also collected for the map popups and then the map is plotted. library(leaflet) aurn_unique &lt;- distinct(aurn_detailed, site, .keep_all = TRUE) # information for map markers content &lt;- paste( paste( aurn_unique$site, paste(&quot;Code:&quot;, aurn_unique$code), paste(&quot;Start:&quot;, aurn_unique$start_date), paste(&quot;End:&quot;, aurn_unique$end_date), paste(&quot;Site Type:&quot;, aurn_unique$site_type), sep = &quot;&lt;br/&gt;&quot; ) ) # plot map leaflet(aurn_unique) %&gt;% addTiles() %&gt;% addMarkers(~ longitude, ~ latitude, popup = content, clusterOptions = markerClusterOptions()) "],
["sec-worldmet.html", "Section4 Access meteorological data 4.1 The worldmet package", " Section4 Access meteorological data 4.1 The worldmet package Most of the import functions described in Section 3 return basic modelled hourly meteorological data (wind speed, wind direction and surface temperature). These data are derived from the WRF model that Ricardo runs to provide the data. Alternatively it may be advantageous to used surface measurements. worldmet provides an easy way in which to access surface meteorological data from &gt;30,000 sites across the world (Carslaw 2020). The package accesses the NOAA webservers to download hourly data. Access to surface meteorological data is very useful in general but is especially useful when using openair and the polarPlot function. To install the package, type: install.packages(&quot;worldmet&quot;) There are two main functions in the package: getMeta and importNOAA. The former helps the user find meteorological sites by name, country and proximity to a location based on the latitude and longitude. getMeta will also return a code that can be supplied to importNOAA, which then imports the data. Probably the most common use of getMeta is to search around a location of interest based on its latitude and longitude. First we will load the worldmet package: library(worldmet) Now we can do a search for the 10 nearest sites to Dublin (latitude = 53.3, longitude = -6.3)1: getMeta(lat = 53.3, lon = -6.3, returnMap = TRUE) Figure 4.1: Map of returned area of interest. The user can interactively select a site of interest and find its code to import data. Note that it is just as easy to access all the site information at once because it is quick to use the map to select the site and its code i.e. getMeta() We can use the map that is produced to select a site of interest and import the data. For example, to import data for Dublin Airport and look at some of the data: dublin_met &lt;- importNOAA(code = &quot;039690-99999&quot;, year = 2019) # first few lines of data dublin_met ## # A tibble: 8,760 x 24 ## code station date latitude longitude elev ws wd ## &lt;fct&gt; &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0396… DUBLIN… 2019-01-01 00:00:00 53.4 -6.27 73.8 5.07 250. ## 2 0396… DUBLIN… 2019-01-01 01:00:00 53.4 -6.27 73.8 4.73 247. ## 3 0396… DUBLIN… 2019-01-01 02:00:00 53.4 -6.27 73.8 4.07 250. ## 4 0396… DUBLIN… 2019-01-01 03:00:00 53.4 -6.27 73.8 4.40 250. ## 5 0396… DUBLIN… 2019-01-01 04:00:00 53.4 -6.27 73.8 5.47 257. ## 6 0396… DUBLIN… 2019-01-01 05:00:00 53.4 -6.27 73.8 4.90 260 ## 7 0396… DUBLIN… 2019-01-01 06:00:00 53.4 -6.27 73.8 4.90 260 ## 8 0396… DUBLIN… 2019-01-01 07:00:00 53.4 -6.27 73.8 4.40 254. ## 9 0396… DUBLIN… 2019-01-01 08:00:00 53.4 -6.27 73.8 5.27 270 ## 10 0396… DUBLIN… 2019-01-01 09:00:00 53.4 -6.27 73.8 4.73 263. ## # … with 8,750 more rows, and 16 more variables: air_temp &lt;dbl&gt;, ## # atmos_pres &lt;dbl&gt;, visibility &lt;dbl&gt;, dew_point &lt;dbl&gt;, RH &lt;dbl&gt;, ## # ceil_hgt &lt;dbl&gt;, cl_1 &lt;dbl&gt;, cl_2 &lt;dbl&gt;, cl_3 &lt;dbl&gt;, cl &lt;dbl&gt;, ## # cl_1_height &lt;dbl&gt;, cl_2_height &lt;dbl&gt;, cl_3_height &lt;dbl&gt;, precip_12 &lt;dbl&gt;, ## # precip_6 &lt;dbl&gt;, precip &lt;dbl&gt; Plot a wind rose. windRose(dublin_met) References "],
["sec-windRose.html", "Section5 Wind and Pollution Roses 5.1 Example of use 5.2 Comparing two meteorological data sets", " Section5 Wind and Pollution Roses The wind rose is a very useful way of summarising meteorological data. It is particularly useful for showing how wind speed and wind direction conditions vary by year. The windRose function can plot wind roses in a variety of ways: summarising all available wind speed and wind direction data, plotting individual wind roses by year, and also by month. The latter is useful for considering how meteorological conditions vary by season. Data are summarised by direction, typically by 45 or 30\\(^\\circ\\) and by different wind speed categories. Typically, wind speeds are represented by different width ‘paddles’. The plots show the proportion (here represented as a percentage) of time that the wind is from a certain angle and wind speed range. The windRose function also calculates the percentage of ‘calms’ i.e. when the wind speed is zero. UK Met Office data assigns these periods to 0 degrees wind direction with valid northerly winds being assigned to 360\\(^\\circ\\). The windRose function will also correct for bias when wind directions are rounded to the nearest 10 degrees but are displayed at angles that 10 degrees is not exactly divisible into e.g. 22.5\\(^\\circ\\). When such data are binned, some angles i.e. N, E, S, W will comprise three intervals whereas others will comprise two, which can lead to significant bias. This issue and its solution is discussed by Droppo and Napier (2008) and Applequist (2012).2 openair uses a simple method to correct for the bias by globally rescaling the count in each wind direction bin by the number of directions it represents relative to the average. Thus, the primary four directions are each reduced by a factor of 0.75 and the remaining 12 directions are multiplied by 1.125. 5.1 Example of use First we load the packages: library(openair) library(tidyverse) The function is very simply called as shown for Figure 5.1. windRose(mydata) Figure 5.1: Use of windRose function to plot wind speed/direction frequencies. Wind speeds are split into the intervals shown by the scale in each panel. The grey circles show the % frequencies. Figure 5.2 highlights some interesting differences between the years. In 2000, for example, there were numerous occasions when the wind was from the SSW and 2003 clearly had more occasions when the wind was easterly. It can also be useful to use type = \"month\" to get an idea of how wind speed and direction vary seasonally. windRose(mydata, type = &quot;year&quot;, layout = c(4, 2)) Figure 5.2: Use of windRose function to plot wind speed/direction frequencies by year. Wind speeds are split into the intervals shown by the scale in each panel. The grey circles show the 10 and 20% frequencies. The type option is very flexible in openair and can be used to quickly consider the dependencies between variables. Section 19.2 describes the basis of this option in openair plot. As an example, consider the question: what are the meteorological conditions that control high and low concentrations of PM10? By setting type = \"pm10\", openair will split the PM10 concentrations into four quantiles i.e. roughly equal numbers of points in each level. The plot will then show four different wind roses for each quantile level, although the default number of levels can be set by the user — see ?cutData for more details. Figure 5.3 shows the results of setting type = \"pm10\". For the lowest concentrations of PM10 the wind direction is dominated by northerly winds, and relatively low wind speeds. By contrast, the highest concentrations (plot furthest right) are dominated by relatively strong winds from the south-west. It is therefore very easy to obtain a good idea about the conditions that tend to lead to high (or low) concentrations of a pollutant. Furthermore, the type option is available in almost all openair functions. windRose(mydata, type = &quot;pm10&quot;, layout = c(4, 1)) Figure 5.3: Wind rose for four different levels of PM10 concentration. The levels are defined as the four quantiles of PM10 concentration and the ranges are shown on each of the plot labels. A comparison of the effect that bias has can be seen by plotting the following. Note the prominent frequencies for W, E and N in particular that are due to the bias issue discussed by Applequist (2012). ## no bias correction windRose(mydata, angle = 22.5, bias.corr = FALSE) ## bias correction (the default) windRose(mydata, angle = 22.5) pollutionRose is a variant of windRose that is useful for considering pollutant concentrations by wind direction, or more specifically the percentage time the concentration is in a particular range. This type of approach can be very informative for air pollutant species, as demonstrated by Ronald Henry and co-authors in Henry et al. (2009). You can produce similar pollution roses using the pollutionRose function in recent versions of openair, e.g. as in Figure 5.4: pollutionRose(mydata, pollutant = &quot;nox&quot;) Figure 5.4: NOx pollution rose produced using pollutionRose and default pollutionRose settings. pollutionRose is wrapper for windRose. It simply replaces the wind speed data series in the supplied data set with another variable using the argument pollutant before passing that on to windRose. It also modifies breaks to estimate a sensible set of break points for that pollutant and uses a slightly different set of default options (key to right, wedge style plot) but otherwise handles arguments just like the parent windRose function. While Figure 5.4 indicates that higher NOx concentrations are also associated with the SW, conditioning allows you to be much informative. For example, conditioning by SO2 (Figure 5.5 demonstrates that higher NOx concentrations are associated with the SW and much of the higher SO2 concentrations. However, it also highlights a notable NOx contribution from the E, most apparent at highest SO2 concentrations that is obscured in Figure 5.4 by a relatively high NOx background (Figure 5.5. pollutionRose(mydata, pollutant = &quot;nox&quot;, type = &quot;so2&quot;, layout = c(4, 1)) Figure 5.5: NOx pollution rose conditioned by SO2 concentration. pollutionRose can also usefully be used to show which wind directions dominate the overall concentrations. By supplying the option statistic = \"prop.mean\" (proportion contribution to the mean), a good idea can be gained as to which wind directions contribute most to overall concentrations, as well as providing information on the different concentration levels. A simple plot is shown in Figure 5.6, which clearly shows the dominance of south-westerly winds controlling the overall mean NOx concentrations at this site. Indeed, almost half the overall NOx concentration is contributed by two wind sectors to the south-west. The polarFreq function can also show this sort of information, but the pollution rose is more effective because both length and colour are used to show the contribution. These plots are very useful for understanding which wind directions control the overall mean concentrations. pollutionRose(mydata, pollutant = &quot;nox&quot;, statistic = &quot;prop.mean&quot;) Figure 5.6: Pollution rose showing which wind directions contribute most to overall mean concentrations. It is sometimes useful to more clearly understand the contributions from wind directions that have low frequencies. For example, for a pollution rose of SO2 there are few occurrences of easterly winds making it difficult to see how the concentration intervals are made up. Try: pollutionRose(mydata, pollutant = &quot;so2&quot;, seg = 1) However, each wind sector can be normalised to give a probability between 0 and 1 to help show the variation within each wind sector more clearly. An example is shown in Figure 5.7 where for easterly winds it is now clearer that a greater proportion of the time the concentration is made up of high SO2 concentrations. In this plot each wind sector is scaled between 0 and 1. Also shown with a black like is an indication of the wind direction frequency to remind us that winds from the east occur with a low frequency. pollutionRose(mydata, pollutant = &quot;so2&quot;, normalise = TRUE, seg = 1) Figure 5.7: SO2 pollution rose produced using pollutionRose normalised by each wind sector. 5.2 Comparing two meteorological data sets The pollutionRose function is also useful for comparing two meteorological data sets. In this case a ‘reference’ data set is compared with a second data set. There are many reasons for doing so e.g. to see how one site compares with another or for meteorological model evaluation (more on that in later sections). In this case, ws and wd are considered to the the reference data sets with which a second set of wind speed and wind directions are to be compared (ws2 and wd2). The first set of values is subtracted from the second and the differences compared. If for example, wd2 was biased positive compared with wd then pollutionRose will show the bias in polar coordinates. In its default use, wind direction bias is colour-coded to show negative bias in one colour and positive bias in another. Note that this plot is mostly aimed at showing wind direction biases. It does also show the wind speed bias but only if there is a wind direction bias also. However, in most practical situations the plot should show both wind speed and direction biases together. An example of a situation where no wind speed bias would be shown would be for westerly winds where there was absolutely no bias between two data sets in terms of westerly wind direction but there was a difference in wind speed. Users should be aware of this limitation. In the next example, some artificial wind direction data are generated by adding a positive bias of 30~degrees with some normally distributed scatter. Also, the wind speed data are given a positive bias. The results are shown in Figure 5.8. The Figure clearly shows the mean positive bias in wind direction i.e. the direction is displaced from north (no bias). The colour scale also shows the extent to which wind speeds are biased i.e. there is a higher proportion of positively biased wind speeds shown by the red colour compared with the negatively biased shown in blue. Also shown in Figure 5.8 is the mean wind speed and direction bias as numerical values. Note that the type option can be used in Figure 5.8 e.g. type = \"month\" to split the analysis in useful ways. This is useful if one wanted to see whether a site or the output from a model was biased for different periods. For example, type = \"daylight\" would show whether there are biases between nighttime and daytime conditions. ## $example of comparing 2 met sites ## first we will make some new ws/wd data with a postive bias mydata &lt;- mutate(mydata, ws2 = ws + 2 * rnorm(nrow(mydata)) + 1, wd2 = wd + 30 * rnorm(nrow(mydata)) + 30) ## need to correct negative wd id &lt;- which(mydata$wd2 &lt; 0) mydata$wd2[id] &lt;- mydata$wd2[id] + 360 ## results show postive bias in wd and ws pollutionRose(mydata, ws = &quot;ws&quot;, wd = &quot;wd&quot;, ws2 = &quot;ws2&quot;, wd2 = &quot;wd2&quot;, grid.line = 5) Figure 5.8: Pollution rose showing the difference between two meteorological data sets. The colours are used to show whether data tend to be positively or negatively biased with respect to the reference data set. An example of using user-supplied breaks is shown in Figure 5.9. In this case six intervals are chosen including one that spans -0.5 to +0.5 that is useful to show wind speeds that do not change. ## add some wd bias to some nighttime hours id &lt;- which(as.numeric(format(mydata$date, &quot;%H&quot;)) %in% c(23, 1, 2, 3, 4, 5)) mydata$wd2[id] &lt;- mydata$wd[id] + 30 * rnorm(length(id)) + 120 id &lt;- which(mydata$wd2 &lt; 0) mydata$wd2[id] &lt;- mydata$wd2[id] + 360 pollutionRose(mydata, ws = &quot;ws&quot;, wd = &quot;wd&quot;, ws2 = &quot;ws2&quot;, wd2 = &quot;wd2&quot;, breaks = c(-11, -2, -1, -0.5, 0.5, 1, 2, 11), cols = c(&quot;dodgerblue4&quot;, &quot;white&quot;, &quot;firebrick&quot;), grid.line = 5, type = &quot;daylight&quot;) Figure 5.9: Pollution rose showing the difference between two meteorological data sets. The colours are used to show whether data tend to be positively or negatively biased with respect to the reference data set. In this case the example shows how to use user-defined breaks and split the data by day/night for a latitude assumed to be London. References "],
["sec-percentileRose.html", "Section6 Percentile roses 6.1 Introduction 6.2 Examples 6.3 Condtional probability function", " Section6 Percentile roses 6.1 Introduction percentileRose calculates percentile levels of a pollutant and plots them by wind direction. One or more percentile levels can be calculated and these are displayed as either filled areas or as lines. By default, the function plots percentile concentrations in 10 degree segments. Alternatively, the levels by wind direction are calculated using a cyclic smooth cubic spline. The wind directions are rounded to the nearest 10 degrees, consistent with surface data from the UK Met Office before a smooth is fitted. The percentileRose function compliments other similar functions including windRose, pollutionRose, polarFreq or polarPlot. It is most useful for showing the distribution of concentrations by wind direction and often can reveal different sources e.g. those that only affect high percentile concentrations such as a chimney stack. Similar to other functions, flexible conditioning is available through the `type} option. It is easy for example to consider multiple percentile values for a pollutant by season, year and so on. See examples below. 6.2 Examples The first example is a basic plot of percentiles of O3 shown in Figure 6.1. percentileRose(mydata, pollutant = &quot;o3&quot;) Figure 6.1: A percentileRose plot of O3 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction. It shows for example that higher concentrations occur for northerly winds, as expected at this location. However, it also shows, for example the actual value of the 95th percentile O3 concentration. A slightly more interesting plot is shown in Figure 6.2 for SO2 concentrations. We also take the opportunity of changing some default options. In this case it can be clearly seen that the highest concentrations of SO2 are dominated by east and south-easterly winds; likely reflecting the influence of stack emissions in those directions. percentileRose(mydata, pollutant = &quot;so2&quot;, percentile = c(25, 50, 75, 90, 95, 99, 99.9), col = &quot;brewer1&quot;, key.position = &quot;right&quot;, smooth = TRUE) Figure 6.2: A percentileRose plot of SO2 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction. This plot sets some user-defined percentile levels to consider the higher SO2 concentrations, moves the key to the right and uses an alternative colour scheme. Lots more insight can be gained by considering how percentile values vary by other factors i.e. conditioning. For example, what do O3 concentrations look like split by season and whether it is daylight or nighttime hours? We can set the type to consider season and whether it is daylight or nighttime.3 This Figure reveals some interesting features. First, O3 concentrations are higher in the spring and summer and when the wind is from the north. O3 concentrations are higher on average at this site in spring due to the peak of northern hemispheric O3 and to some extent local production. This may also explain why O3 concentrations are somewhat higher at nighttime in spring compared with summer. Second, peak O3 concentrations are higher during daylight hours in summer when the wind is from the south-east. This will be due to more local (UK/European) production that is photochemically driven — and hence more important during daylight hours. percentileRose(mydata, type = c(&quot;season&quot;, &quot;daylight&quot;), pollutant = &quot;o3&quot;, col = &quot;Set3&quot;, mean.col = &quot;black&quot;) Figure 6.3: A percentileRose plot of O3 concentrations at Marylebone Road. The percentile intervals are shaded and are shown by wind direction.The plot shows the variation by season and whether it is nighttime or daylight hours. 6.3 Condtional probability function The percentileRose function can also plot conditional probability functions (CPF) (Ashbaugh, Malm, and Sadeh 1985). The CPF is defined as CPF = \\(m_\\theta/n_\\theta\\), where \\(m_\\theta\\) is the number of samples in the wind sector \\(\\theta\\) with mixing ratios greater than some `high’ concentration, and \\(n_\\theta\\) is the total number of samples in the same wind sector. CPF analysis is very useful for showing which wind directions are dominated by high concentrations and give the probability of doing so. In openair, a CPF plot can be produced as shown in 6.4. Note that in these plots only one percentile is provided and the method must be supplied. In 6.4 it is clear that the high concentrations (greater than the 95th percentile of all observations) is dominated by easterly wind directions. There are very low conditional probabilities of these concentrations being experienced for other wind directions. percentileRose(mydata, poll=&quot;so2&quot;, percentile = 95, method = &quot;cpf&quot;, col = &quot;darkorange&quot;, smooth = TRUE) Figure 6.4: A CPF plot of SO2 concentrations at Marylebone Road. It is easy to plot several species on the same plot and this works well because they all have the same probability scale (i.e. 0 to 1). In the example below (not shown) it is easy to see for each pollutant the wind directions that dominate the contributions to the highest (95th percentile) concentrations. For example, the highest CO and concentrations are totally dominated by south/south-westerly winds and the probability of their being such high concentrations from other wind directions is effectively zero. percentileRose(mydata, pollutant = c(&quot;nox&quot;, &quot;so2&quot;, &quot;o3&quot;, &quot;co&quot;, &quot;pm10&quot;, &quot;pm25&quot;), percentile = 95, method = &quot;cpf&quot;, col = &quot;darkorange&quot;, layout = c(3, 2)) Figure 6.5: A CPF plot of many pollutants at Marylebone Road. References "],
["sec-polarPlot.html", "Section7 Polar plots 7.1 Introduction to polar plots 7.2 Examples 7.3 Nonparametric Wind Regression, NWR 7.4 Conditional Probability Function (CPF) plot 7.5 Pairwise statistics 7.6 Clustering", " Section7 Polar plots This Sections considers the flexible polarPlot function that is used to provide information about source characteritics. The clustering of polar plots is also considered using polarCluster. 7.1 Introduction to polar plots The polarPlot function plots a bivariate polar plot of concentrations. Concentrations are shown to vary by wind speed and wind direction. In many respects they are similar to the plots shown in Section ?? but include some additional enhancements. These enhancements include: plots are shown as a continuous surface and surfaces are calculated through modelling using smoothing techniques. These plots are not entirely new as others have considered the joint wind speed-direction dependence of concentrations (see for example Yu et al. (2004)). However, plotting the data in polar coordinates and for the purposes of source identification is new. Furthermore, the basic polar plot is since been enhanced in many ways as described below. Publications that describe or use the technique are Carslaw et al. (2006) and Westmoreland et al. (2007). These plots have proved to be useful for quickly gaining a graphical impression of potential sources influences at a location. The polarPlot function is described in more detail in Carslaw et al. (2006) where it is used to triangulate sources in an airport setting, Carslaw and Beevers (2013) where it is used with a clustering technique to identify features in a polar plot with similar characteristics and Uria-Tellaetxe and Carslaw (2014) where it is extended to include a conditional probability function to extract more information from the plots. For many, maybe most situations, increasing wind speed generally results in lower concentrations due to increased dilution through advection and increased mechanical turbulence. There are, however, many processes that can lead to interesting concentration-wind speed dependencies, and we will provide a more theoretical treatment of this in due course. However, below are a few reasons why concentrations can change with increasing wind speeds. Buoyant plumes from tall stacks can be brought down to ground-level resulting in high concentrations under high wind speed conditions. Particle suspension increases with increasing wind speeds e.g. PM10 from spoil heaps and the like. ‘Particle’ suspension can be important close to coastal areas where higher wind speeds generate more sea spray. The wind speed dependence of concentrations in a street canyon can be very complex: higher wind speeds do not always results in lower concentrations due to re-circulation. Bivariate polar plots are very good at revealing these complexities. As Carslaw et al. (2006) showed, aircraft emissions have an unusual wind speed dependence and this can help distinguish them from other sources. If several measurement sites are available, polar plots can be used to triangulate different sources. Concentrations of NO2 can increase with increasing wind speed — or at least not decline steeply due to increased mixing. This mixing can result in O3-rich air converting NO to NO2. The function has been developed to allow variables other than wind speed to be plotted with wind direction in polar coordinates. The key issue is that the other variable plotted against wind direction should be discriminating in some way. For example, temperature can help reveal high-level sources brought down to ground level in unstable atmospheric conditions, or show the effect a source emission dependent on temperature e.g. biogenic isoprene. For research applications where many more variables could be available, discriminating sources by these other variables could be very insightful. Bivariate polar plots are constructed in the following way. First, wind speed, wind direction and concentration data are partitioned into wind speed-direction bins and the mean concentration calculated for each bin. Testing on a wide range of data suggests that wind direction intervals at 5–10\\(^\\circ\\) and 40 wind speed intervals capture sufficient detail of the concentration distribution. The wind direction data typically available are generally rounded to 10\\(^\\circ\\) and for typical surface measurements. Binning the data in this way is not strictly necessary but acts as an effective data reduction technique without affecting the fidelity of the plot itself. Furthermore, because of the inherent wind direction variability in the atmosphere, data from several weeks, months or years typically used to construct a bivariate polar plot tends to be diffuse and does not vary abruptly with either wind direction or speed and more finely resolved bin sizes or working with the raw data directly does not yield more information. The wind components, \\(u\\) and \\(v\\) are calculated i.e. \\[\\begin{equation} u = \\overline{u} . sin\\left(\\frac{2\\pi}{\\theta}\\right), v = \\overline{u} . cos\\left(\\frac{2\\pi}{\\theta}\\right) \\tag{7.1} \\end{equation}\\] with \\(\\overline{u}\\) is the mean hourly wind speed and \\(\\theta\\) is the mean wind direction in degrees with 90\\(^\\circ\\) as being from the east. The calculations above provides a \\(u\\), \\(v\\), concentration (\\(C\\)) surface. While it would be possible to work with this surface data directly a better approach is to apply a model to the surface to describe the concentration as a function of the wind components \\(u\\) and \\(v\\) to extract real source features rather than noise. A flexible framework for fitting a surface is to use a Generalized Additive Model (GAM) e.g. Hastie and Tibshirani (1990), Wood (2006). GAMs are a useful modelling framework with respect to air pollution prediction because typically the relationships between variables are non-linear and variable interactions are important, both of which issues can be addressed in a GAM framework. GAMs can be expressed as shown in Equation (7.2): \\[\\begin{equation} \\sqrt{C_i} = \\beta_0 + \\sum_{j=1}^{n}s_j(x_{ij}) + e_i \\tag{7.2} \\end{equation}\\] where \\(C_i\\) is the ith pollutant concentration, \\(\\beta_0\\) is the overall mean of the response, \\(s_j(x_{ij})\\) is the smooth function of ith value of covariate \\(j\\), \\(n\\) is the total number of covariates, and \\(e_i\\) is the \\(i\\)th residual. Note that \\(C_i\\) is square-root transformed as the transformation generally produces better model diagnostics e.g. normally distributed residuals. The model chosen for the estimate of the concentration surface is given by @ref(eq:mod}. In this model the square root-transformed concentration is a smooth function of the bivariate wind components \\(u\\) and \\(v\\). Note that the smooth function used is isotropic because \\(u\\) and \\(v\\) are on the same scales. The isotropic smooth avoids the potential difficulty of smoothing two variables on different scales e.g. wind speed and direction, which introduces further complexities. \\[\\begin{equation} \\sqrt{C_i} = s(u, v) + e_i \\tag{7.3} \\end{equation}\\] 7.2 Examples We first use the function in its simplest form to make a polar plot of NOx. The code is very simple as shown in Figure 7.1. polarPlot(mydata, pollutant = &quot;nox&quot;) Figure 7.1: Default use of the polarPlot function applied to Marylebone Road NOx concentrations. This produces Figure 7.1. The scale is automatically set using whatever units the original data are in. This plot clearly shows highest NOx concentrations when the wind is from the south-west. Given that the monitor is on the south side of the street and the highest concentrations are recorded when the wind is blowing away from the monitor is strong evidence of street canyon recirculation. Figure 7.2 and Figure 7.3 shows polar plots using different defaults and for other pollutants. In the first (Figure 7.2, a different colour scheme is used and some adjustments are made to the key. In Figure 7.3, SO2 concentrations are shown. What is interesting about this plot compared with either Figure 7.2 or Figure 7.1 is that the concentration pattern is very different i.e. high concentrations with high wind speeds from the east. The most likely source of this SO2 are industrial sources to the east of London. The plot does still however show evidence of a source to the south-west, similar to the plot for NOx, which implies that road traffic sources of SO2 can also be detected. These plots often show interesting features at higher wind speeds. For these conditions there can be very few measurements and therefore greater uncertainty in the calculation of the surface. There are several ways in which this issue can be tackled. First, it is possible to avoid smoothing altogether and use polarFreq. The problem with this approach is that it is difficult to know how best to bin wind speed and direction: the choice of interval tends to be arbitrary. Second, the effect of setting a minimum number of measurements in each wind speed-direction bin can be examined through min.bin. It is possible that a single point at high wind speed conditions can strongly affect the surface prediction. Therefore, setting min.bin = 3, for example, will remove all wind speed-direction bins with fewer than 3 measurements before fitting the surface. This is a useful strategy for testing how sensitive the plotted surface is to the number of measurements available. While this is a useful strategy to get a feel for how the surface changes with different min.bin settings, it is still difficult to know how many points should be used as a minimum. Third, consider setting uncertainty = TRUE. This option will show the predicted surface together with upper and lower 95% confidence intervals, which take account of the frequency of measurements. The uncertainty approach ought to be the most robust and removes any arbitrary setting of other options. There is a close relationship between the amount of smoothing and the uncertainty: more smoothing will tend to reveal less detail and lower uncertainties in the fitted surface and vice-versa. The default however is to down-weight the bins with few data points when fitting a surface. Weights of 0.25, 0.5 and 0.75 are used for bins containing 1, 2 and 3 data points respectively. The advantage of this approach is that no data are actually removed (which is what happens when using min.bin). This approach should be robust in a very wide range of situations and is also similar to the approaches used when trying to locate sources when using back trajectories as described in Section 17. Users can ignore the automatic weighting by supplying the option weights = c(1, 1, 1). ## NOx plot polarPlot(mydata, pollutant = &quot;nox&quot;, col = &quot;jet&quot;, key.position = &quot;bottom&quot;, key.header = &quot;mean nox (ug/m3)&quot;, key.footer = NULL) Figure 7.2: Example plots using the polarPlot function with different options for the mean concentration of NOx. polarPlot(mydata, pollutant = &quot;so2&quot;) Figure 7.3: Example plots using the polarPlot function for the mean concentration of SO2. A very useful approach for understanding air pollution is to consider ratios of pollutants. One reason is that pollutant ratios can be largely independent of meteorological variation. In many circumstances it is possible to gain a lot of insight into sources if pollutant ratios are considered. First, it is necessary to calculate a ratio, which is easy in R. In this example we consider the ratio of SO2/NOx: library(tidyverse) mydata &lt;- mutate(mydata, ratio = so2 / nox) This makes a new variable called ratio. Sometimes it can be problematic if there are values equal to zero on the denominator, as is the case here. The mean and maximum value of the ratio is infinite, as shown by the Inf in the statistics below. Luckily, R can deal with infinity and the openair functions will remove these values before performing calculations. It is very simple therefore to calculate ratios. summary(mydata$ratio) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.000 0.018 0.024 Inf 0.034 Inf 11782 A polar plot of the SO2/NOx ratio is shown in Figure 7.4. The plot highlights some new features not seen before. First, to the north there seems to be evidence that the air tends to have a higher SO2/NOx ratio. Also, the source to the east has a higher SO2/NOx ratio compared with that when the wind is from the south-west i.e. dominated by road sources. It seems therefore that the easterly source(s), which are believed to be industrial sources have a different SO2/NOx ratio compared with road sources. This is a very simple analysis, but ratios can be used effectively in many functions and are particularly useful in the presence of high source complexity. polarPlot(mydata, pollutant = &quot;ratio&quot;, main = &quot;so2/nox ratio&quot;) Figure 7.4: Bivariate polar plot of the ratio of SO2/NOx. Sometimes when considering ratios it might be necessary to limit the values in some way; perhaps due to some unusually low value denominator data resulting in a few very high values for the ratio. This is easy to do with the dplyr filter command. The code below selects ratios less than 0.1. polarPlot(filter(mydata, ratio &lt; 0.1), pollutant = &quot;ratio&quot;) The uncertainties in the surface can be calculated by setting the option uncertainty = TRUE. The details are described above and here we show the example of SO2 concentrations (Figure 7.5. In general the uncertainties are higher at high wind speeds i.e. at the ‘fringes’ of a plot where there are fewer data. However, the magnitude depends on both the frequency and magnitude of the concentration close to the points of interest. The pattern of uncertainty is not always obvious and it can differ markedly for different pollutants. polarPlot(mydata, pollutant = &quot;so2&quot;, uncertainty = TRUE) Figure 7.5: Bivariate polar plot of SO2 concentrations at Marylebone Road. Three surfaces are shown: the central prediction (middle) and the lower and upper 95% estimated uncertainties. These plots help to show that in this particular case, some of the concentrations for strong easterly and south-easterly winds are rather uncertain. However, the central feature to the east remains, suggesting this feature is real and not an artifact of there being too few data. The polarPlot function can also produce plots dependent on another variable (see the type option). For example, the variation of SO2 concentrations at Marylebone Road by hour of the day in the code below. The function was called as shown in below, and in this case the minimum number of points in each wind speed/direction was set to 2. polarPlot(mydata, pollutant = &quot;so2&quot;, type = &quot;hour&quot;, min.bin = 2) This plot shows that concentrations of SO2 tend to be highest from the east (as also shown in Figure 7.3) and for hours in the morning. Together these plots can help better understand different source types. For example, does a source only seem to be present during weekdays, or winter months etc. In the case of type = \"hour\", the more obvious presence during the morning hours could be due to meteorological factors and this possibility should be investigated as well. In other settings where there are many sources that vary in their source emission and temporal characteristics, the polarPlot function should prove to be very useful. One issue to be aware of is the amount of data required to generate some of these plots; particularly the hourly plots. If only a relatively short time series is available there may not be sufficient information to produce useful plots. Whether this is important or not will depend on the specific circumstances e.g. the prevalence of wind speeds and directions from the direction of interest. When used to produce many plots (e.g. type = \"hour\"), the run time can be quite long. 7.3 Nonparametric Wind Regression, NWR An alternative approach to modelling the surface concentrations with a GAM is to use kernel smoothers, as described by Henry et al. (2009). In NWR, smoothing is achieved using nonparametric kernel smoothers that weight concentrations on a surface according to their proximity to defined wind speed and direction intervals. In the approach adopted in openair (which is not identical to Henry et al. (2009)), Gaussian smoothers are used for both wind direction and wind speed. Unlike the default GAM approach in openair, the NWR technique works directly with the raw (often hourly) data. It tends to provide similar results to openair but may have advantages in certain situations e.g. when there is insufficient data available to use a GAM. The width of the Gaussian kernels (\\(\\sigma\\)) is controlled by the options wd_spread and ws_spread. An example for SO2 concentrations is shown in Figure 7.6, which can be compared with Figure 7.3. polarPlot(mydata, pollutant = &quot;so2&quot;, statistic = &quot;nwr&quot;) Figure 7.6: polarPlot of SO2 concentrations at Marylebone Road based on the NWR approach. 7.4 Conditional Probability Function (CPF) plot The conditional probability functions (CPF) was described on in the context of the percentileRose function. The CPF was originally used to show the wind directions that dominate a (specified) high concentration of a pollutant; showing the probability of such concentrations occurring by wind direction Ashbaugh, Malm, and Sadeh (1985). However, these ideas can very usefully be applied to bivariate polar plots. In this case the CPF is defined as CPF = \\(m_{\\theta,j}/n_{\\theta,j}\\), where \\(m_{\\theta,j}\\) is the number of samples in the wind sector \\(\\theta\\) and wind speed interval \\(j\\) with mixing ratios greater than some ‘high’ concentration, and \\(n_{\\theta, j}\\) is the total number of samples in the same wind direction-speed interval. Note that \\(j\\) does not have to be wind speed but could be any numeric variable e.g. ambient temperature. CPF analysis is very useful for showing which wind direction, wind speed intervals are dominated by high concentrations and give the probability of doing so. A full explanation of the development and use of the bivariate case of the CPF is described in Uria-Tellaetxe and Carslaw (2014) where it is applied to monitoring data close to steelworks. polarPlot(mydata, pollutant = &quot;so2&quot;, statistic = &quot;cpf&quot;, percentile = 90) Figure 7.7: polarPlot of SO2 concentrations at Marylebone Road based on the CPF function. An example of a CPF polar plot is shown in Figure 7.7 for the 90th percentile concentration of SO2. This plot shows that for most wind speed-directions the probability of SO2 concentrations being greater than the 90th percentile is zero. The clearest areas where the probability is higher is to the east. Indeed, the plot now clearly reveals two potential sources of SO2, which are not as apparent in the ‘standard’ plot shown in Figure 7.3. Note that Figure 7.7 also gives the calculated percentile at the bottom of the plot (9.2 ppb in this case). Figure 7.7 can also be compared with the CPF plot based only on wind direction shown in Figure 6.4. While Figure 6.4 very clearly shows that easterly wind dominate high concentrations of SO2, Figure 7.7 provides additional valuable information by also considering wind speed, which in this case is able to discriminate between two sources (or groups of sources) to the east. The polar CPF plot is therefore potentially very useful for source identification and characterisation. It is, for example, worth also considering other percentile levels and other pollutants. For example, considering the 95th percentile for SO2 ‘removes’ one of the sources (the one at highest wind speed). This helps to show some maybe important differences between the sources that could easily have been missed. Similarly, considering other pollutants can help build up a good understanding of these sources. A CPF plot for NO2 at the 90th percentile shows the single dominance of the road source. However, a CPF plot at the 75th percentile level indicates source contributions from the east (likely tall stacks), which again are not as clear in the standard bivariate polar plot. Considering a range of percentile values can therefore help to build up a more complete understanding of source contributions. polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(0, 10)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(10, 20)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(20, 30)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(30, 40)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(40, 50)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(50, 60)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(60, 70)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(70, 80)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(80, 90)) polarPlot(mydata, poll = &quot;so2&quot;, stati = &quot;cpf&quot;, percentile = c(90, 100)) Figure 7.8: polarPlot of SO2 concentrations at Marylebone Road based on the CPF function for a range of percentile intervals from 0–10, 10–20, … , 90–100. However, even more useful information can be gained by considering intervals of percentiles e.g. 50–60, 60–70 etc. By considering intervals of percentiles it becomes clear that some sources only affect a limited percentile range. polarPlot can accept a percentile argument of length two e.g. percentile = c(80, 90). In this case concentrations in the range from the lower to upper percentiles will be considered. In Figure 7.8 for example, it is apparent that the road source to the south west is only important between the 60 to 90th percentiles. As mentioned previously, the chimney stacks to the east are important for the higher percentiles (90 to 100). What is interesting though is the emergence of what appears to be other sources at the lower percentile intervals. These potential sources are not apparent in Figure 7.3. The other interesting aspect is that it does seem that specific sources tend to be prominent for specific percentile ranges. If this characteristic is shown to be the case more generally, then CPF intervals could be a powerful way in which to identify many sources. Whether these particular sources are important or not is questionable and depends on the aims of the analysis. However, there is no reason to believe that the potential sources shown in the percentile ranges 0 to 50 are artefacts. They could for example be signals from more distant point sources whose plumes have diluted more over longer distances. Such sources would be ‘washed out’ in an ordinary polar plot. For a fuller example of this approach see Uria-Tellaetxe and Carslaw (2014). Note that it is easy to work out what the concentration intervals are for the percentiles shown in Figure 7.8: quantile(mydata$so2, probs = seq(0, 1, by = 0.1), na.rm = TRUE) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% ## 0.0000 1.0125 1.8825 2.5000 3.2500 4.0000 4.9375 5.9100 7.2375 9.2500 ## 100% ## 63.2050 To plot the Figures on one page it is necessary to make the plot objects first and then decide how to plot them. To plot the Figures in a particular layout see ??. 7.5 Pairwise statistics Grange, Lewis, and Carslaw (2016) further developed the capabilities of the polarPlot function by allowing pairwise statistics to be used. This method makes it possible to consider the relationship between two pollutants to be considered. The relationship between two pollutants often yields useful source apportionment information and when combined with the polarPlot function can provide enhanced information. The pairwise statistics that can be considered include: The Pearson or Spearman correlation coefficient, \\(r\\); The robust slope (gradient) resulting from a linear regressions between two pollutants. The quantile slope from a quantile regression applied to two variables with a quantile value of \\(tau\\). By default, the median slope (i.e. \\(tau\\) = 0.5) is used by the actual level can be set by the user. The calculation involves a weighted Pearson correlation coefficient, which is weighted by Gaussian kernels for wind direction and the radial variable (by default wind speed). More weight is assigned to values close to a wind speed-direction interval. Kernel weighting is used to ensure that all data are used rather than relying on the potentially small number of values in a wind speed-direction interval. The calculation involves a weighted statistics by Gaussian kernels for wind direction and the radial variable (by default wind speed). More weight is assigned to values close to a wind speed-direction interval. Kernel weighting is used to ensure that all data are used rather than relying on the potentially small number of values in a wind speed-direction interval. An example usage scenario is that measurements of metal concentrations are made close to a steelworks where there is interest in understanding the principal sources. While it is useful to consider the correlation between potentially many metal concentrations, the contention is that if the correlation is also considered as a function of wind speed and direction, improved information will be available of the types of sources contributing. For example, it may be that Fe and Mn are quite strongly correlated overall, but they tend to be most correlated under specific wind speed and direction ranges — suggesting a specific source origin. As an example of usage we will consider the relationship between PM2.5 and PM10 at the rural Harwell site in Oxfordshire. Additionally, we will use meteorological data from a nearby site rather than rely on modelled values that are provided in importAURN. library(worldmet) # to access met data library(tidyverse) har &lt;- importAURN(&quot;har&quot;, year = 2013) # import met data from nearby site (Benson) met &lt;- importNOAA(code = &quot;036580-99999&quot;, year = 2013) # merge AQ and met but don&#39;t use modelled ws and wd har &lt;- inner_join( select(har, -ws, -wd, -air_temp), met, by = &quot;date&quot; ) An example pairwise regression surface relationship relating PM2.5 and PM10 is shown in Figure 7.9. This plot reveals that almost all the PM10 is in the form of PM2.5 when the wind has an easterly component, which is attributed to the large secondary contribution likely dominated by ammonium nitrate. A simple scatter plot between PM2.5 and PM10 strongly suggests a 1:1 relationship and it is not obvious that there is a higher PM2.5/PM10 ratio when the wind is from the east. polarPlot(har, poll = c(&quot;pm2.5&quot;, &quot;pm10&quot;), statistic = &quot;robust_slope&quot;, col = &quot;jet&quot;, limits = c(0, 1), ws_spread = 1.5, wd_spread = 10) Figure 7.9: Use of the polarPlot function to investigate the linear regression slope between PM2.5 and PM10 at Harwell in 2013. In this case the robust slope is calculated. 7.6 Clustering The polarPlot function will often identify interesting features that would be useful to analyse further. It is possible to select areas of interest based only on a consideration of a plot. Such a selection could be based on wind direction and wind speed intervals for example e.g. subdata &lt;- filter(mydata, ws &gt; 3, wd &gt;= 180, wd &lt;= 270) which would select wind speeds &gt;3 m s-1 and wind directions from 180 to 270 degrees from mydata. That subset of data, subdata, could then be analysed using other functions. While this approach may be useful in many circumstances it is rather arbitrary. In fact, the choice of ‘interesting feature’ in the first place can even depend on the colour scale used, which is not very robust. Furthermore, many interesting patterns can be difficult to select and won’t always fall into convenient intervals of other variables such as wind speed and direction. A better approach is to use a method that can select group similar features together. One such approach is to use cluster analysis. openair uses k-means clustering as a way in which bivariate polar plot features can be identified and grouped. The main purpose of grouping data in this way is to identify records in the original time series data by cluster to enable post-processing to better understand potential source characteristics. The process of grouping data in k-means clustering proceeds as follows. First, \\(k\\) points are randomly chosen form the space represented by the objects that are being clustered into \\(k\\) groups. These points represent initial group centroids. Each object is assigned to the group that has the closest centroid. When all objects have been assigned, the positions of the \\(k\\) centroids is re-calculated. The previous two steps are repeated until the centroids no longer move. This produces a separation of the objects into groups from which the metric to be minimised can be calculated. Central to the idea of clustering data is the concept of distance i.e. some measure of similarity or dissimilarity between points. Clusters should be comprised of points separated by small distances relative to the distance between the clusters. Careful consideration is required to define the distance measure used because the effectiveness of clustering itself fundamentally depends on its choice. The similarity of concentrations shown in Figure 7.1 for example is determined by three variables: the \\(u\\) and \\(v\\) wind components and the concentration. All three variables are equally important in characterising the concentration-location information, but they exist on different scales i.e. a wind speed-direction measure and a concentration. Let \\(X = \\{x_i\\}, i = 1,\\ldots,n\\) be a set of \\(n\\) points to be clustered into \\(K\\) clusters, \\(C = \\{c_k, k = 1,\\ldots,K\\}\\). The basic k-means algorithm for \\(K\\) clusters is obtained by minimising: \\[\\begin{equation} \\sum_{k=1}^{K} \\sum_{x_i \\in c_k} || x_i - \\mu_k ||^2 \\tag{7.4} \\end{equation}\\] where \\(|| x_i - \\mu_k ||^2\\) is a chosen distance measure, \\(\\mu_k\\) is the mean of cluster \\(c_k\\). The distance measure is defined as the Euclidean distance: \\[\\begin{equation} d_{x, y} = \\left({\\sum_{j=1}^{J} (x_j - y_j) ^ 2}\\right)^{1/2} \\tag{7.5} \\end{equation}\\] Where x and y are two J-dimensional vectors, which have been standardized by subtracting the mean and dividing by the standard deviation. In the current case \\(J\\) is of length three i.e. the wind components \\(u\\) and \\(v\\) and the concentration \\(C\\), each of which is standardized e.g.: \\[\\begin{equation} x_j = \\left(\\frac{x_j - \\overline{x}}{\\sigma_x}\\right) \\tag{7.6} \\end{equation}\\] Standardization is necessary because the wind components \\(u\\) and \\(v\\) are on different scales to the concentration. In principle, more weight could be given to the concentration rather than the \\(u\\) and \\(v\\) components, although this would tend to identify clusters with similar concentrations but different source origins. polarCluster can be thought of as the ‘local’ version of clustering of back trajectories. Rather than using air mass origins, wind speed, wind direction and concentration are used to group similar conditions together. Section 17.6 provides the details of clustering back trajectories in openair. A fuller description of the clustering approach is described in Carslaw and Beevers (2013). The use of the polarCluster is very similar to the use of all openair functions. While there are many techniques available to try and find the optimum number of clusters, it is difficult for these approaches to work in a consistent way for identifying features in bivariate polar plots. For this reason it is best to consider a range of solutions that covers a number of clusters. Cluster analysis is computationally intensive and the polarCluster function can take a comparatively long time to run. The basic idea is to calculate the solution to several cluster levels and then choose one that offers the most appropriate solution for post-processing. The example given below is for concentrations of SO2, shown in Figure 7.3 and the aim is to identify features in that plot. A range of numbers of clusters will be calculated — in this case from two to ten. polarCluster(mydata, pollutant=&quot;so2&quot;, n.clusters=2:10, cols= &quot;Set2&quot;) Figure 7.10: Use of the polarCluster function applied to SO2 concentrations at Marylebone Road. In this case 2 to 10 clusters have been chosen. results &lt;- polarCluster(mydata, pollutant=&quot;so2&quot;, n.clusters = 8, cols = &quot;Set2&quot;) Figure 7.11: Use of the polarCluster function applied to SO2 concentrations at Marylebone Road. In this case 8 clusters have been chosen. The real benefit of polarCluster is being able to identify clusters in the original data frame. To do this, the results from the analysis must be read into a new variable, as in Figure 7.11, where the results are read into a data frame `results}. Now it is possible to use this new information. In the 8-cluster solution to Figure 7.11, cluster~6 seems to capture the elevated SO2 concentrations to the east well (see Figure 7.3 for comparison), while cluster 5 will strongly represent the road contribution. The results are here: head(results[[&quot;data&quot;]]) ## .id date ws wd nox no2 o3 pm10 so2 co pm25 ## 1 1 1998-01-01 00:00:00 0.60 280 285 39 1 29 4.7225 3.3725 NA ## 2 10 1998-01-01 09:00:00 3.96 170 113 39 2 12 2.9225 1.2050 NA ## 3 100 1998-01-05 03:00:00 8.76 240 53 24 16 12 1.5950 0.5400 NA ## 4 1000 1998-02-11 15:00:00 7.20 230 372 92 2 49 8.5950 3.7175 NA ## 5 10000 1999-02-21 15:00:00 6.36 290 71 32 20 10 2.6225 1.0500 8 ## 6 10001 1999-02-21 16:00:00 7.68 290 85 39 16 12 2.8700 1.4750 9 ## ws2 wd2 ratio cluster ## 1 -0.244089 312.9327 0.01657018 4 ## 2 5.366733 185.3829 0.02586283 5 ## 3 10.971351 382.0663 0.03009434 4 ## 4 9.839028 242.3029 0.02310484 4 ## 5 6.776110 344.4104 0.03693662 7 ## 6 10.861975 303.2430 0.03376471 7 Note that there is an additional column cluster that gives the cluster a particular row belongs to and that this is a character variable. It might be easier to read these results into a new data frame: results &lt;- results[[&quot;data&quot;]] It is easy to find out how many points are in each cluster: table(results[[&quot;cluster&quot;]]) ## ## 1 2 3 4 5 6 7 8 ## 206 412 160 24133 16049 2590 7925 2832 Now other openair analysis functions can be used to analyse the results. For example, to consider the temporal variations by cluster: timeVariation(results, pollutant = &quot;so2&quot;, group = &quot;cluster&quot;, key.columns = 4, col = &quot;Set2&quot;, ci = FALSE, lwd = 3) Figure 7.12: Temporal variation in SO2 split by cluster. Or if we just want to plot a couple of clusters (5 and 6) using the same colours as in Figure 7.11: timeVariation(filter(results, cluster %in% c(&quot;5&quot;, &quot;6&quot;)), pollutant = &quot;so2&quot;, group = &quot;cluster&quot;, col = openColours(&quot;Set2&quot;, 8)[5:6], lwd = 3) Figure 7.13: Temporal variation in SO2 split by cluster — showing only two clusters. polarCluster will work on any surface that can be plotted by polarPlot e.g. the radial variable does not have to be wind speed but could be another variable such as temperature. While it is not always possible for polarCluster to identify all features in a surface it certainly makes it easier to post-process polarPlots using other openair functions or indeed other analyses altogether. Another useful way of understanding the clusters is to use the timeProp function, which can display a time series as a bar chart split by a categorical variable (in this case the cluster). In this case it is useful to plot the time series of SO2 and show how much of the concentration is contributed to by each cluster. Such a plot is shown in Figure 7.14. It is now easy to see for example that many of the peaks in SO2 are associated with cluster 6 (power station sources from the east), seen in Figure 7.11. Cluster~6 is particularly prominent during springtime, but those sources also make important contributions through the whole year. timeProp(selectByDate(results, year = 2003), pollutant = &quot;so2&quot;, avg.time = &quot;day&quot;, proportion= &quot;cluster&quot;, col = &quot;Set2&quot;, key.position = &quot;top&quot;, key.columns = 8, date.breaks = 10, ylab = &quot;so2 (ug/m3)&quot;) Figure 7.14: Temporal variation in daily SO2 concentration at the Marylebone Road site show by contribution of each cluster for 2003. References "],
["sec-polarAnnulus.html", "Section8 Polar annulus 8.1 Purpose 8.2 Example of use", " Section8 Polar annulus 8.1 Purpose The polarAnnulus function provides a way in which to consider the temporal aspects of a pollutant concentration by wind direction. This is another means of visualising diurnal, day of week, seasonal and trend variations. Plotting as an annulus, rather than a circle avoids to some extent the difficulty in interpreting values close to the origin. These plots have the capacity to display potentially important information regarding sources; particularly if more than one pollutant is available. 8.2 Example of use We apply the four variations of the polarAnnulus plot to PM10 concentrations at Marylebone Road. Figure 8.1 shows the different temporal components. Similar to other analyses for PM10, the trend plot show that concentrations are dominated by southerly winds and there is little overall change in concentrations from 1998 to 2005, as shown by the red colouring over the period. The seasonal plot shows that February/March is important for easterly winds, while the summer/late summer period is more important for southerly and south-westerly winds. The day of the week plot clearly shows concentrations to be elevated for during weekdays but not weekends — for all wind directions. Finally, the diurnal plot highlights that higher concentrations are observed from 6 am to 6 pm. Interestingly, the plot for NOx and CO (not shown, but easily produced) did not show such a strong contribution for south-easterly winds. This raises the question whether the higher particle concentrations seen for these wind directions are dominated by different sources (i.e. not the road itself). One explanation is that during easterly flow, concentrations are strongly affected by long-range transport. However, as shown in the diurnal plot in Figure 8.1, the contribution from the south-east also has a sharply defined profile — showing very low concentrations at night, similar to the likely contribution from the road. This type of profile might not be expected from a long-range source where emissions are well-mixed and secondary particle formation has had time to occur. The same is also true for the day of the week plot, where there is little evidence of ‘smeared-out’ long-range transport sources. These findings may suggest a different, local source of PM10 that is not the road itself. Clearly, a more detailed analysis would be required to confirm the patterns shown; but it does highlight the benefit of being able to analyse data in different ways. data(mydata) polarAnnulus(mydata, poll = &quot;pm10&quot;, period = &quot;trend&quot;, main = &quot;Trend&quot;) polarAnnulus(mydata, poll = &quot;pm10&quot;, period = &quot;season&quot;, main = &quot;Season&quot;) polarAnnulus(mydata, poll = &quot;pm10&quot;, period = &quot;weekday&quot;, main = &quot;Weekday&quot;) polarAnnulus(mydata, poll = &quot;pm10&quot;,period = &quot;hour&quot;, main = &quot;Hour&quot;) Figure 8.1: Examples of the polarAnnulus function applied to Marylebone Road. Where there is interest in considering the wind direction dependence of concentrations, it can be worth filtering for wind speeds. At low wind speed with wind direction becomes highly variable (and is often associated with high pollutant concentrations). Therefore, for some situations it might be worth considering removing the very low wind speeds. The code below provides two ways of doing this using the dplyr filter function. The first selects data where the wind speed is &gt;2 m s-1. The second part shows how to select wind speeds greater than the 10th percentile, using the quantile function. The latter way of selecting is quite useful, because it is known how much data are selected i.e. in this case 90%. It is worth experimenting with different values because it is also important not to lose information by ignoring wind speeds that provide useful information. ## wind speed &gt;2 polarAnnulus(filter(mydata, ws &gt; 2), poll=&quot;pm10&quot;, type = &quot;hour&quot;) ## wind speed &gt; 10th percentile polarAnnulus(subset(mydata, ws &gt; quantile(ws, probs = 0.1, na.rm = TRUE)), poll=&quot;pm10&quot;, type = &quot;hour&quot;) "],
["sec-timePlot.html", "Section9 Time series plots 9.1 Background 9.2 Examples of time series plotting", " Section9 Time series plots 9.1 Background The timePlot function is designed to quickly plot time series of data, perhaps for several pollutants or variables. This is, or should be, a very common task in the analysis of air pollution. In doing so, it is helpful to be able to plot several pollutants at the same time (and maybe other variables) and quickly choose the time periods of interest. It will plot time series of type Date and hourly and high time resolution data. With packages such as ggplot2 it is very easy to plot time series. However, there are a few enhancements in timePlot such as flexible time-averaging and adding annotations for wind speed and direction that make it useful in some situations. The function offers fine control over many of the plot settings such as line type, colour and width. If more than one pollutant is selected, then the time series are shown compactly in different panels with different scales. Sometimes it is useful to get and idea of whether different variables ‘go up and down’ together. Such comparisons in timePlot are made easy by setting group = TRUE, and maybe also normalise = \"mean\". The latter setting divides each variable by its mean value, thus enabling several variables to be plotted together using the same scale. The normalise option will also take a date as a string (in British format dd/mm/YYYY), in which case all data are normalise to equal 100 at that time. Normalising data like this makes it easy to compare time series on different scales e.g. emissions and ambient measurements. timePlot works very well in conjunction with selectByDate, which makes it easy to select specific time series intervals. See (Section 19.1) for examples of how to select parts of a data frame based on the date. Another useful feature of timePlot is the ability to average the data in several ways. This makes it easy, for example, to plot daily or monthly means from hourly data, or hourly means from 15-minute data. See the option avg.time for more details and (Section 19.5) where a full description of time averaging of data frames is given. 9.2 Examples of time series plotting A full set of examples is shown in the help pages — see ?timePlot for details. At the basic level, concentrations are shown using a simple call e.g. to plot time series of NOx and O3 in separate panels with their own scales. timePlot(mydata, pollutant = c(&quot;nox&quot;, &quot;o3&quot;), y.relation = &quot;free&quot;) Often it is necessary to only consider part of a time series and using the openair function selectByDate makes it easy to do this. Some examples are shown below. To plot data only for 2003: timePlot(selectByDate(mydata, year = 2003), pollutant = c(&quot;nox&quot;, &quot;o3&quot;), y.relation = &quot;free&quot;) Plots for several pollutants for August 2003, are shown in Figure 9.1. library(openair) library(tidyverse) timePlot(selectByDate(mydata, year = 2003, month = &quot;aug&quot;), pollutant = c(&quot;nox&quot;, &quot;o3&quot;, &quot;pm25&quot;, &quot;pm10&quot;, &quot;ws&quot;), y.relation = &quot;free&quot;) Figure 9.1: Time series for several variables using the timePlot and the selectByDate functions. The data shown are for August 2003. Some other examples (not plotted) are: ## plot monthly means of ozone and no2 timePlot(mydata, pollutant = c(&quot;o3&quot;, &quot;no2&quot;), avg.time = &quot;month&quot;, y.relation = &quot;free&quot;) ## plor 95th percentile monthly concentrations timePlot(mydata, pollutant = c(&quot;o3&quot;, &quot;no2&quot;), avg.time = &quot;month&quot;, statistic = &quot;percentile&quot;, percentile = 95, y.relation = &quot;free&quot;) ## plot the number of valid records in each 2-week period timePlot(mydata, pollutant = c(&quot;o3&quot;, &quot;no2&quot;), avg.time = &quot;2 week&quot;, statistic = &quot;frequency&quot;, y.relation = &quot;free&quot;) An example of normalising data is shown in Figure 9.2. In this plot we have: Averaged the data to annual means; Chosen to normalise to the beginning of 2008; Set the line width to 4 and the line type to 1 (continuous line); Chosen to group the data in one panel. Figure 9.2 shows that concentrations of NO2 and O3 have increased over the period 1998–2005; SO2 and CO have shown the greatest reductions (by about 60%), whereas NOx concentrations have decreased by about 20%. timePlot(mydata, pollutant = c(&quot;nox&quot;, &quot;no2&quot;, &quot;co&quot;, &quot;so2&quot;, &quot;pm10&quot;), avg.time = &quot;year&quot;, normalise = &quot;1/1/1998&quot;, lwd = 4, lty = 1, group = TRUE, ylim = c(0, 120)) Figure 9.2: An example of normalising time series data to fix values to equal 100 at the beginning of 1998. Another example is grouping pollutants from several sites on one plot. It is easy to import data from several sites and to plot the data in separate panels e.g. ## import data from 3 sites aq &lt;- importAURN(site = c(&quot;kc1&quot;, &quot;my1&quot;, &quot;nott&quot;), year = 2005:2010) ## plot it timePlot(aq, pollutant = &quot;nox&quot;, type = &quot;site&quot;, avg.time = &quot;month&quot;) Figure 9.3: Plot of NOx xconcentrations from three sites in separate panels. Using the code above it is also possible to include several species. But what if we wanted to plot NOx concentrations across all sites in one panel? An example of how to do this is shown below. Note, in order to make referring to the columns easier, we will drop the full (long) site name and use the site code instead. ## first drop site name aq &lt;- select(aq, -site) ## now reshape the data using the tidyr package aq &lt;- pivot_wider(aq, id_cols = date, names_from = code, values_from = co:air_temp) names(aq) ## [1] &quot;date&quot; &quot;co_KC1&quot; &quot;co_MY1&quot; &quot;co_NOTT&quot; ## [5] &quot;nox_KC1&quot; &quot;nox_MY1&quot; &quot;nox_NOTT&quot; &quot;no2_KC1&quot; ## [9] &quot;no2_MY1&quot; &quot;no2_NOTT&quot; &quot;no_KC1&quot; &quot;no_MY1&quot; ## [13] &quot;no_NOTT&quot; &quot;o3_KC1&quot; &quot;o3_MY1&quot; &quot;o3_NOTT&quot; ## [17] &quot;so2_KC1&quot; &quot;so2_MY1&quot; &quot;so2_NOTT&quot; &quot;pm10_KC1&quot; ## [21] &quot;pm10_MY1&quot; &quot;pm10_NOTT&quot; &quot;pm2.5_KC1&quot; &quot;pm2.5_MY1&quot; ## [25] &quot;pm2.5_NOTT&quot; &quot;v10_KC1&quot; &quot;v10_MY1&quot; &quot;v10_NOTT&quot; ## [29] &quot;v2.5_KC1&quot; &quot;v2.5_MY1&quot; &quot;v2.5_NOTT&quot; &quot;nv10_KC1&quot; ## [33] &quot;nv10_MY1&quot; &quot;nv10_NOTT&quot; &quot;nv2.5_KC1&quot; &quot;nv2.5_MY1&quot; ## [37] &quot;nv2.5_NOTT&quot; &quot;ws_KC1&quot; &quot;ws_MY1&quot; &quot;ws_NOTT&quot; ## [41] &quot;wd_KC1&quot; &quot;wd_MY1&quot; &quot;wd_NOTT&quot; &quot;air_temp_KC1&quot; ## [45] &quot;air_temp_MY1&quot; &quot;air_temp_NOTT&quot; The final step will make columns of each site/pollutant combination e.g. nox_KC1, pm10_KC1 and so on. It is then easy to use any of these names to make the plot (with a few plotting option enhancements): timePlot(aq, pollutant = c(&quot;nox_KC1&quot;, &quot;nox_MY1&quot;, &quot;nox_NOTT&quot;), avg.time = &quot;month&quot;, group = TRUE, lty = 1, lwd = c(1, 3, 5), ylab = &quot;nox (ug/m3)&quot; ) Figure 9.4: Plot of NOx xconcentrations from three sites grouped in a single plot. An alternative way of selecting all columns containing the character ‘nox’ is to use the grep command. For example: timePlot(thedata, pollutant = names(thedata)[grep(pattern = &quot;nox&quot;, names(thedata))], avg.time = &quot;month&quot;, group = TRUE) If wind speed ws and wind direction wd are available they can be used in plots and shown as ‘wind vectors’. Plotting data in this way conveys more information in an easy-to-understand way, which works best for relatively short time periods e.g. a pollution episode lasting a few days. As an example Figure 9.5 shows the first 48 hours of NOx and NO2 data with wind arrows shown. The arrows are controlled by a list of option that control the length, shape and colour of the arrows. The maximum length of the arrow plotted is a fraction of the plot dimension with the longest arrow being scale of the plot x-y dimension. Note, if the plot size is adjusted manually by the user it should be re-plotted to ensure the correct wind angle. The list may contain other options to panel.arrows in the lattice package. Other useful options include length, which controls the length of the arrow head and angle, which controls the angle of the arrow head. Wind vector arrows can also be used with the scatterPlot function. timePlot(head(mydata, 48), pollutant = c(&quot;nox&quot;, &quot;no2&quot;), windflow = list(scale = 0.1, lwd = 2, col = &quot;turquoise4&quot;), lwd = 3, group = FALSE, ylab = &quot;concentration (ug/m3)&quot;) Figure 9.5: An example of using the windflow option in timePlot. "],
["sec-timeVariation.html", "Section10 Temporal variations 10.1 Purpose 10.2 Applications 10.3 Output", " Section10 Temporal variations 10.1 Purpose In air pollution, the variation of a pollutant by time of day and day of week can reveal useful information concerning the likely sources. For example, road vehicle emissions tend to follow very regular patterns both on a daily and weekly basis. By contrast some industrial emissions or pollutants from natural sources (e.g. sea salt aerosol) may well have very different patterns. The timeVariation function produces four plots: day of the week variation, mean hour of day variation and a combined hour of day – day of week plot and a monthly plot. Also shown on the plots is the 95% confidence interval in the mean. These uncertainty limits can be helpful when trying to determine whether one candidate source is different from another. The uncertainty intervals are calculated through bootstrap re-sampling, which will provide better estimates than the application of assumptions based on normality, particularly when there are few data available. The function can consider one or two input variables. In addition, there is the option of ‘normalising’ concentrations (or other quantities). Normalising is very useful for comparing the patterns of two different pollutants, which often cover very different ranges in concentration. Normalising is achieved by dividing the concentration of a pollutant by its mean value. Note also that any other variables besides pollutant concentrations can be considered e.g. meteorological or traffic data. There is also an option difference which is very useful for considering the difference in two time series and how they vary over different temporal resolutions. Again, bootstrap re-sampling methods are used to estimate the uncertainty of the difference in two means. Care has been taken to ensure that wind direction (wd) is vector-averaged. Less obvious though is the uncertainty in wind direction. A pragmatic approach has been adopted here that considers how wind direction changes. For example, consider the following wind directions: 10, 10, 10, 180, 180, 180\\(^\\circ\\) The standard deviation of these numbers is 93\\(^\\circ\\). However, what actually occurs is the wind direction is constant at 10\\(^\\circ\\) then switches to 180\\(^\\circ\\). In terms of changes there is a sequence of numbers: 0, 0, 170, 0, 0 with a standard deviation of 76\\(^\\circ\\). We use the latter method as a basis of calculating the 95% confidence intervals in the mean. There are also problems with simple averaging—for example, what is the average of 20 and 200\\(^\\circ\\). It can’t be known. In some situations where the wind direction is bi-modal with differences around 180\\(^\\circ\\), the mean can be ‘unstable’. For example, wind that is funnelled along a valley forcing it to be either easterly or westerly. Consider for example the mean of 0\\(^\\circ\\) and 179\\(^\\circ\\) (89.5\\(^\\circ\\)), but a small change in wind direction to 181\\(^\\circ\\) gives a mean of 270.5\\(^\\circ\\). Some care should be exercised therefore when averaging wind direction. It is always a good idea to use thewindRosefunction with type set to ‘month’ or ‘hour’. The timeVariation function is probably one of the most useful functions that can be used for the analysis of air pollution. Here are a few uses/advantages: Variations in time are one of the most useful ways of characterising air pollution for a very wide range of pollutants including local urban pollutants and tropospheric background concentrations of ozone and the like. The function works well in conjunction with other functions such as polarPlot (see Section 7), where the latter may identify conditions of interest (say a wind speed/direction range). By sub-setting for those conditions in timeVariation the temporal characteristics of a particular source could be characterised and perhaps contrasted with another subset of conditions. The function can be used to compare a wide range of variables, if available. Suggestions include meteorological e.g. boundary layer height and traffic flows. The function can be used for comparing pollutants over different sites. See Section (??) for examples of how to do this. The function can be used to compare one part of a time series with another. This is often a very powerful thing to do, particularly if concentrations are normalised. For example, there is often interest in knowing how diurnal/weekday/seasonal patterns vary with time. If a pollutant showed signs of an increase in recent years, then splitting the data set and comparing each part together can provide information on what is driving the change. Is there, for example, evidence that morning rush hour concentrations have become more important, or Sundays have become relatively more important? An example is given below using the splitByDate function. timeVariation can be used to consider the differences between two time series, which will have multiple benefits. For example, for model evaluation it can be very revealing to consider the difference between observations and modelled values over different time scales. Considering such differences can help reveal the character and some reasons for why a model departs from reality. 10.2 Applications We apply the timeVariation function to PM10 concentrations and take the opportunity to filter the data to maximise the signal from the road. The polarPlot function described in Section (Section 7) is very useful in this respect in highlighting the conditions under which different sources have their greatest impact. A subset of data is used filtering for wind speeds &gt; 3 m s-1 and wind directions from 100–270 degrees. The code used is: The results are shown in Figure 10.1. The plot shown at the top-left shows the diurnal variation of concentrations for all days. It shows for example that PM10 concentrations tend to peak around 9 am. The shading shows the 95% confidence intervals of the mean. The plot at the top-right shows how PM10 concentrations vary by day of the week. Here there is strong evidence that PM10 is much lower at the weekends and that there is a significant difference compared with weekdays. It also shows that concentrations tend to increase during the weekdays. Finally, the plot at the bottom shows both sets of information together to provide an overview of how concentrations vary. Note that the plot need not just consider pollutant concentrations. Other useful variables (if available) are meteorological and traffic flow or speed data. Often, the combination of several sets of data can be very revealing. The filter function is extremely useful in this respect. For example, if it were believed that a source had an effect under specific conditions; they can be isolated with the filter function. It is also useful if it is suspected that two or more sources are important that they can be isolated to some degree and compared. This is where the uncertainty intervals help — they provide an indication whether the behaviour of one source differs significantly from another. library(openair) library(tidyverse) timeVariation(filter(mydata, ws &gt; 3, wd &gt; 100, wd &lt; 270), pollutant = &quot;pm10&quot;, ylab = &quot;pm10 (ug/m3)&quot;) Figure 10.1: Example plot using the timeVariation function to plot PM10 concentrations at Marylebone Road. Figure 10.2 shows the function applied to concentrations of NOx, CO, NO2 and O3 concentrations. In this case the concentrations have been normalised. The plot clearly shows the markedly different temporal trends in concentration. For CO, there is a very pronounced increase in concentrations during the peak pm rush hour. The other important difference is on Sundays when CO concentrations are relatively much higher than NOx. This is because flows of cars (mostly petrol) do not change that much by day of the week, but flows of vans and HGVs (diesel vehicles) are much less on Sundays. Note, however, that the monthly trend is very similar in each case — which indicates very similar source origins. Taken together, the plots highlight that traffic emissions dominate this site for CO and NOx, but there are important difference in how these emissions vary by hour of day and day of week. Also shown in the very different behaviour of O3. Because O3 reacts with NO, concentrations of NOx and O3 tend to be anti-correlated. Note also the clear peak in O3 in April/May, which is due to higher northern hemispheric background concentrations in the spring. Even at a busy roadside site in central London this influence is clear to see. timeVariation(mydata, pollutant = c(&quot;nox&quot;, &quot;co&quot;, &quot;no2&quot;, &quot;o3&quot;), normalise = TRUE) Figure 10.2: Example plot using the timeVariation function to plot NOx, CO, NO2 and O3 concentrations at Marylebone Road. In this plot, the concentrations are normalised. Another example is splitting the data set by time. We use the splitByDate function to divide up the data into dates before January 2003 and after January 2003. This time the option difference is used to highlight how NO2 concentrations have changed over these two periods. The results are shown in Figure 10.3. There is some indication in this plot that data after 2003 seem to show more of a double peak in the diurnal plots; particularly in the morning rush hour. Also, the difference line does more clearly highlight a more substantial change over weekdays and weekends. Given that cars are approximately constant at this site each day, the change may indicate a change in vehicle emissions from other vehicle types. Given that it is known that primary NO2 emissions are known to have increased sharply from the beginning of 2003 onwards, this perhaps provides clues as to the principal cause. ## split data into two periods (see Utlities section for more details) mydata &lt;- splitByDate(mydata, dates= &quot;1/1/2003&quot;, labels = c(&quot;before Jan. 2003&quot;, &quot;After Jan. 2003&quot;)) timeVariation(mydata, pollutant = &quot;no2&quot;, group = &quot;split.by&quot;, difference = TRUE) Figure 10.3: Example plot using the timeVariation function to plot NO2 concentrations at Marylebone Road. In this plot, the concentrations are shown before and after January 2003. In the next example it is shown how to compare one subset of data of interest with another. Again, there can be many reasons for wanting to do this and perhaps the data set at Marylebone Road is not the most interesting to consider. Nevertheless, the code below shows how to approach such a problem. The scenario would be that one is interested in a specific set of conditions and it would be useful to compare that set, with another set. A good example would be from an analysis using the polarPlot function where a ‘feature’ of interest has been identified—maybe an indication of a different source. But does this potentially different source behave differently in terms of temporal variation? If it does, then maybe that provides evidence to support that it is a different source. In a wider context, this approach could be used in many different ways depending on available data. A good example is the analysis of model output where many diagnostic meteorological data are available. This is an area that will be developed. The approach here is to first make a new variable called feature' and fill it with the valueother’. A subset of data is defined and the associated locations in the data frame identified. The subset of data is then used to update the `feature’ field with a new description. This approach could be extended to some quite complex situations. There are a couple of things to note in Figure 10.2. There seems to be evidence that for easterly winds &gt; 4 m s-1 that concentrations of SO2 are lower at night. Also, there is some evidence that concentrations for these conditions are also lower at weekends. This might reflect that SO2 concentrations for these conditions tend to be dominated by tall stack emissions that have different activities to road transport sources. This technique will be returned to with different data sets in future. ## make a field called &quot;feature&quot; and fill: make all values = &quot;other&quot; mydata &lt;- mutate(mydata, feature = ifelse(ws &gt; 4 &amp; wd &gt; 0 &amp; wd &lt;= 180, &quot;easterly&quot;, &quot;other&quot;)) timeVariation(mydata, pollutant =&quot;so2&quot;, group = &quot;feature&quot;, ylab = &quot;so2 (ppb)&quot;, difference = TRUE) Figure 10.4: Example plot using the timeVariation function to plot SO2 concentrations at Marylebone Road. In this plot, the concentrations are shown for a subset of easterly conditions and everything else. Note that the uncertainty in the mean values for easterly winds is greater than other. This is mostly because the sample size is much lower for easterly compared with other. By default timeVariation shows the mean variation in different temporal components and the 95% confidence interval in the mean. However, it is also possible to show how the data are distributed by using a different option for statistic. When statistic = \"median\" the median line is shown together with the 25/75th and 5/95th quantile values. Users can control the quantile values shown be setting the conf.int. For example, conf.int = c(0.25, 0.99) will show the median, 25/75th and 1/99th quantile values. The statistic = \"median\" option is therefore very useful for showing how the data are distributed — somewhat similar to a box and whisker plot. Note that it is expected that only one pollutant should be shown when statistic = \"median\" is used due to potential over-plotting; although the function will display several species of required. An example is shown in Figure 10.5 for PM10 concentrations. timeVariation(mydata, pollutant = &quot;pm10&quot;, statistic = &quot;median&quot;, col = &quot;firebrick&quot;) Figure 10.5: Example plot using the timeVariation function to show the variation in the median, 25/75th and 5/95th quantile values for PM10. The shading shows the extent to the 25/75th and 5/95th quantiles. 10.3 Output The timeVariation function produces several outputs that can be used for further analysis or plotting. It is necessary to read the output into a variable for further processing. The code below shows the different objects that are returned and the code shows how to access them. myOutput &lt;- timeVariation(mydata, pollutant = &quot;so2&quot;) ## show the first part of the day/hour variation ## note that value = mean, and Upper/Lower the 95% confid. intervals head(myOutput$data$day.hour) ## # A tibble: 6 x 8 ## # Groups: ci [1] ## variable wkday hour default Mean Lower Upper ci ## &lt;fct&gt; &lt;ord&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 so2 Monday 0 01 January 1998 to 23 June 2… 2.93 2.63 3.19 0.95 ## 2 so2 Tuesday 0 01 January 1998 to 23 June 2… 3.21 3.02 3.46 0.95 ## 3 so2 Wednesday 0 01 January 1998 to 23 June 2… 3.35 3.09 3.59 0.95 ## 4 so2 Thursday 0 01 January 1998 to 23 June 2… 3.22 2.98 3.56 0.95 ## 5 so2 Friday 0 01 January 1998 to 23 June 2… 3.64 3.39 3.90 0.95 ## 6 so2 Saturday 0 01 January 1998 to 23 June 2… 4.25 4.01 4.58 0.95 ## can make a new data frame of this data e.g. day.hour &lt;- myOutput$data$day.hour head(day.hour) ## # A tibble: 6 x 8 ## # Groups: ci [1] ## variable wkday hour default Mean Lower Upper ci ## &lt;fct&gt; &lt;ord&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 so2 Monday 0 01 January 1998 to 23 June 2… 2.93 2.63 3.19 0.95 ## 2 so2 Tuesday 0 01 January 1998 to 23 June 2… 3.21 3.02 3.46 0.95 ## 3 so2 Wednesday 0 01 January 1998 to 23 June 2… 3.35 3.09 3.59 0.95 ## 4 so2 Thursday 0 01 January 1998 to 23 June 2… 3.22 2.98 3.56 0.95 ## 5 so2 Friday 0 01 January 1998 to 23 June 2… 3.64 3.39 3.90 0.95 ## 6 so2 Saturday 0 01 January 1998 to 23 June 2… 4.25 4.01 4.58 0.95 All the numerical results are given by: myOutput$data$day.hour ## are the weekday and hour results myOutput$data$hour ## are the diurnal results myOutput$data$day ## are the weekday results myOutput$data$month ## are the monthly results It is also possible to plot the individual plots that make up the (four) plots produced by timeVariation: ## just the diurnal variation plot(myOutput, subset = &quot;hour&quot;) ## day and hour plot(myOutput, subset = &quot;day.hour&quot;) ## weekday variation plot(myOutput, subset = &quot;day&quot;) ## monthly variation plot(myOutput, subset = &quot;month&quot;) "],
["sec-timeProp.html", "Section11 Time proportion plots 11.1 Background 11.2 Examples", " Section11 Time proportion plots 11.1 Background The timeProp (‘time proportion’) function shows time series plots as stacked bar charts. For a particular time, proportions of a chosen variable are shown as a stacked bar chart. The different categories in the bar chart are made up from a character or factor variable in a data frame. The function is primarily developed to support the plotting of cluster analysis output from polarCluster (see Section 7.6 and trajCluster (see Section 17.6 that consider local and regional (back trajectory) cluster analysis respectively. However, the function has more general use for understanding time series data. In order to plot time series in this way, some sort of time aggregation is needed, which is controlled by the option avg.time. The plot shows the value of pollutant on the y-axis (averaged according to avg.time). The time intervals are made up of bars split according to proportion. The bars therefore show how the total value of `pollutant} is made up for any time interval. 11.2 Examples An example of the timeProp function is shown in Figure 11.1. In this example SO2 concentrations are considered for 2003 (using the selectByDate function). The averaging period is set to 3~days and the mean concentration is plotted and the proportion contribution by wind sector is given. Other options are chosen to place the key at the top and choose the number of columns used in the key. It is apparent from Figure 11.1 that the highest SO2 concentrations are dominated by winds from an easterly sector, but actually occur throughout the year. timeProp(selectByDate(mydata, year = 2003), pollutant = &quot;so2&quot;, avg.time = &quot;3 day&quot;, proportion = &quot;wd&quot;, date.breaks = 10, key.position = &quot;top&quot;, key.columns = 8, ylab = &quot;so2 (ug/m3)&quot;) Figure 11.1: timeProp plot for SO2 concentrations in 2003. The data are categorised into 8 wind sectors for 3-day averages. Note that proportion can be an existing categorical (i.e. factor or character) variable in a data frame. If a numeric variable is supplied, then it is typically cut into four quantile levels. So, for example, the plot below would show four intervals of wind speed, which would help show the wind speed conditions that control high SO2 concentration — and importantly, when they occur. An example of using timeProp with a continuous variable is shown in Figure 11.2. In this case the wind speed values are split into 3 quantile levels. The number of quantiles used is determined by the option n.levels. This approach can be used for any numeric variables. timeProp(selectByDate(mydata, year = 2003), pollutant = &quot;so2&quot;, avg.time = &quot;3 day&quot;, n.levels = 3, cols = &quot;viridis&quot;, proportion = &quot;ws&quot;, date.breaks = 10, key.position = &quot;top&quot;, key.columns = 3) Figure 11.2: timeProp plot for SO2 concentrations in 2003. The data are categorised into 4 wind speed categories for 3-day averages. One of the key uses of timeProp is to post-process cluster analysis data. Users should consider the uses of timeProp for cluster analysis shown in Section 7.6 and Section 17.6. In both these cases the cluster analysis yields a categorical output directly i.e. cluster, which lends itself to analysis using timeProp. "],
["sec-trendLevel.html", "Section12 Trend heat maps 12.1 Another way of representing trends 12.2 Examples", " Section12 Trend heat maps 12.1 Another way of representing trends The trendLevel function provides a way of rapidly showing a large amount of data in a condensed way. It is particularly useful for plotting the level of a value against two categorical variables. These categorical variables can pre-exist in a data set or be made on the fly using openair. By default it will show the mean value of a variable against two categorical variables but can also consider a wider range of statistics e.g. the maximum, frequency, or indeed a user-defined function. The function is much more flexible than this by showing temporal data and can plot ‘heat maps’ in many flexible ways. Both continuous colour scales and user-defined categorical scales can be used. The trendLevel function shows how the value of a variable varies according to intervals of two other variables. The \\(x\\) and \\(y\\) variables can be categorical (factor or character) or numeric. The third variable (\\(z\\)) must be numeric and is coloured according to its value. Despite being called trendLevel the function is flexible enough to consider a wide range of plotting variables. If the \\(x\\) and \\(y\\) variables are not categorical they are made so by splitting the data into quantiles (using cutData). Furthermore, the user can supply as many levels as they wish for the quantile using the option n.levels. Remeber also there are lots of built-in options for x or y based on temporal variations (see Section 19.2) e.g. “month” (the default), “week”, “daylight” and so on. 12.2 Examples The standard output from trendLevel is shown in Figure 12.1, which shows the variation in NOx concentrations by year and hour of the day. By default the function will use “month” for the x-axis and “hour” for the y-axis. library(openair) library(tidyverse) trendLevel(mydata, pollutant = &quot;nox&quot;) Figure 12.1: Example output from trendLevel trendLevel(mydata, pollutant = &quot;nox&quot;, y = &quot;wd&quot;, border = &quot;white&quot;, cols = &quot;jet&quot;) Figure 12.2: trendLevel output with wind direction as y. Figure 12.3 indicates that the highest NOx concentrations most strongly associate with wind sectors about 200 degrees, appear to be decreasing over the years, but do not appear to associate with an SO2 rich NOx source. Using type = \"so2\" would have conditioned by absolute SO2 concentration. As both a moderate contribution from an SO2 rich source and a high contribution from an SO2 poor source could generate similar SO2 concentrations, such conditioning can sometimes blur interpretations. The use of this type of ‘over pollutant’ ratio reduces this blurring by focusing conditioning on cases when NOx concentrations (be they high or low) associate with relatively high or low SO2 concentrations. ## new field: so2/nox ratio mydata &lt;- mutate(mydata, ratio = so2 / nox) ## condition by mydata$new trendLevel(mydata, &quot;nox&quot;, x = &quot;year&quot;, y = &quot;wd&quot;, type = &quot;ratio&quot;, cols = &quot;inferno&quot;) Figure 12.3: trendLevel output with SO2 : NOx ratio type conditioning. The plot can be used in much more flexible ways. Here are some examples (not plotted): A plot of mean O3 concentration shown by season and by daylight/nighttime hours. trendLevel(mydata, x = &quot;season&quot;, y = &quot;daylight&quot;, pollutant = &quot;o3&quot;) Or by season and hour of the day: trendLevel(mydata, x = &quot;season&quot;, y = &quot;hour&quot;, pollutant = &quot;o3&quot;, cols = &quot;increment&quot;) How about NOx versus NO2 coloured by the concentration of O3? scatterPlot could also be used to produce such a plot. However, one interesting difference with using trendLevel is that the data are split into quantiles where equal numbers of data exist in each interval. This approach can make it a bit easier to see the underlying relationship between variables. A scatter plot may have too much data to be clear and also outliers (or regions with relatively few data) that make it harder to see what is going on. The plot generated by the command below makes it a bit easier to see that it is the higher quantiles of NO2 that are associated with higher O3 concentration (as well as low NOx and NO2 concentrations). trendLevel(mydata, x = &quot;nox&quot;, y = &quot;no2&quot;, pollutant = &quot;o3&quot;, border = &quot;white&quot;, n.levels = 30, statistic = &quot;max&quot;, limits = c(0, 50)) Figure 12.4: trendLevel showing NOx against NO2, coloured by the concentration of O3. The plot can also be shown by wind direction sector, this time showing how O3 varies by weekday, wind direction sector and NOx quantile. trendLevel(mydata, x = &quot;nox&quot;, y = &quot;weekday&quot;, pollutant = &quot;o3&quot;, border = &quot;white&quot;, n.levels = 10, statistic = &quot;max&quot;, limits = c(0, 50), type = &quot;wd&quot;) By default trendLevel subsamples the plotted pollutant data by the supplied x, y and type parameters and in each case calculates the mean. The option statistic has always let you apply other statistics. For example, trendLevel also calculated the maximum via the option statistic = \"max\". The user may also use their own statistic function. As a simple example, consider the above plot which summarises by mean. This tells us about average concentrations. It might also be useful to consider a particular percentile of concentrations. This can be done by defining one’s own function as shown in Figure 12.5. ## function to estimate 95th percentile percentile &lt;- function(x) quantile(x, probs = 0.95, na.rm = TRUE) ## apply to present plot trendLevel(mydata, &quot;nox&quot;, x = &quot;year&quot;, y = &quot;wd&quot;, type = &quot;ratio&quot;, cols = &quot;viridis&quot;, statistic = percentile) Figure 12.5: trendLevel using locally defined statistic. This type of flexibility really opens up the potential of the function as a screening tool for the early stages of data analysis. Increased control of x, y, type and statistic allow you to very quick explore your data and develop an understanding of how different parameters interact. Patterns in trendLevel plots can also help to direct your openair analysis. For example, possible trends in data conditioned by year would suggest that functions like smoothTrend or TheilSen could provide further insight. Likewise, windRose or polarPlot could be useful next steps if wind speed and direct conditioning produces interesting features. However, perhaps most interestingly, novel conditioning or the incorporation of novel parameters in this type of highly flexible function provides a means of developing new data visualisation and analysis methods. trendLevel can also be used with user defined discrete colour scales as shown in Figure ??. In this case the default \\(x\\) and \\(y\\) variables are chosen (month and hour) split by type (year). trendLevel(mydata, pollutant = &quot;no2&quot;, x = &quot;week&quot;, border = &quot;white&quot;, statistic = &quot;max&quot;, breaks = c(0, 50, 100, 500), labels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;), cols = c(&quot;forestgreen&quot;, &quot;yellow&quot;, &quot;red&quot;), key.position = &quot;top&quot;) Figure 12.6: trendLevel plot for maximum NO2 concentrations using a user-defined discrete colour scale. "],
["sec-scatterPlot.html", "Section13 Scatter plots 13.1 Purpose 13.2 Examples", " Section13 Scatter plots 13.1 Purpose Scatter plots are extremely useful and a very commonly used analysis technique for considering how variables relate to one another. R does of course have many capabilities for plotting data in this way. However, it can be tricky to add linear relationships, or split scatter plots by levels of other variables etc. The purpose of the scatterPlot function is to make it straightforward to consider how variables are related to one another in a way consistent with other openair functions. We have added several capabilities that can be used just by setting different options, some of which are shown below. There is less need for this function now that ggplot2 is available, but it still has some benefits for intercative use. A smooth fit is automatically added to help reveal the underlying relationship between two variables together with the estimated 95% confidence intervals of the fit. This is in general an extremely useful thing to do because it helps to show the (possibly) non-linear relationship between variables in a very robust way — or indeed whether the relationship is linear. It is easy to add a linear regression line. The resulting equation is shown on the plot together with the R\\(^2\\) value. For large data sets there is the possibility to `bin’ the data using hexagonal binning or kernel density estimates. This approach is very useful when there is considerable over-plotting. It is easy to show how two variables are related to one another dependent on levels of a third variable. This capability is very useful for exploring how different variables depend on one another and can help reveal the underlying important relationships. A plot of two variables can be colour-coded by a continuous colour scale of a third variable. It can handle date/time x-axis formats to provide an alternative way of showing time series, which again can be colour-coded by a third variable. The scatterPlot function isn’t really specific to atmospheric sciences, in the same way as other plots. It is more a function for convenience, written in a style that is consistent with other openair functions. Nevertheless, along with the timePlot function they do form an important part of openair because of the usefulness of understanding show variables relate to one another. Furthermore, there are many options to make it easy to explore data in an interactive way without worrying about processing data or formatting plots. 13.2 Examples We provide a few examples of use and as usual, users are directed towards the help pages (type ?scatterPlot) for more extensive examples. First we select a subset of data (2003) using the openair selectByDate function and plot NOx vs. NO2 Figure (13.1). data2003 &lt;- selectByDate(mydata, year = 2003) scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;) Figure 13.1: Scatter plot of hourly NOx vs. NO2 at Marylebone Road for 2003. Often with several years of data, points are over-plotted and it can be very difficult to see what the underlying relationship looks like. One very effective method to use in these situations is to ‘bin’ the data and to colour the intervals by the number of counts of occurrences in each bin. There are various ways of doing this, but `hexagonal binning’ is particularly effective because of the way hexagons can be placed next to one another.4 To use hexagonal binning it will be necessary to install the hexbin package: 13.2.1 Hexaganol binning Now it should be possible to make the plot by setting the method option to method = \"hexbin\", as shown in Figure 13.2. The benefit of hexagonal binning is that it works equally well with enormous data sets e.g. several million records. In this case Figure 13.2 provides a clearer indication of the relationship between NOx and NO2 than Figure 13.1 because it reveals where most of the points lie, which is not apparent from Figure 13.1. Note that For method = \"hexbin\" it can be useful to transform the scale if it is dominated by a few very high values. This is possible by supplying two functions: one that that applies the transformation and the other that inverses it. For log scaling for example (the default), trans = function(x) log(x) and inv = function(x) exp(x). For a square root transform use trans = sqrt and inv = function(x) x\\^2. To not apply any transformation trans = NULL and inv = NULL should be used. scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;, method = &quot;hexbin&quot;, col= &quot;jet&quot;) Figure 13.2: Scatter plot of hourly NOx vs. NO2 at Marylebone Road using hexagonal binning. The number of occurrences in each bin is colour-coded (not on a linear scale). It is now possible to see where most of the data lie and a better indication of the relationship between NOx and NO2 is revealed. Note that when method = \"hexbin\" there are various options that are useful e.g. a border around each bin and the number of bins. For example, to place a grey border around each bin and set the bin size try: scatterPlot(mydata, x = &quot;nox&quot;, y = &quot;no2&quot;, method = &quot;hexbin&quot;, col = &quot;jet&quot;, border = &quot;grey&quot;, xbin = 15) The hexagonal binning and other binning methods are useful but often the choice of bin size is somewhat arbitrary. Another useful approach is to use a kernel density estimate to show where most points lie. This is possible in scatterPlot with the method = \"density\" option. Such a plot is shown in Figure 13.3. scatterPlot(selectByDate(mydata, year = 2003), x = &quot;nox&quot;, y = &quot;no2&quot;, method = &quot;density&quot;, cols = &quot;jet&quot;) ## (loaded the KernSmooth namespace) Figure 13.3: Scatter plot of hourly NOx vs. NO2 at Marylebone Road using a kernel density estimate to show where most of the points lie. The intensity is a measure of how many points lie in a unit area of NOx and NO2 concentration. Sometimes it is useful to consider how the relationship between two variables varies by levels of a third. In openair this approach is possible by setting the option type. When type is another numeric variables, four plots are produced for different quantiles of that variable. We illustrate this point by considering how the relationship between NOx and NO2 varies with different levels of O3. We also take the opportunity to not plot the smooth line, but plot a linear fit instead and force the layout to be a 2 by 2 grid. scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;, type = &quot;o3&quot;, smooth = FALSE, linear = TRUE, layout = c(2, 2)) Figure 13.4: Scatter plot of hourly NOx vs. NO2 at Marylebone Road by different levels of O3. Below is an extended example that brings together data manipulation, refined plot options and linear fitting of two variables with NOx. The aim is to plot the weekly concentration of NOx against PM10 and PM2.5 and fit linear equations to both relationships. To do this we need the \\(x\\) variable as NOx and the \\(y\\) variable as PM10 or PM2.5, which means we also need a column that will act as a grouping column i.e. identifies whether the \\(y\\) is PM10 or PM2.5. # load the packages we need library(tidyverse) # select the variables of interest subdat &lt;- select(mydata, date, nox, pm10, pm25) # calculate weekly averages subdat &lt;- timeAverage(subdat, avg.time = &quot;week&quot;) # reshape so we have two variable columns subdat &lt;- pivot_longer(subdat, cols = c(pm10, pm25), names_to = &quot;pollutant&quot;) head(subdat) ## # A tibble: 6 x 4 ## date nox pollutant value ## &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1997-12-29 00:00:00 128. pm10 21.8 ## 2 1997-12-29 00:00:00 128. pm25 NaN ## 3 1998-01-05 00:00:00 189. pm10 33.6 ## 4 1998-01-05 00:00:00 189. pm25 NaN ## 5 1998-01-12 00:00:00 203. pm10 29.1 ## 6 1998-01-12 00:00:00 203. pm25 NaN Now we will plot weekly NOx versus PM10 and PM2.5 and fit a linear equation to both — and adjust some of the symbols (shown in Figure 13.5. scatterPlot(subdat, x = &quot;nox&quot;, y = &quot;value&quot;, group = &quot;pollutant&quot;, pch = 21:22, cex = 1.6, fill = c(&quot;dodgerblue&quot;, &quot;tomato&quot;), col = &quot;white&quot;, linear = TRUE, xlab = &quot;nox (ppb)&quot;, ylab = &quot;PM concentration (ug/m3)&quot;) Figure 13.5: Scatter plot of weekly NOx vs. PM10 and PM2.5 at Marylebone Road with linear equations shown and plot symbols modified. To gain a better idea of where the data lie and the linear fits, adding some transparency helps: scatterPlot(subdat, x = &quot;nox&quot;, y = &quot;value&quot;, group = &quot;variable&quot;, pch = 21:22, cex = 1.6, fill = c(&quot;dodgerblue&quot;, &quot;tomato&quot;), col = &quot;white&quot;, linear = TRUE, xlab = &quot;nox (ppb)&quot;, ylab = &quot;PM concentration (ug/m3)&quot;, alpha = 0.2) The above example will also work with type. For example, to consider how NOx againts PM10 and PM2.5 varies by season: scatterPlot(subdat, x = &quot;nox&quot;, y = &quot;value&quot;, group = &quot;variable&quot;, pch = 21:22, cex = 2, fill = c(&quot;dodgerblue&quot;, &quot;tomato&quot;), col = &quot;white&quot;, linear = TRUE, xlab = &quot;nox (ppb)&quot;, ylab = &quot;PM concentration (ug/m3)&quot;, type = &quot;season&quot;) Finally, we show how to plot a continuous colour scale for a third numeric variable setting the value of z to the third variable. Figure 13.6 shows again the relationship between NOx and NO2 but this time colour-coded by the concentration of O3. We also take the opportunity to split the data into seasons and weekday/weekend by setting type = c(\"season\", \"weekend\"). There is an enormous amount of information that can be gained from plots such as this. Differences between weekdays and the weekend can highlight changes in emission sources, splitting by seasons can show seasonal influences in meteorology and background O3 and colouring the data by the concentration of O3 helps to show how O3 concentrations affect NO2 concentrations. For example, consider the summertime-weekday panel where it clearly shows that the higher NO2 concentrations are associated with high O3 concentrations. Indeed there are some hours where NO2 is $&gt;\\(100~ppb at quite low concentrations of NO~x~ (\\)$200 ppb). It would also be interesting instead of using O3 concentrations from Marylebone Road to use O3 from a background site. Figure 13.6 was very easily produced but contains a huge amount of useful information showing the relationship between NOx and NO2 dependent upon the concentration of O3, the season and the day of the week. There are of course numerous other plots that are equally easily produced. scatterPlot(data2003, x = &quot;nox&quot;, y = &quot;no2&quot;, z = &quot;o3&quot;, type = c(&quot;season&quot;, &quot;weekend&quot;), limits = c(0, 30)) Figure 13.6: Scatter plot of hourly NOx vs. NO2 at Marylebone Road by different levels of O3 split by season and weekday-weekend. Figure 13.7 shows that scatterPlot can also handles dates on the x-axis; in this case shown for SO2 concentrations coloured by wind direction for August 2003. scatterPlot(selectByDate(data2003, month = 8), x = &quot;date&quot;, y = &quot;so2&quot;, z = &quot;wd&quot;) Figure 13.7: Scatter plot of date vs. SO2- at Marylebone Road by different levels of wind direction for August 2003. Similar to Section 9, scatterPlot can also plot wind vector arrows if wind speed and wind direction are available in the data frame. Figure 13.8 shows an example of using the windflow option. The Figure also sets many other options including showing the concentration of O3 as a colour, setting the colour scale used and selecting a few days of interest using the selectByDate function. Figure 13.8 shows that when the wind direction changes to northerly, the concentration of NO2 decreases and that of O3 increases. scatterPlot(selectByDate(mydata, start = &quot;1/6/2001&quot;, end = &quot;5/6/2001&quot;), x = &quot;date&quot;, y = &quot;no2&quot;, z = &quot;o3&quot;, col = &quot;increment&quot;, windflow = list(scale = 0.15), key.footer = &quot;o3\\n (ppb)&quot;, main = NULL, ylab = &quot;no2 (ppb)&quot;) Figure 13.8: Scatter plot of date vs. NO2 with the colour scale representing O3. The wind flow vectors are also shown. In fact it is not possible to have a shape with more than 6 sides that can be used to forma a lattice without gaps.↩︎ "],
["sec-calendarPlot.html", "Section14 Calendar plots 14.1 Purpose 14.2 Calendar examples", " Section14 Calendar plots 14.1 Purpose Sometimes it is useful to visualise data in a familiar way. Calendars are the obvious way to represent data for data on the time scale of days or months. The calendarPlot function provides an effective way to visualise data in this way by showing daily concentrations laid out in a calendar format. The concentration of a species is shown by its colour. The data can be shown in different ways. By default, calendarPlot overlays the day of the month. However, if wind speed and wind direction are available then an arrow can be shown for each day giving the vector-averaged wind direction. In addition, the arrow can be scaled according to the wind speed to highlight both the direction and strength of the wind on a particular day, which can help show the influence of meteorology on pollutant concentrations. calendarPlot can also show the daily mean concentration as a number on each day and can be extended to highlight those conditions where daily mean (or maximum etc.) concentrations are above a particular threshold. This approach is useful for highlighting daily air quality limits e.g. when the daily mean concentration is greater than 50 \\(\\mu\\)g m-3. The calendarPlot function can also be used to plot categorical scales. This is useful for plotting concentrations expressed as an air quality index i.e. intervals of concentrations that are expressed in ways like ‘very good’, ‘good’, ‘poor’ and so on. 14.2 Calendar examples The function is called in the usual way. As a minimum, a data frame, pollutant and year is required. So to show O3 concentrations for each day in 2003 (Figure 14.1). Note that if year is not supplied the full data set will be used. library(openair) calendarPlot(mydata, pollutant = &quot;o3&quot;, year = 2003) Figure 14.1: calendarPlot for O3 concentrations in 2003. It is sometimes useful to annotate the plots with other information. It is possible to show the daily mean wind angle, which can also be scaled to wind speed. The idea here being to provide some information on meteorological conditions on each day. Another useful option is to set annotate = \"value\" in which case the daily concentration will be shown on each day. Furthermore, it is sometimes useful to highlight particular values more clearly. For example, to highlight daily mean PM10 concentrations above 50 \\(\\mu\\)g m-3. This is where setting lim (a concentration limit) is useful. In setting lim the user can then differentiate the values below and above lim by colour of text, size of text and type of text e.g. plain and bold. Figure 14.2 highlights those days where PM10 concentrations exceed 50 \\(\\mu\\)g m-3 by making the annotation for those days bigger, bold and orange. Plotting the data in this way clearly shows the days where PM10 &gt; 50 \\(\\mu\\)g m-3. Other openair functions can be used to plot other statistics. For example, rollingMean could be used to calculate rolling 8-hour mean O3 concentrations. Then, calendarPlot could be used with statistic = \"max\" to show days where the maximum daily rolling 8-hour mean O3 concentration is greater than a certain threshold e.g. 100 or 120~\\(\\mu\\)g m-3. calendarPlot(mydata, pollutant = &quot;pm10&quot;, year = 2003, annotate = &quot;value&quot;, lim =50, cols = &quot;Purples&quot;, col.lim = c(&quot;black&quot;, &quot;orange&quot;), layout = c(4, 3)) Figure 14.2: calendarPlot for PM10 concentrations in 2003 with annotations highlighting those days where the concentration of PM10 &gt;50~\\(\\mu\\)g m-3. The numbers show the PM10 concentration in \\(\\mu\\)g m-3. To show wind angle, scaled to wind speed (Figure 14.3). calendarPlot(mydata, pollutant = &quot;o3&quot;, year = 2003, annotate = &quot;ws&quot;) Figure 14.3: calendarPlot for O3 concentrations in 2003 with annotations showing wind angle scaled to wind speed i.e. the longer the arrow, the higher the wind speed. It shows for example high O3 concentrations on the 17 and 18th of April were associated with strong north-easterly winds Note again that selectByDate can be useful. For example, to plot select months: calendarPlot(selectByDate(mydata, year = 2003, month = c(&quot;jun&quot;, &quot;jul&quot;, &quot;aug&quot;)), pollutant = &quot;o3&quot;, year = 2003) Figure 14.4 shows an example of plotting data with a categorical scale. In this case the options labels and breaks have been used to define concentration intervals and their descriptions. Note that breaks needs to be one longer than labels. In the example in Figure 14.4 the first interval (‘Very low’) is defined as concentrations from 0 to 50 (ppb), ‘Low’ is 50 to 100 and so on. Note that the upper value of breaks should be a number greater than the maximum value contained in the data to ensure that it is encompassed. In the example given in Figure 14.4 the maximum daily concentration is plotted. These types of plots are very useful for considering national or international air quality indexes. calendarPlot(mydata, pollutant = &quot;no2&quot;, year = 2003, breaks = c(0, 50, 100, 150, 1000), labels = c(&quot;Very low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Very High&quot;), cols = &quot;increment&quot;, statistic = &quot;max&quot;) Figure 14.4: calendarPlot for NO2 concentrations in 2003 with a user-defined categorical scale. The user can explicitly set each colour interval: calendarPlot(mydata, pollutant = &quot;no2&quot;, year = 2003, breaks = c(0, 50, 100, 150, 1000), labels = c(&quot;Very low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Very High&quot;), cols = c(&quot;lightblue&quot;, &quot;forestgreen&quot;, &quot;yellow&quot;, &quot;red&quot;), statistic = &quot;max&quot;) Note that in the case of categorical scales it is possible to define the breaks and labels first and then make the plot. For example: breaks &lt;- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) labels &lt;- c(&quot;Low.1&quot;, &quot;Low.2&quot;, &quot;Low.3&quot;, &quot;Moderate.4&quot;, &quot;Moderate.5&quot;, &quot;Moderate.6&quot;, &quot;High.7&quot;, &quot;High.8&quot;, &quot;High.9&quot;, &quot;Very High.10&quot;) calendarPlot(mydata, pollutant = &quot;no2&quot;, year = 2003, breaks = breaks, labels = labels, cols = &quot;jet&quot;, statistic = &quot;max&quot;) It is also possible to first use rollingMean to calculate statistics. For example, if one was interested in plotting the maximum daily rolling 8-hour mean concentration, the data could be prepared and plotted as follows. ## makes a new field &#39;rolling8o3&#39; dat &lt;- rollingMean(mydata, pollutant = &quot;o3&quot;, hours = 8) breaks &lt;- c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) labels &lt;- c(&quot;Low.1&quot;, &quot;Low.2&quot;, &quot;Low.3&quot;, &quot;Moderate.4&quot;, &quot;Moderate.5&quot;, &quot;Moderate.6&quot;, &quot;High.7&quot;, &quot;High.8&quot;, &quot;High.9&quot;, &quot;Very High.10&quot;) calendarPlot(dat, pollutant = &quot;rolling8o3&quot;, year = 2003, breaks = breaks, labels = labels, cols = &quot;jet&quot;, statistic = &quot;max&quot;) The UK has an air quality index for O3, NO2, PM10 and described in detail at (http://uk-air.defra.gov.uk/air-pollution/daqi) and COMEAP (2011). The index is most relevant to air quality forecasting, but is used widely for public information. Most other countries have similar indexes. Note that the indexes are calculated for different averaging times dependent on the pollutant: rolling 8-hour mean for O3, hourly means for NO2 and a fixed 24-hour mean for PM10 and PM2.5. In the code below the labels and breaks are defined for each pollutant to make it easier to use the index in the calendarPlot function. ## the labels - same for all species labels &lt;- c(&quot;1 - Low&quot;, &quot;2 - Low&quot;, &quot;3 - Low&quot;, &quot;4 - Moderate&quot;, &quot;5 - Moderate&quot;, &quot;6 - Moderate&quot;, &quot;7 - High&quot;, &quot;8 - High&quot;, &quot;9 - High&quot;, &quot;10 - Very High&quot;) o3.breaks &lt;-c(0, 34, 66, 100, 121, 141, 160, 188, 214, 240, 500) no2.breaks &lt;- c(0, 67, 134, 200, 268, 335, 400, 468, 535, 600, 1000) pm10.breaks &lt;- c(0, 17, 34, 50, 59, 67, 75, 84, 92, 100, 1000) pm25.breaks &lt;- c(0, 12, 24, 35, 42, 47, 53, 59, 65, 70, 1000) Remember it is necessary to use the correct averaging time. Assuming data are imported using importAURN or similar, then the units will be in \\(\\mu\\)g m-3 — if not the user should ensure this is the case. Note that rather than showing the day of the month (the default), annotate = \"value\" can be used to show the actual numeric value on each day. In this way, the colours represent the categorical interval the concentration on a day corresponds to and the actual value itself is shown. ## import test data dat &lt;- importAURN(site = &quot;kc1&quot;, year = 2010) ## no2 index example calendarPlot(dat, year = 2010, pollutant = &quot;no2&quot;, labels = labels, breaks = no2.breaks, statistic = &quot;max&quot;, cols = &quot;jet&quot;) ## for PM10 or PM2.5 we need the daily mean concentration calendarPlot(dat, year = 2010, pollutant = &quot;pm10&quot;, labels = labels, breaks = pm10.breaks, statistic = &quot;mean&quot;, cols = &quot;jet&quot;) ## for ozone, need the rolling 8-hour mean dat &lt;- rollingMean(dat, pollutant = &quot;o3&quot;, hours = 8) calendarPlot(dat, year = 2010, pollutant = &quot;rolling8o3&quot;, labels = labels, breaks = o3.breaks, statistic = &quot;max&quot;, cols = &quot;jet&quot;) References "],
["sec-TheilSen.html", "Section15 Theil-Sen trends 15.1 Trend estimates 15.2 Example trend analysis", " Section15 Theil-Sen trends 15.1 Trend estimates Calculating trends for air pollutants is one of the most important and common tasks that can be undertaken. Trends are calculated for all sorts of reasons. Sometimes it is useful to have a general idea about how concentrations might have changed. On other occasions a more definitive analysis is required; for example, to establish statistically whether a trend is significant or not. The whole area of trend calculation is a complex one and frequently trends are calculated with little consideration as to their validity. Perhaps the most common approach is to apply linear regression and not think twice about it. However, there can be many pitfalls when using ordinary linear regression, such as the assumption of normality, autocorrelation etc. One commonly used approach for trend calculation in studies of air pollution is the non-parametric Mann-Kendall approach (Hirsch, Slack, and Smith 1982). Wilcox (2010) provides an excellent case for using ‘modern methods’ for regression including the benefits of non-parametric approaches and bootstrap simulations. Note also that the all the regression parameters are estimated through bootstrap resampling. The Theil-Sen method dates back to 1950, but the basic idea pre-dates 1950 (Theil 1950; Sen 1968). It is one of those methods that required the invention of fast computers to be practical. The basic idea is as follows. Given a set of \\(n\\) \\(x\\), \\(y\\) pairs, the slopes between all pairs of points are calculated. Note, the number of slopes can increase by \\(\\approx\\) \\(n^2\\) so that the number of slopes can increase rapidly as the length of the data set increases. The Theil-Sen estimate of the slope is the median of all these slopes. The advantage of the using the Theil-Sen estimator is that it tends to yield accurate confidence intervals even with non-normal data and heteroscedasticity (non-constant error variance). It is also resistant to outliers — both characteristics can be important in air pollution. As previously mentioned, the estimates of these parameters can be made more robust through bootstrap-resampling, which further adds to the computational burden, but is not an issue for most time series which are expressed either as monthly or annual means. Bootstrap resampling also provides the estimate of \\(p\\) for the slope. An issue that can be very important for time series is dependence or autocorrelation in the data. Normal (in the statistical sense) statistics assume that data are independent, but in time series this is rarely the case. The issue is that neighbouring data points are similar to one another (correlated) and therefore not independent. Ignoring this dependence would tend to give an overly optimistic impression of uncertainties. However, taking account of it is far from simple. A discussion of these issues is beyond the aims of this report and readers are referred to standard statistical texts on the issue. In openair we follow the suggestion of Kunsch (1989) of setting the block length to \\(n^{1/3}\\) where n is the length of the time series. There is a temptation when considering trends to use all the available data. Why? Often it is useful to consider specific periods. For example, is there any evidence that concentrations of have decreased since 2000? Clearly, the time period used depends on both the data and the questions, but it is good to be aware that considering subsets of data can be very insightful. Another aspect is that almost all trends are shown as mean concentration versus time; typically by year. Such analyses are very useful for understanding how concentrations have changed through time and for comparison with air quality limits and regulations. However, if one is interested in understanding why trends are as they are, it can be helpful to consider how concentrations vary in other ways. The trend functions in openair do just this. Trends can be plotted by day of the week, month, hour of the day, by wind direction sector and by different wind speed ranges. All these capabilities are easy to use and their effectiveness will depend on the situation in question. One of the reasons that trends are not considered in these many different ways is that there can be a considerable overhead in carrying out the analysis, which is avoided by using these functions. Few, for example, would consider a detailed trend analysis by hour of the day, ensuring that robust statistical methods were used and uncertainties calculated. However, it can be useful to consider how concentrations vary in this way. It may be, for example, that the hours around midday are dominated by heavy vehicle emissions rather than by cars — so is the trend for a pollutant different for those hours compared with say, hours dominated by other vehicle types? Similarly, a much more focussed trend analysis can be done by considering different wind direction, as this can help isolate different source influences. The TheilSen function is typically used to determine trends in pollutant concentrations over several years. However, it can be used to calculate the trend in any numeric variable. It calculates monthly mean values from daily, hourly or higher time resolution data, as well as working directly with monthly means. Whether it is meaningful to calculate trends over shorter periods of time (e.g. 2 years) depends very much on the data. It may well be that statistically significant trends can be detected over relatively short periods but it is another matter whether it matters. Because seasonal effects can be important for monthly data, there is the option to deseasonalise the data first. The timeVariation function are both useful to determine whether there is a seasonal cycle that should be removed. Note also that the symbols shown next to each trend estimate relate to how statistically significant the trend estimate is: \\(p\\) \\(&lt;\\) 0.001 = \\(\\ast\\ast\\ast\\), \\(p\\) \\(&lt;\\) 0.01 = \\(\\ast\\ast\\), \\(p\\) \\(&lt;\\) 0.05 = \\(\\ast\\) and \\(p\\) \\(&lt;\\) 0.1 = \\(+\\). 15.2 Example trend analysis We first show the use of the TheilSen function by applying it to concentrations of O3. The function is called as shown in Figure 15.1. library(openair) TheilSen(mydata, pollutant = &quot;o3&quot;, ylab = &quot;ozone (ppb)&quot;, deseason = TRUE, date.format = &quot;%Y&quot;) ## [1] &quot;Taking bootstrap samples. Please wait.&quot; Figure 15.1: Trends in ozone at Marylebone Road. The plot shows the deseasonalised monthly mean concentrations of O3. The solid red line shows the trend estimate and the dashed red lines show the 95% confidence intervals for the trend based on resampling methods. The overall trend is shown at the top-left as 0.38 (ppb) per year and the 95% confidence intervals in the slope from 0.21–0.51 ppb/year. The \\(\\ast\\ast\\ast\\) show that the trend is significant to the 0.001 level. Because the function runs simulations to estimate the uncertainty in the slope, it can take a little time for all the calculations to finish. These printed results show that in this case the trend in O3 was +0.38 units (i.e. ppb) per year as an average over the entire period. It also shows the 95% confidence intervals in the trend ranged between 0.21 to 0.51 ppb/year. Finally, the significance level in this case is very high; providing very strong evidence that concentrations of O3 increased over the period. The plot together with the summary results is shown in Figure 15.1. Note that if one wanted to display the confidence intervals in the slope at the 99% confidence intervals, the code would be Figure 15.2. TheilSen(mydata, pollutant = &quot;o3&quot;, ylab = &quot;ozone (ppb)&quot;, alpha = 0.01) Sometimes it is useful to consider a subset of data, perhaps by excluding some years. This is easy with the filter function. The following code calculates trends for years greater than 1999 i.e. from 2000 onward. TheilSen(filter(mydata, format(date, &quot;%Y&quot;) &gt; 1999), pollutant = &quot;o3&quot;, ylab = &quot;ozone (ppb)&quot;) It is also possible to calculate trends in many other ways e.g. by wind direction. Considering how trends vary by wind direction can be extremely useful because the influence of different sources invariably depends on the direction of the wind. The TheilSen function splits the wind direction into 8 sectors i.e. N, NE, E etc. The Theil-Sen slopes are then calculated for each direction in turn. This function takes rather longer to run because the simulations need to be run eight times in total. Considering concentrations of O3 again, the output is shown in Figure 15.2. Note that this plot is specifically laid out to assist interpretation, with each panel located at the correct point on the compass. This makes it easy to see immediately that there is essentially no trend in O3 for southerly winds i.e. where the road itself has the strongest influence. On the other hand the strongest evidence of increasing O3 are for northerly winds, where the influence of the road is much less. The reason that there is no trend in O3 for southerly winds is that there is always a great excess of NO, which reacts with O3 to form NO2. At this particular location it will probably take many more years before O3 concentrations start to increase when the wind direction is southerly. Nevertheless, there will always be some hours that do not have such high concentrations of NO. TheilSen(mydata, pollutant = &quot;o3&quot;, type = &quot;wd&quot;, deseason = TRUE, date.format = &quot;%Y&quot;, ylab = &quot;ozone (ppb)&quot;) ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; Figure 15.2: Trends in ozone at Marylebone Road split by eight wind sectors. The TheilSen function will automatically organise the separate panels by the different compass directions. The option slope.percent can be set to express slope estimates as a percentage change per year. This is useful for comparing slopes for sites with very different concentration levels and for comparison with emission inventories. The percentage change uses the concentration at the beginning and end months to express the mean slope. The trend, \\(T\\) is defined as: \\[ T [\\%.yr^{-1}] = 100.\\left(\\frac{C_{End}}{C_{Start}} - 1\\right)\\Bigg /N_{years} \\tag{15.1} \\] where \\(C_{End}\\) and \\(C_{Start}\\) are the mean concentrations for the end and start date, respectfully. \\(N_{years}\\) is the number of years (or fractions of) the time series spans. TheilSen(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, slope.percent = TRUE) The TheilSen function was written to work with hourly data, which is then averaged into monthly or annual data. However, it is realised that users may already have data that is monthly or annual. The function can therefore accept as input monthly or annual data directly. However, it is necessary to ensure the date field is in the correct format. Assuming data in an Excel file in the format dd/mm/YYYY (e.g. 23/11/2008), it is necessary to convert this to a date format understood by R, as shown below. Similarly, if annual data were available, get the dates in formats like ‘2005-01-01’, ‘2006-01-01’ … and make sure the date is again formatted using as.Date. Note that if dates are pre-formatted as YYYY-mm-dd, then it is sufficient to use as.Date without providing any format information because it is already in the correct format. mydata$date = as.Date(mydata$date, format = &quot;%d/%m/%Y&quot;) Finally, the TheilSen function can consider trends at different sites, provided the input data are correctly formatted. For input, a data frame with three columns is required: date, pollutant and site. The call would then be, for example: TheilSen(mydata, pollutant = &quot;no2&quot;, type = &quot;site&quot;) 15.2.1 Output The TheilSen function provides lots of output data for further analysis or adding to a report. To obtain it, it is necessary to read it into a variable: MKresults &lt;- TheilSen(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, type = &quot;wd&quot;) ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; ## [1] &quot;Taking bootstrap samples. Please wait.&quot; This returns a list of two data frames containing all the monthly mean values and trend statistics and an aggregated summary. The first 6 lines are shown next: head(MKresults$data[[1]]) ## wd date conc a b upper.a upper.b lower.a ## 1 E 1998-01-01 5.552253 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 2 E 1998-02-01 2.919639 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 3 E 1998-03-01 3.849363 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 4 E 1998-04-01 4.051668 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 5 E 1998-05-01 2.304686 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## 6 E 1998-06-01 -1.560438 -2.825001 0.0007185124 -8.826666 0.001230109 3.848278 ## lower.b p p.stars slope intercept intercept.lower ## 1 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 2 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 3 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 4 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 5 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## 6 0.0001449688 0.01669449 * 0.262257 -2.825001 3.848278 ## intercept.upper lower upper slope.percent lower.percent ## 1 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 2 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 3 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 4 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 5 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## 6 -8.826666 0.05291362 0.4489896 5.79801 0.9925881 ## upper.percent ## 1 11.9614 ## 2 11.9614 ## 3 11.9614 ## 4 11.9614 ## 5 11.9614 ## 6 11.9614 Often only the trend statistics are required and not all the monthly values. These can be obtained by: MKresults$data[[2]] ## wd p.stars date conc a b upper.a ## 1 E * 2001-09-15 5.989974 -2.825001 7.185124e-04 -8.8266663 ## 2 N *** 2001-09-15 9.786267 -19.142102 2.485907e-03 -28.7157702 ## 3 NE *** 2001-09-15 9.728994 -9.728741 1.624970e-03 -24.8702270 ## 4 NW *** 2001-09-15 9.786755 -12.940875 1.937119e-03 -22.4192235 ## 5 S 2001-09-15 5.052728 4.472516 2.509968e-05 1.0744367 ## 6 SE 2001-09-15 5.780645 4.822713 7.357549e-05 0.8230425 ## 7 SW 2001-09-15 4.761100 1.818359 2.444766e-04 -1.9192744 ## 8 W ** 2001-09-15 5.618727 -1.178793 5.736058e-04 -5.6090971 ## upper.b lower.a lower.b p slope intercept ## 1 0.0012301086 3.848278 1.449688e-04 0.016694491 0.262257022 -2.825001 ## 2 0.0032893251 -9.725509 1.649012e-03 0.000000000 0.907355964 -19.142102 ## 3 0.0029640572 3.185458 5.003924e-04 0.000000000 0.593114074 -9.728741 ## 4 0.0027690442 -3.396506 1.122414e-03 0.000000000 0.707048447 -12.940875 ## 5 0.0003281521 7.156572 -2.081018e-04 0.868113523 0.009161382 4.472516 ## 6 0.0004318379 8.727162 -2.572925e-04 0.684474124 0.026855053 4.822713 ## 7 0.0005716588 5.538235 -8.999974e-05 0.110183639 0.089233973 1.818359 ## 8 0.0009552189 3.041723 2.044787e-04 0.003338898 0.209366110 -1.178793 ## intercept.lower intercept.upper lower upper slope.percent ## 1 3.848278 -8.8266663 0.05291362 0.4489896 5.7980099 ## 2 -9.725509 -28.7157702 0.60188926 1.2006037 14.4454302 ## 3 3.185458 -24.8702270 0.18264323 1.0818809 8.6085473 ## 4 -3.396506 -22.4192235 0.40968128 1.0107011 10.2917646 ## 5 7.156572 1.0744367 -0.07595715 0.1197755 0.1937191 ## 6 8.727162 0.8230425 -0.09391177 0.1576208 0.4816904 ## 7 5.538235 -1.9192744 -0.03284991 0.2086555 2.0662607 ## 8 3.041723 -5.6090971 0.07463472 0.3486549 4.4665024 ## lower.percent upper.percent ## 1 0.9925881 11.961401 ## 2 8.4310805 24.381910 ## 3 2.1997335 19.875875 ## 4 5.0687904 17.131133 ## 5 -1.5105886 2.703463 ## 6 -1.5405900 3.008348 ## 7 -0.7113745 5.313247 ## 8 1.4540385 8.381275 In the results above the lower and upper fields provide the 95% (or chosen confidence interval using the alpha option) of the trend and slope is the trend estimate expressed in units/year. References "],
["sec-smoothTrend.html", "Section16 Smooth trends 16.1 Background 16.2 Examples of trend analysis 16.3 Seasonal averages", " Section16 Smooth trends 16.1 Background The smoothTrend function calculates smooth trends in the monthly mean concentrations of pollutants. In its basic use it will generate a plot of monthly concentrations and fit a smooth line to the data and show the 95% confidence intervals of the fit. The smooth line is essentially determined using Generalized Additive Modelling using the mgcv package. This package provides a comprehensive and powerful set of methods for modelling data. In this case, however, the model is a relationship between time and pollutant concentration i.e. a trend. One of the principal advantages of this approach is that the amount of smoothness in the trend is optimised in the sense that it is neither too smooth (therefore missing important features) nor too variable (perhaps fitting ‘noise’ rather than real effects). Some background information on the use of this approach in an air quality setting can be found in Carslaw, Beevers, and Tate (2007). Section ?? considers smooth trends in more detail and considers how different models can be developed that can be quite sophisticated. Readers should consider this section if they are considering trend analysis in more depth. The user can select to deseasonalise the data first to provide a clearer indication of the overall trend on a monthly basis. The data are deseasonalised using the stl function. The user may also select to use bootstrap simulations to provide an alternative method of estimating the uncertainties in the trend. In addition, the simulated estimates of uncertainty can account for autocorrelation in the residuals using a block bootstrap approach. 16.2 Examples of trend analysis We apply the function to concentrations of O3 and NO2 using the code below. The first plot shows the smooth trend in raw O3 concentrations, which shows a very clear seasonal cycle. By removing the seasonal cycle of O3, a better indication of the trend is given, shown in the second plot. Removing the seasonal cycle is more effective for pollutants (or locations) where the seasonal cycle is stronger e.g. for ozone and background sites. Figure #ref(fig:smoothTrends) shows the results of the simulations for NO2 without the seasonal cycle removed. It is clear from this plot that there is little evidence of a seasonal cycle. The principal advantage of the smoothing approach compared with the Theil-Sen method is also clearly shown in this plot. Concentrations of NO2 first decrease, then increase strongly. The trend is therefore not monotonic, violating the Theil-Sen assumptions. Finally, the last plot shows the effects of first deaseasonalising the data: in this case with little effect. smoothTrend(mydata, pollutant = &quot;o3&quot;, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean o3&quot;) smoothTrend(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean deseasonalised o3&quot;) smoothTrend(mydata, pollutant = &quot;no2&quot;, simulate = TRUE, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean no2 (bootstrap uncertainties)&quot;) ## [1] &quot;Taking bootstrap samples. Please wait...&quot; smoothTrend(mydata, pollutant = &quot;no2&quot;, deseason = TRUE, simulate =TRUE, ylab = &quot;concentration (ppb)&quot;, main = &quot;monthly mean deseasonalised no2 (bootstrap uncertainties)&quot;) ## [1] &quot;Taking bootstrap samples. Please wait...&quot; Figure 16.1: Examples of the smoothTrend function applied to Marylebone Road The smoothTrend function share many of the functionalities of the TheilSen function. Figure 16.2 shows the result of applying this function to O3 concentrations. The code that produced Figure 16.2 was: smoothTrend(mydata, pollutant = &quot;o3&quot;, deseason = TRUE, type = &quot;wd&quot;) Figure 16.2: Trends in O3 using the smoothTrend function applied to Marylebone Road. The shading shows the estimated 95% confidence intervals. The smoothTrend function can easily be used to gain a large amount of information on trends easily. For example, how do trends in NO2, O3 and PM10 vary by season and wind sector. There are 8 wind sectors and four seasons i.e. 32 plots. In Figure 16.3 all three pollutants are chosen and two types (season and wind direction). We also reduce the number of axis labels and the line to improve clarity. There are numerous combinations of analyses that could be produced here and it is very easy to explore the data in a wide number of ways. smoothTrend(mydata, pollutant = c(&quot;no2&quot;, &quot;pm10&quot;, &quot;o3&quot;), type = c(&quot;wd&quot;, &quot;season&quot;), date.breaks = 3, lty = 0) Figure 16.3: The smoothTrend function applied to three pollutants, split by wind sector and season. 16.3 Seasonal averages If the interest is in considering seasonal trends, it makes sense to set the averaging time to ‘season’ and also show each season in a separate panel. In Figure 16.4 some data is imported for O3 from Lullington Heath on the south coast of England. This plot shows there is strongest evidence for an increase in O3 concentrations during springtime from about 2012 onwards. A probable reason for this increase is a general reduction in NOx concentrations. lh &lt;- importAURN(site = &quot;lh&quot;, year = 2000:2019) smoothTrend(lh, pollutant = &quot;o3&quot;, avg.time = &quot;season&quot;, type = &quot;season&quot;, date.breaks = 4) Figure 16.4: Trends in O3 by season at the Lullington Heath site with a seasonal averaging time and panel for each season. References "],
["sec-trajPlot.html", "Section17 Trajectory analysis 17.1 Plotting trajectories 17.2 Trajectory gridded frequencies 17.3 Trajectory source contribution functions 17.4 Potential Source Contribution Function (PSCF) 17.5 Concentration Weighted Trajectory (CWT) 17.6 Trajectory clustering", " Section17 Trajectory analysis Back trajectories are extremely useful in air pollution and can provide important information on air mass origins. Despite the clear usefulness of back trajectories, their use tends to be restricted to the research community. Back trajectories are used for many purposes from understanding the origins of air masses over a few days to undertaking longer term analyses. They are often used to filter air mass origins to allow for more refined analyses of air pollution — for example trends in concentration by air mass origin. They are often also combined with more sophisticated analyses such as cluster analysis to help group similar type of air mass by origin. Perhaps one of the reasons why back trajectory analysis is not carried out more often is that it can be time consuming to do. This is particularly so if one wants to consider several years at several sites. It can also be difficult to access back trajectory data. In an attempt to overcome some of these issues and expand the possibilities for data analysis, openair makes several functions available to access and analyse pre-calculated back trajectories. Currently these functions allow for the import of pre-calculated back trajectories are several pre-define locations and some trajectory plotting functions. In time all of these functions will be developed to allow more sophisticated analyses to be undertaken. Also it should be recognised that these functions are in their early stages of development and will may continue to change and be refined. The importTraj function imports pre-calculated back trajectories using the Hysplit trajectory model Hybrid Single Particle Lagrangian Integrated Trajectory Model. Trajectories are run at 3-hour intervals and stored in yearly files (see below). The trajectories are started at ground-level (10m) and propagated backwards in time. The data are stored on web-servers at Ricardo Energy &amp; Environment similar to that for importAURN, which makes it very easy to import pre-processed trajectory data for a range of locations and years.5 Users may for various reasons wish to run Hysplit themselves e.g. for different starting heights, longer periods or more locations. Code and instructions have been provided in Section ?? for users wishing to do this. Users can also use different means of calculating back trajectories e.g. ECMWF and plot them in openair provided a few basic fields are present: date (POSIXct), lat (decimal latitude), lon (decimal longitude) and hour.inc the hour offset from the arrival date (i.e. from zero decreasing to the length of the back trajectories). See ?importTraj for more details. These trajectories have been calculated using the Global NOAA-NCEP/NCAR reanalysis data archives. The global data are on a latitude-longitude grid (2.5 degree). Note that there are many meteorological data sets that can be used to run Hysplit e.g. including ECMWF data. However, in order to make it practicable to run and store trajectories for many years and sites, the NOAA-NCEP/NCAR reanalysis data is most useful. In addition, these archives are available for use widely, which is not the case for many other data sets e.g. ECMWF. Hysplit calculated trajectories based on archive data may be distributed without permission (see https://ready.arl.noaa.gov/HYSPLIT_agreement.php). For those wanting, for example, to consider higher resolution meteorological data sets it may be better to run the trajectories separately. openair uses the mapproj package to allow users to user different map projections. By default, the projection used is Lambert conformal, which is a conic projection best used for mid-latitude areas. The Hysplit model itself will use any one of three different projections depending on the latitude of the origin. If the latitude greater than 55.0 (or less than -55.0) then a polar stereographic projection is used, if the latitude greater than -25.0 and less than 25.0 the mercator projection is used and elsewhere (the mid-latitudes) the Lambert projection. All these projections (and many others) are available in the mapproj package. Users should see the help file for importTraj to get an up to date list of receptors where back trajectories have been calculated. First, the packages are loaded that are needed. library(openair) library(tidyverse) library(lubridate) As an example, we will import trajectories for London in 2010. Importing them is easy: traj &lt;- importTraj(site = &quot;london&quot;, year = 2010) The file itself contains lots of information that is of use for plotting back trajectories: head(traj) ## receptor year month day hour hour.inc lat lon height pressure ## 1 1 2010 1 1 9 0 51.500 -0.100 10.0 994.7 ## 2 1 2010 1 1 8 -1 51.766 0.057 10.3 994.9 ## 3 1 2010 1 1 7 -2 52.030 0.250 10.5 995.0 ## 4 1 2010 1 1 6 -3 52.295 0.488 10.8 995.0 ## 5 1 2010 1 1 5 -4 52.554 0.767 11.0 995.4 ## 6 1 2010 1 1 4 -5 52.797 1.065 11.3 995.6 ## date2 date ## 1 2010-01-01 09:00:00 2010-01-01 09:00:00 ## 2 2010-01-01 08:00:00 2010-01-01 09:00:00 ## 3 2010-01-01 07:00:00 2010-01-01 09:00:00 ## 4 2010-01-01 06:00:00 2010-01-01 09:00:00 ## 5 2010-01-01 05:00:00 2010-01-01 09:00:00 ## 6 2010-01-01 04:00:00 2010-01-01 09:00:00 The traj data frame contains among other things the latitude and longitude of the back trajectory, the height (m) and pressure (Pa) of the trajectory. The date field is the arrival time of the air-mass and is useful for linking with ambient measurement data. The trajPlot function is used for plotting back trajectory lines and density plots and has the following options: 17.1 Plotting trajectories Next, we consider how to plot back trajectories with a few simple examples. The first example will consider a potentially interesting period when the Icelandic volcano, Eyjafjallajokull erupted in April 2010. The eruption of Eyjafjallajokull resulted in a flight-ban that lasted six days across many European airports. In Figure 17.1 selectByDate is used to consider the 7 days of interest and we choose to plot the back trajectories as lines rather than points (the default). Figure 17.1 does indeed show that many of the back trajectories originated from Iceland over this period. Note also the plot automatically includes a world base map. The base map itself is not at very high resolution by default but is useful for the sorts of spatial scales that back trajectories exist over. The base map is also global, so provided that there are pre-calculated back trajectories, these maps can be generated anywhere in the world. By default the function uses the ‘world’ map from the maps package. If map.res = \"hires\" then the (much) more detailed base map worldHires from the mapdata package is used.6 trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end =&quot;21/4/2010&quot;), map.cols = openColours(&quot;hue&quot;, 10), col = &quot;grey30&quot;) ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map Figure 17.1: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010. Note the additional option to vary the country colours using map.cols. By default the map colours for all countries are grey. Note that trajPlot will only plot full length trajectories. This can be important when plotting something like a single month e.g. by using selectByDate when on partial sections of some trajectories may be selected. There are a few other ways of representing the data shown in Figure 17.1. For example, it might be useful to plot the trajectories for each day. To do this we need to make a new column day which can be used in the plotting. The first example considers plotting the back trajectories in separate panels Figure (17.2. ## make a day column traj$day &lt;- as.Date(traj$date) ## plot it choosing a specfic layout trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), type = &quot;day&quot;, layout = c(7, 1)) Figure 17.2: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, shown separately for each day. Another way of plotting the data is to group the trajectories by day and colour them. This time we also set a few other options to get the layout we want — shown in Figure 17.3. trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), group = &quot;day&quot;, col = &quot;jet&quot;, lwd = 2, key.pos = &quot;top&quot;, key.col = 4) Figure 17.3: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, shown grouped for each day and coloured accordingly. So far the plots have provided information on where the back trajectories come from, grouped or split by day. It is also possible, in common with most other openair functions to split the trajectories by many other variables e.g. month, season and so on. However, perhaps one of the most useful approaches is to link the back trajectories with the concentrations of a pollutant. As mentioned previously, the back trajectory data has a column date representing the arrival time of the air mass that can be used to link with concentration measurements. A couple of steps are required to do this using the left_join function. ## import data for North Kensington kc1 &lt;- importAURN(&quot;kc1&quot;, year = 2010) # now merge with trajectory data by &#39;date&#39; traj &lt;- left_join(traj, kc1, by = &quot;date&quot;) ## look at first few lines head(traj) ## receptor year month day hour hour.inc lat lon height pressure ## 1 1 2010 1 2010-01-01 9 0 51.500 -0.100 10.0 994.7 ## 2 1 2010 1 2010-01-01 8 -1 51.766 0.057 10.3 994.9 ## 3 1 2010 1 2010-01-01 7 -2 52.030 0.250 10.5 995.0 ## 4 1 2010 1 2010-01-01 6 -3 52.295 0.488 10.8 995.0 ## 5 1 2010 1 2010-01-01 5 -4 52.554 0.767 11.0 995.4 ## 6 1 2010 1 2010-01-01 4 -5 52.797 1.065 11.3 995.6 ## date2 date site code co nox no2 ## 1 2010-01-01 09:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 2 2010-01-01 08:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 3 2010-01-01 07:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 4 2010-01-01 06:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 5 2010-01-01 05:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## 6 2010-01-01 04:00:00 2010-01-01 09:00:00 London N. Kensington KC1 0.3 38 29 ## no o3 so2 pm10 pm2.5 v10 v2.5 nv10 nv2.5 ws wd air_temp ## 1 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 2 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 3 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 4 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 5 6 46 0 8 NA 0 NA 8 NA NA NA NA ## 6 6 46 0 8 NA 0 NA 8 NA NA NA NA This time we can use the option pollutant in the function trajPlot, which will plot the back trajectories coloured by the concentration of a pollutant. Figure 17.4 does seem to show elevated PM10 concentrations originating from Iceland over the period of interest. In fact, these elevated concentrations occur on two days as shown in Figure 17.2. However, care is needed when interpreting such data because other analysis would need to rule out other reasons why PM10 could be elevated; in particular due to local sources of PM10. There are lots of openair functions that can help here e.g. timeVariation or timePlot to see if NOx concentrations were also elevated (which they seem to be). It would also be worth considering other sites for back trajectories that could be less influenced by local emissions. trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), pollutant = &quot;pm10&quot;, col = &quot;jet&quot;, lwd =2) Figure 17.4: 96-hour Hysplit back trajectories centred on London for 7 days in April 2010, coloured by the concentration of PM10 in \\(\\mu\\)g m-3. However, it is possible to account for the PM that is local to some extent by considering the relationship between NOx and PM10 (or PM2.5). For example, using scatterPlot (not shown): scatterPlot(kc1, x = &quot;nox&quot;, y = &quot;pm2.5&quot;, avg = &quot;day&quot;, linear = TRUE) which suggests a gradient of 0.084. Therefore, we can remove the PM10 that is associated NOx in kc1 data, making a new column pm.new: kc1 &lt;- mutate(kc1, pm.new = pm10 - 0.084 * nox) We have already merged kc1 with traj, so to keep things simple we import traj again and merge it with kc1. Note that if we had thought of this initially, pm.new would have been calculated first before merging with traj. traj &lt;- importTraj(site = &quot;london&quot;, year = 2010) traj &lt;- left_join(traj, kc1, by = &quot;date&quot;) Now it is possible to plot the trajectories: trajPlot(selectByDate(traj, start = &quot;15/4/2010&quot;, end = &quot;21/4/2010&quot;), pollutant = &quot;pm.new&quot;, col = &quot;jet&quot;, lwd = 2) Which, interestingly still clearly shows elevated PM10 concentrations for those two days that cross Iceland. The same is also true for PM2.5. However, as mentioned previously, checking other sites in more rural areas would be a good idea. 17.2 Trajectory gridded frequencies The Hysplit model itself contains various analysis options for gridding trajectory data. Similar capabilities are also available in openair where the analyses can be extended using other openair capabilities. It is useful to gain an idea of where trajectories come from. Over the course of a year representing trajectories as lines or points results in a lot of over-plotting. Therefore it is useful to grid the trajectory data and calculate various statistics by considering latitude-longitude intervals. The first analysis considers the number of unique trajectories in a particular grid square. This is achieved by using the trajLevel function and setting the statistic option to “frequency”. Figure 17.5 shows the frequency of back trajectory crossings for the North Kensington data. In this case it highlights that most trajectory origins are from the west and north for 2010 at this site. Note that in this case, pollutant can just be the trajectory height (or another numeric field) rather than an actual pollutant because only the frequencies are considered. trajLevel(traj, statistic = &quot;frequency&quot;) Figure 17.5: Gridded back trajectory frequencies. The border = NA option removes the border around each grid cell. It is also possible to use hexagonal binning to gain an idea about trajectory frequencies. In this case each 3-hour point along each trajectory is used in the counting. The code below focuses more on Europe and uses the hexagonal binning method. Note that the effect of the very few high number of points at the origin has been diminished by plotting the data on a log scale — see Section 13.2.1 for details. trajLevel(subset(traj, lat &gt; 30 &amp; lat &lt; 70 &amp; lon &gt; -30 &amp; lon &lt; 20), method = &quot;hexbin&quot;, col = &quot;jet&quot;, xbin = 40) Figure 17.6: Gridded back trajectory frequencies with hexagonal binning. 17.3 Trajectory source contribution functions Back trajectories offer the possibility to undertake receptor modelling to identify the location of major emission sources. When many back trajectories (over months to years) are analysed in specific ways they begin to show the geographic origin most associated with elevated concentrations. With enough (dissimilar) trajectories those locations leading to the highest concentrations begin to be revealed. When a whole year of back trajectory data is plotted the individual back trajectories can extend 1000s of km. There are many approaches using back trajectories in this way and Fleming, Monks, and Manning (2012) provide a good overview of the methods available. openair has implemented a few of these techniques and over time these will be refined and extended. 17.3.1 Identifying the contribution of high concentration back trajectories A useful analysis to undertake is to consider the pattern of frequencies for two different conditions. In particular, there is often interest in the origin of high concentrations for different pollutants. For example, compared with data over a whole year, how do the frequencies of occurrence differ? Figure 17.7 shows an example of such an analysis for PM10 concentrations. By default the function will compare concentrations $&gt;$90th percentile with the full year. The percentile level is controlled by the option percentile. Note also there is an option min.bin that will exclude grid cells where there are fewer than min.bin data points. The analysis compares the percentage of time the air masses are in particular grid squares for all data and a subset of data where the concentrations are greater than the given percentile. The graph shows the absolute percentage difference between the two cases i.e. high minus base. Figure 17.7 shows that compared with the whole year, high PM10 concentrations ($&gt;$90th percentile) are more prevalent when the trajectories originate from the east, which is seen by the positive values in the plot. Similarly there are relatively fewer occurrences of these high concentration back trajectories when they originate from the west. This analysis is in keeping with the highest PM10 concentrations being largely controlled by secondary aerosol formation from air-masses originating during anticyclonic conditions from mainland Europe. trajLevel(traj, pollutant = &quot;pm10&quot;, statistic = &quot;difference&quot;, col = c(&quot;skyblue&quot;, &quot;white&quot;, &quot;tomato&quot;), min.bin = 50, border = NA, xlim = c(-20, 20), ylim = c(40, 70)) Figure 17.7: Gridded back trajectory frequencies showing the percentage difference in occurrence for high PM10 concentrations (90th percentile) compared with conditions over the full year. Note that it is also possible to use conditioning with these plots. For example to split the frequency results by season: trajLevel(traj, pollutant = &quot;pm10&quot;, statistic = &quot;frequency&quot;, col = &quot;heat&quot;, type = &quot;season&quot;) 17.3.2 Allocating trajectories to different wind sectors One of the key aspects of trajectory analysis is knowing something about where air masses have come from. Cluster analysis can be used to group trajectories based on their origins and this is discussed in Section 17.6. A simple approach is to consider different wind sectors e.g. N, NE, E and calculate the proportion of time a particular back trajectory resides in a specific sector. It is then possible to allocate a particular trajectory to a sector based on some assumption about the proportion of time it is in that sector — for example, assume a trajectory is from the west sector if it spends at least 50% of its time in that sector or otherwise record the allocation as `unallocated’. The code below can be used as the basis of such an approach. First we import the trajectories, which in this case are for London in 2010: traj &lt;- importTraj(site = &quot;london&quot;, year = 2010) ## need start/end lat and lon to work out angles id &lt;- which(traj$hour.inc == 0) y0 &lt;- traj$lat[id[1]] x0 &lt;- traj$lon[id[1]] ## calculate angle and then assign sector traj &lt;- mutate(traj, angle = atan2(lon - x0, lat - y0) * 360 / 2 / pi) ids &lt;- which(traj$angle &lt; 0) traj$angle[ids] &lt;- traj$angle[ids] + 360 traj$sector &lt;- cut(traj$angle, breaks = seq(22.5, 382.5, 45), labels = c(&quot;NE&quot;, &quot;E&quot;, &quot;SE&quot;, &quot;S&quot;, &quot;SW&quot;, &quot;W&quot;, &quot;NW&quot;, &quot;N&quot;)) traj[ , &quot;sector&quot;][is.na(traj[ , &quot;sector&quot;])] &lt;- &quot;N&quot; # for wd &lt; 22.5 ## count frequencies of sectors for each trajectory and the maximum alloc &lt;- tapply(traj$sector, traj$date, table) alloc &lt;- as.data.frame(do.call(rbind, alloc)) alloc$max &lt;- apply(alloc, 1, max) ## identify the most frequent sector alloc$sec &lt;- sapply(1:nrow(alloc), function(x) colnames(alloc)[which.max(alloc[x, ])]) ## assign to most frequent sector, or label unallocated ## below assumes at least 50 out of 96 hours for assignment alloc$sec &lt;- ifelse(alloc$max &gt; 50, alloc$sec, &quot;unallocated&quot;) alloc$date &lt;- ymd_hms(rownames(alloc)) alloc &lt;- alloc[, c(&quot;date&quot;, &quot;sec&quot;)] ## merge with orginal data traj &lt;- merge(traj, alloc, by = &quot;date&quot;, all = TRUE) Now it is possible to post-process the data. traj now has the angle, sector and allocation (sec). head(traj) ## date receptor year month day hour hour.inc lat lon ## 1 2010-01-01 09:00:00 1 2010 1 1 9 0 51.500 -0.100 ## 2 2010-01-01 09:00:00 1 2010 1 1 8 -1 51.766 0.057 ## 3 2010-01-01 09:00:00 1 2010 1 1 7 -2 52.030 0.250 ## 4 2010-01-01 09:00:00 1 2010 1 1 6 -3 52.295 0.488 ## 5 2010-01-01 09:00:00 1 2010 1 1 5 -4 52.554 0.767 ## 6 2010-01-01 09:00:00 1 2010 1 1 4 -5 52.797 1.065 ## height pressure date2 angle sector sec ## 1 10.0 994.7 2010-01-01 09:00:00 0.00000 N unallocated ## 2 10.3 994.9 2010-01-01 08:00:00 30.55019 NE unallocated ## 3 10.5 995.0 2010-01-01 07:00:00 33.43987 NE unallocated ## 4 10.8 995.0 2010-01-01 06:00:00 36.48747 NE unallocated ## 5 11.0 995.4 2010-01-01 05:00:00 39.44005 NE unallocated ## 6 11.3 995.6 2010-01-01 04:00:00 41.93103 NE unallocated First, merge the air quality data from North Kensington: traj &lt;- left_join(traj, kc1, by = &quot;date&quot;) We can work out the mean concentration by allocation, which shows the clear importance for the east and south-east sectors. tapply(traj$pm2.5, traj$sec, mean, na.rm = TRUE) ## E N NE NW S SE ## 21.64671 11.79452 12.61986 13.60474 14.50000 28.72500 ## SW unallocated W ## 10.80992 15.47481 11.75882 Finally, the percentage of the year in each sector can be calculated as follows: 100 * prop.table(table(traj$sec)) ## ## E N NE NW S SE ## 6.712201 5.512401 10.252995 9.164367 2.170508 2.067150 ## SW unallocated W ## 8.750937 29.530061 25.839380 17.4 Potential Source Contribution Function (PSCF) If statistic = \"pscf\" then the Potential Source Contribution Function (PSCF) is plotted. The PSCF calculates the probability that a source is located at latitude \\(i\\) and longitude \\(j\\) (Fleming, Monks, and Manning 2012; Pekney et al. 2006). The PSCF is somewhat analogous to the CPF function described on Section 6.3 that considers local wind direction probabilities. In fact, the two approaches have been shown to work well together (Pekney et al. 2006). The PSCF approach has been widely used in the analysis of air mass back trajectories. Ara Begum et al. (2005) for example assessed the method against the known locations of wildfires and found it performed well for PM2.5, EC (elemental carbon) and OC (organic carbon) and that other (non-fire related) species such as sulphate had different source origins. The basis of PSCF is that if a source is located at (\\(i\\), \\(j\\)), an air parcel back trajectory passing through that location indicates that material from the source can be collected and transported along the trajectory to the receptor site. PSCF solves \\[\\begin{equation} PSCF = \\frac{m_{ij}}{n_{ij}} \\tag{17.1} \\end{equation}\\] where \\(n_{ij}\\) is the number of times that the trajectories passed through the cell (\\(i\\), \\(j\\)) and \\(m_{ij}\\) is the number of times that a source concentration was high when the trajectories passed through the cell (\\(i\\), \\(j\\)). The criterion for determining \\(m_{ij}\\) is controlled by `percentile}, which by default is 90. Note also that cells with few data have a weighting factor applied to reduce their effect. An example of a PSCF plot is shown in Figure 17.8 for PM2.5 for concentrations &gt;90th percentile. This Figure gives a very clear indication that the principal (high) sources are dominated by source origins in mainland Europe — particularly around the Benelux countries. trajLevel(filter(traj, lon &gt; -20, lon &lt; 20, lat &gt; 45, lat &lt; 60), pollutant = &quot;pm2.5&quot;, statistic = &quot;pscf&quot;, col = &quot;increment&quot;, border = NA) Figure 17.8: PSCF probabilities for PM2.5 concentrations (90th percentile). 17.5 Concentration Weighted Trajectory (CWT) A limitation of the PSCF method is that grid cells can have the same PSCF value when sample concentrations are either only slightly higher or much higher than the criterion (Hsu, Holsen, and Hopke 2003). As a result, it can be difficult to distinguish moderate sources from strong ones. Seibert et al. (1994) computed concentration fields to identify source areas of pollutants. This approach is sometimes referred to as the CWT or CF (concentration field). A grid domain was used as in the PSCF method. For each grid cell, the mean (CWT) or logarithmic mean (used in the Residence Time Weighted Concentration (RTWC) method) concentration of a pollutant species was calculated as follows: \\[\\begin{equation} ln(\\overline{C}_{ij}) = \\frac{1}{\\sum_{k=1}^{N}\\tau_{ijk}}\\sum_{k=1}^{N}ln(c_k)\\tau_{ijk} \\tag{17.2} \\end{equation}\\] where \\(i\\) and \\(j\\) are the indices of grid, \\(k\\) the index of trajectory, \\(N\\) the total number of trajectories used in analysis, \\(c_k\\) the pollutant concentration measured upon arrival of trajectory \\(k\\), and \\(\\tau_{ijk}\\) the residence time of trajectory \\(k\\) in grid cell (\\(i\\), \\(j\\)). A high value of \\(\\overline{C}_{ij}\\) means that, air parcels passing over cell (\\(i\\), \\(j\\)) would, on average, cause high concentrations at the receptor site. Figure 17.9 shows the situation for PM2.5 concentrations. It was calculated by recording the associated PM2.5 concentration for each point on the back trajectory based on the arrival time concentration using 2010 data. The plot shows the geographic areas most strongly associated with high PM2.5 concentrations i.e. to the east in continental Europe. Both the CWT and PSCF methods have been shown to give similar results and each have their advantages and disadvantages (Lupu and Maenhaut 2002; Hsu, Holsen, and Hopke 2003). Figure 17.9 can be compared with Figure 17.8 to compare the overall identification of source regions using the CWT and PSCF techniques. Overall the agreement is good in that similar geographic locations are identified as being important for PM2.5. trajLevel(filter(traj,lon &gt; -20, lon &lt; 20, lat &gt; 45, lat &lt; 60), pollutant = &quot;pm2.5&quot;, statistic=&quot;cwt&quot;, col = &quot;increment&quot;, border = &quot;white&quot;) Figure 17.9: Gridded back trajectory concentrations showing mean PM2.5 concentrations using the CWT approach. Figure 17.9 is useful, but it can be clearer if the trajectory surface is smoothed, which has been done for PM2.5 concentrations shown in Figure 17.10. trajLevel(subset(traj, lat &gt; 45 &amp; lat &lt; 60 &amp; lon &gt;-20 &amp; lon &lt;20), pollutant =&quot;pm2.5&quot;, statistic = &quot;cwt&quot;, smooth = TRUE, col = &quot;increment&quot;) Figure 17.10: Gridded and smoothed back trajectory concentrations showing mean PM2.5 concentrations using the CWT approach. In common with most other openair functions, the flexible type option can be used to split the data in different ways. For example, to plot the smoothed back trajectories for PM2.5 concentrations by season. trajLevel(subset(traj, lat &gt; 40 &amp; lat &lt; 70 &amp; lon &gt;-20 &amp; lon &lt;20), pollutant = &quot;pm2.5&quot;, type = &quot;season&quot;, statistic = &quot;pscf&quot;, layout = c(4, 1)) It should be noted that it makes sense to analyse back trajectories for pollutants that have a large regional component — such as particles or . It makes little sense to analyse pollutants that are known to have local impacts e.g. . However, a species such as can be helpful to exclude `fresh’ emissions from the analysis. 17.6 Trajectory clustering Often it is useful to use cluster analysis on back trajectories to group similar air mass origins together. The principal purpose of clustering back trajectories is to post-process data according to cluster origin. By grouping data with similar geographic origins it is possible to gain information on pollutant species with similar chemical histories. There are several ways in which clustering can be carried out and several measures of the similarity of different clusters. A key issue is how the distance matrix is calculated, which determines the similarity (or dissimilarity) of different back trajectories. The simplest measure is the Euclidean distance. However, an angle-based measure is also often used. The two distance measures are defined below. In openair the distance matrices are calculated using C\\(++\\) code because their calculation is computationally intensive. Note that these calculations can also be performed directly in the Hysplit model itself. The Euclidean distance between two trajectories is given by Equation (17.3). Where \\(X_1\\), \\(Y_1\\) and \\(X_2\\), \\(Y_2\\) are the latitude and longitude coordinates of back trajectories \\(1\\) and \\(2\\), respectively. \\(n\\) is the number of back trajectory points (96 hours in this case). \\[\\begin{equation} d_{1, 2} = \\left({\\sum_{i=1}^{n} ((X_{1i} - X_{2i}) ^ 2 + (Y_{1i} - Y_{2i})) ^ 2}\\right)^{1/2} \\tag{17.3} \\end{equation}\\] The angle distance matrix is a measure of how similar two back trajectory points are in terms of their angle from the origin i.e. the starting location of the back trajectories. The angle-based measure will often capture some of the important circulatory features in the atmosphere e.g. situations where there is a high pressure located to the east of the UK. However, the most appropriate distance measure will be application dependent and is probably best tested by the extent to which they are able to differentiate different air-mass characteristics, which can be tested through post-processing. The angle-based distance measure is defined as: \\[\\begin{equation} d_{1, 2} = \\frac{1}{n}\\sum_{i=1}^{n}cos^{-1} \\left(0.5\\frac{A_i + B_i + C_i}{\\sqrt{A_iB_i}}\\right) \\tag{17.4} \\end{equation}\\] where \\[\\begin{equation} \\tag{17.5} A_i = (X_1(i) - X_0)^2 + (Y_1(i) - Y_0)^2 \\end{equation}\\] \\[\\begin{equation} \\tag{17.6} B_i = (X_2(i) - X_0)^2 + (Y_2(i) - Y_0)^2 \\end{equation}\\] \\[\\begin{equation} \\tag{17.7} C_i = (X_2(i) - X_1(i))^2 + (Y_2(i) - Y_1(i))^2 \\end{equation}\\] where \\(X_0\\) and \\(Y_0\\) are the coordinates of the location being studied i.e. the starting location of the trajectories. As an example we will consider back trajectories for London in 2011. First, the back trajectory data for London is imported together with the air pollution data for the North Kensington site (KC1). traj &lt;- importTraj(site = &quot;london&quot;, year = 2011) kc1 &lt;- importAURN(site = &quot;kc1&quot;, year = 2011) The clusters are straightforward to calculate. In this case the back trajectory data (traj) is supplied and the angle-based distance matrix is used. Furthermore, we choose to calculate 6 clusters and choose a specific colour scheme. In this case we read the output from trajCluster into a variable clust so that the results can be post-processed. clust &lt;- trajCluster(traj, method = &quot;Angle&quot;, n.cluster = 6, col = &quot;Set2&quot;, map.cols = openColours(&quot;Paired&quot;, 10)) Figure 17.11: The 6-cluster solution to back trajectories calculated for the London North Kensington site for 2011 showing the mean trajectory for each cluster. clust returns all the back trajectory information together with the cluster (as a character). This data can now be used together with other data to analyse results further. However, first it is possible to show all trajectories coloured by cluster, although for a year of data there is significant overlap and it is difficult to tell them apart. trajPlot(clust$data, group = &quot;cluster&quot;) A useful way in which to see where these air masses come from by trajectory is to produce a frequency plot by cluster. Such a plot (not shown, but code below) provides a good indication of the spread of the different trajectory clusters as well as providing an indication of where air masses spend most of their time. For the London 2011 data it can be seen cluster 1 is dominated by air from the European mainland to the south. trajLevel(clust$data, type = &quot;cluster&quot;, col = &quot;increment&quot;, border = NA) Perhaps more useful is to merge the cluster data with measurement data. In this case the data at North Kensington site are used. Note that in merging these two data frames it is not necessary to retain all 96 back trajectory hours and for this reason we extract only the first hour. kc1 &lt;- left_join(kc1, filter(clust$data, hour.inc == 0), by = &quot;date&quot;) Now kc1 contains air pollution data identified by cluster. The size of this data frame is about a third of the original size because back trajectories are only run every 3~hours. The numbers of each cluster are given by: table(kc1[[&quot;cluster&quot;]]) ## ## C1 C2 C3 C4 C5 C6 ## 347 661 989 277 280 333 i.e. is dominated by clusters 3 and 2 from west and south-west (Atlantic). Now it is possible to analyse the concentration data according to the cluster. There are numerous types of analysis that can be carried out with these results, which will depend on what the aims of the analysis are in the first place. However, perhaps one of the first things to consider is how the concentrations vary by cluster. As the summary results below show, there are distinctly different mean concentrations of most pollutants by cluster. For example, clusters 1 and 6 are associated with much higher concentrations of PM10 — approximately double that of other clusters. Both of these clusters originate from continental Europe. Cluster 5 is also relatively high, which tends to come from the rest of the UK. Other clues concerning the types of air-mass can be gained from the mean pressure. For example, cluster~5 is associated with the highest pressure (1014~kPa), and as is seen in Figure 17.11 the shape of the line for cluster~5 is consistent with air-masses associated with a high pressure system (a clockwise-type sweep). group_by(kc1, cluster) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## # A tibble: 7 x 28 ## cluster co nox no2 no o3 so2 pm10 pm2.5 v10 v2.5 nv10 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 C1 0.346 89.9 51.4 25.3 32.1 3.12 38.3 31.3 8.79 8.02 29.5 ## 2 C2 0.205 40.9 30.9 6.55 39.4 1.46 17.2 11.6 4.18 3.13 13.1 ## 3 C3 0.198 47.2 32.3 9.78 39.7 1.84 18.1 11.3 3.73 2.80 14.3 ## 4 C4 0.189 44.1 30.6 8.89 39.7 1.56 17.8 11.0 3.41 2.17 14.4 ## 5 C5 0.202 57.0 39.2 11.7 41.4 2.31 24.3 16.5 5.06 4.50 19.2 ## 6 C6 0.267 64.9 42.4 14.8 46.3 2.93 36.6 29.7 7.78 7.57 28.8 ## 7 &lt;NA&gt; 0.225 53.8 36.2 11.6 39.3 2.06 23.7 16.3 5.20 4.14 18.5 ## # … with 16 more variables: nv2.5 &lt;dbl&gt;, ws &lt;dbl&gt;, wd &lt;dbl&gt;, air_temp &lt;dbl&gt;, ## # receptor &lt;dbl&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, day &lt;dbl&gt;, hour &lt;dbl&gt;, ## # hour.inc &lt;dbl&gt;, lat &lt;dbl&gt;, lon &lt;dbl&gt;, height &lt;dbl&gt;, pressure &lt;dbl&gt;, ## # traj_len &lt;dbl&gt;, len &lt;dbl&gt; Simple plots can be generated from these results too. For example, it is easy to consider the temporal nature of the volatile component of PM2.5 concentrations (v2.5 in the kc1 data frame). Figure 17.12 for example shows how the concentration of the volatile component of PM2.5 concentrations varies by cluster by plotting the hour of day-month variation. It is clear from Figure 17.12 that the clusters associated with the highest volatile PM2.5 concentrations are clusters 1 and 6 (European origin) and that these concentrations peak during spring. There is less data to see clearly what is going on with cluster~5. Nevertheless, the cluster analysis has clearly separated different air mass characteristics which allows for more refined analysis of different air-mass types. trendLevel(kc1, pollutant = &quot;v2.5&quot;, type = &quot;cluster&quot;, layout = c(6, 1), cols = &quot;increment&quot;) Figure 17.12: Some of the temporal characteristics of the volatile PM2.5 component plotted by month and hour of the day and by cluster for the London North Kensington site for 2011. Similarly, as considered in Figure 7.6, the timeVariation function can also be used to consider the temporal components. Another useful plot to consider is timeProp (see Section 11), which can show how the concentration of a pollutant is comprised. In this case it is useful to plot the time series of PM2.5 and show how much of the concentration is contributed to by each cluster. Such a plot is shown in Figure 17.13. It is now easy to see for example that during the spring months many of the high concentration events were due to clusters 1 and 6, which correspond to European origin air-masses as shown in Figure 17.11. timeProp(kc1, pollutant = &quot;pm2.5&quot;, avg.time = &quot;day&quot;, proportion = &quot;cluster&quot;, cols = &quot;Set2&quot;, key.position = &quot;top&quot;, key.columns = 6) Figure 17.13: Temporal variation in daily PM2.5 concentrations at the North Kensington site show by contribution of each cluster. References "],
["sec-conditionalQuantile.html", "Section18 Conditional quantiles 18.1 Background 18.2 Conditional evaluation", " Section18 Conditional quantiles 18.1 Background Conditional quantiles are a very useful way of considering model performance against observations for continuous measurements (Wilks 2005). The conditional quantile plot splits the data into evenly spaced bins. For each predicted value bin e.g. from 0 to 10 ppb the corresponding values of the observations are identified and the median, 25/75th and 10/90 percentile (quantile) calculated for that bin. The data are plotted to show how these values vary across all bins. For a time series of observations and predictions that agree precisely the median value of the predictions will equal that for the observations for each bin. The conditional quantile plot differs from the quantile-quantile plot (Q-Q plot) that is often used to compare observations and predictions. A Q-Q~plot separately considers the distributions of observations and predictions, whereas the conditional quantile uses the corresponding observations for a particular interval in the predictions. Take as an example two time series, the first a series of real observations and the second a lagged time series of the same observations representing the predictions. These two time series will have identical (or very nearly identical) distributions (e.g. same median, minimum and maximum). A Q-Q plot would show a straight line showing perfect agreement, whereas the conditional quantile will not. This is because in any interval of the predictions the corresponding observations now have different values. Plotting the data in this way shows how well predictions agree with observations and can help reveal many useful characteristics of how well model predictions agree with observations — across the full distribution of values. A single plot can therefore convey a considerable amount of information concerning model performance. The basic function is considerably enhanced by allowing flexible conditioning easily e.g. to evaluate model performance by season, day of the week and so on, as in other openair functions. To make things more interesting we will use data from a model evaluation exercise organised by Defra in 2010/2011. Many models were evaluated but we only consider hourly ozone predictions from the CMAQ model being used at King’s College London. First the data are loaded: load(&quot;~/My Drive/openair/Data/CMAQozone.RData&quot;) class(CMAQ.KCL$date) &lt;- c(&quot;POSIXct&quot;, &quot;POSIXt&quot;) head(CMAQ.KCL) ## site date o3 rollingO3Meas mod rollingO3Mod group ## 1 Aston.Hill 2006-01-01 00:00:00 NA NA 92.80 NA CMAQ.KCL ## 2 Aston.Hill 2006-01-01 01:00:00 74 NA 92.18 NA CMAQ.KCL ## 3 Aston.Hill 2006-01-01 02:00:00 72 NA 92.14 NA CMAQ.KCL ## 4 Aston.Hill 2006-01-01 03:00:00 72 NA 91.72 NA CMAQ.KCL ## 5 Aston.Hill 2006-01-01 04:00:00 70 NA 91.50 NA CMAQ.KCL ## 6 Aston.Hill 2006-01-01 05:00:00 66 NA 92.28 NA CMAQ.KCL The data consists of hourly observations of O3 in \\(\\mu\\)g m-3 at 15 rural O3 sites in the UK together with predicted values.7 First, we consider O3 predictions across all sites to help illustrate the purpose of the function. The results are shown in Figure ??. An explanation of the Figure is given in its caption. conditionalQuantile(CMAQ.KCL, obs = &quot;o3&quot;, mod = &quot;mod&quot;) Figure 18.1: Example of the use of conditional quantiles applied to the KCL CMAQ model for 15 rural O3 monitoring sites in 2006, for hourly data. The blue line shows the results for a perfect model. In this case the observations cover a range from 0 to 270 \\(\\mu\\)g m-3. The red line shows the median values of the predictions and corresponding observations. The maximum predicted value is 125 \\(\\mu\\)g m-3, somewhat less than the maximum observed value. The shading shows the predicted quantile intervals i.e. the 25/75th and the 10/90th. A perfect model would lie on the blue line and have a very narrow spread. There is still some spread because even for a perfect model a specific quantile interval will contain a range of values. However, for the number of bins used in this plot the spread will be very narrow. Finally, the histogram shows the counts of predicted values. A more informative analysis can be undertaken by considering conditional quantiles separately by site, which is easily done using the type option. The results are shown in Figure 18.2. It is now easier to see where the model performs best and how it varies by site type. For example, at a remote site in Scotland like Strath Vaich it is clear that the model does not capture either the lowest or highest O3 concentrations very well. conditionalQuantile(CMAQ.KCL, obs = &quot;o3&quot;, mod = &quot;mod&quot;, type = &quot;site&quot;) Figure 18.2: Conditional quantiles by site for 15 O3 monitoring sites in the UK. As with other openair functions, the ability to consider conditioning can really help with interpretation. For example, what do the conditional quantiles at Lullington Heath (in south-east England) look like by season? This is easily done by subsetting the data to select that site and setting type = \"season\", as shown in Figure 18.3. These results show that winter predictions have good coverage i.e. with width of the blue `perfect model’ line is the same as the observations. However, the predictions tend to be somewhat lower than observations for most concentrations (the median line is below the blue line) — and the width of the 10/75th and 10/90th percentiles is quite broad. However, the area where the model is less good is in summer and autumn because the predictions have low coverage (the red line only covers less than half of the observation line and the width of the percentiles is wide). Of course, it is also easy to plot by hour of the day, day of the week, by daylight/nighttime and so on — easily. All these approaches can help better understand why a model does not perform very well rather than just quantifying its performance. Also, these types of analysis are particularly useful when more than one model is involved in a comparison as in the recent Defra model evaluation exercise, which we will come back to later when some results are published. conditionalQuantile(subset(CMAQ.KCL, site == &quot;Lullington.Heath&quot;), obs = &quot;o3&quot;, mod = &quot;mod&quot;, type = &quot;season&quot;) Figure 18.3: Conditional quantiles at Lullington Heath conditioned by season. 18.2 Conditional evaluation There are numerous ways in which model performance can be assessed, including the use of common statistical measures described in Section ??. These approaches are very useful for comparing models against observations and other models. However, model developers would generally like to know why a model may have poor performance under some situations. This is a much more challenging issue to address. However, useful information can be gained by considering how other variables vary simultaneously. The conditionalEval function provides information on how other variables vary across the same intervals as shown on the conditional quantile plot. There are two types of variable that can be considered by setting the value of statistic. First, statistic can be another variable in the data frame. In this case the plot will show the different proportions of statistic across the range of predictions. For example statistic = \"season\" will show for each interval of mod the proportion of predictions that were spring, summer, autumn or winter. This is useful because if model performance is worse for example at high concentrations of mod then knowing that these tend to occur during a particular season etc. can be very helpful when trying to understand why a model fails. See Section 19.2 for more details on the types of variable that can be statistic. Another example would be statistic = \"ws\" (if wind speed were available in the data frame), which would then split wind speed into four quantiles and plot the proportions of each. Again, this would help show whether model performance in predicting concentrations of O3 for example is related to low to high wind speed conditions. conditionalEval can also simultaneously plot the model performance of other observed/predicted variable pairs according to different model evaluation statistics. These statistics derive from the Section ?? function and include MB, NMB, r, COE, MGE, NMGE, RMSE and FAC2. More than one statistic can be supplied e.g. statistic = c(\"NMB\", \"COE\"). Bootstrap samples are taken from the corresponding values of other variables to be plotted and their statistics with 95% confidence intervals calculated. In this case, the model performance of other variables is shown across the same intervals of mod, rather than just the values of single variables. In this second case the model would need to provide observed/predicted pairs of other variables. For example, a model may provide predictions of NOx and wind speed (for which there are also observations available). The conditionalEval function will show how well these other variables are predicted for the same prediction intervals of the main variable assessed in the conditional quantile plot e.g. ozone. In this case, values are supplied to var.obs (observed values for other variables) and var.mod (modelled values for other variables). For example, to consider how well the model predicts NOx and wind speed var.obs = c(\"nox.obs\", \"ws.obs\") and var.mod = c(\"nox.mod\", \"ws.mod\") would be supplied (assuming nox.obs, nox.mod, ws.obs, ws.mod are present in the data frame). The analysis could show for example, when ozone concentrations are under-predicted, the model may also be shown to over-predict concentrations of NOx at the same time, or under-predict wind speeds. Such information can thus help identify the underlying causes of poor model performance. For example, an under-prediction in wind speed could result in higher surface NOx concentrations and lower ozone concentrations. Similarly if wind speed predictions were good and NOx was over predicted it might suggest an over-estimate of NOx emissions. One or more additional variables can be plotted. A special case is statistic = \"cluster\". In this case a data frame is provided that contains the cluster calculated by trajCluster and importTraj. Alternatively users could supply their own pre-calculated clusters. These calculations can be very useful in showing whether certain back trajectory clusters are associated with poor (or good) model performance. Note that in the case of statistic = \"cluster\" there will be fewer data points used in the analysis compared with the ordinary statistics above because the trajectories are available for every three hours. Also note that statistic = \"cluster\" cannot be used together with the ordinary model evaluation statistics such as MB. The output will be a bar chart showing the proportion of each interval of mod by cluster number. Far more insight can be gained into model performance through conditioning using type. For example, type = \"season\" will plot conditional quantiles and the associated model performance statistics of other variables by each season. type can also be a factor or character field e.g. representing different models used. As an example, similar data to that described above from CMAQ have been used as an example. A subset of the data for the North Kensington site can be imported as shown below. condDat &lt;- readRDS(url(&quot;https://davidcarslaw.github.io/data/openair/condDat.rds&quot;)) The file contains observed and modelled hourly values for O3, NOx, wind speed, wind direction, temperature and relative humidity. head(condDat) ## date O3.obs NOx.obs ws.obs wd.obs temp.obs rh.obs O3.mod ## 5 2006-01-01 00:00:00 10 29.43665 4.6296 190 4.9 89 14.80 ## 10 2006-01-01 01:00:00 15 17.55393 NA 210 5.1 90 17.46 ## 15 2006-01-01 02:00:00 11 19.64817 2.5720 220 4.9 94 18.31 ## 20 2006-01-01 03:00:00 11 19.15393 3.6008 270 5.7 91 18.25 ## 25 2006-01-01 04:00:00 11 17.03037 3.0864 270 5.0 94 18.08 ## 30 2006-01-01 05:00:00 12 15.98325 3.6008 260 5.8 94 14.87 ## NOx.mod ws.mod wd.mod temp.mod rh.mod ## 5 24.00 2.78 224 3.85 93.16 ## 10 19.91 2.63 226 3.85 92.77 ## 15 18.25 2.52 236 2.85 99.40 ## 20 18.33 2.48 253 2.85 99.19 ## 25 18.09 2.24 275 3.85 97.49 ## 30 21.38 2.43 285 4.85 94.41 The conditionalEval function can be used straightforwardly to provide information on how predictions depend on another variable in general. In this case the option statistic can refer to another variable in the data frame to see how the quality of predictions depend on values of that variable. For example, in Figure 18.4 it can be seen how wind speed varies across the O3 prediction intervals. At low predicted concentrations of O3 there is a high proportion of low wind speed conditions (0 to 2.57 m s-1). When O3 is predicted to be around 40 ppb the wind speed tends to be higher — and finally at higher predicted concentrations of O3 the wind speed tends to decrease again. The important aspect of plotting data in this way is that it can directly relate the prediction performance to values of other variables, which should help develop a much better idea of the conditions that matter most. The user can therefore develop a good feel for the types of conditions where a model performs well or poorly and this might provide clues as to the underlying reasons for specific model behaviour. conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, statistic = &quot;ws.obs&quot;, col.var = &quot;Set3&quot;) Figure 18.4: Conditional quantiles at for O3 concentrations (left plot). On the right is a plot showing how the wind speed varies across the O3 prediction intervals. In an extension to Figure18.4 it is possible to derive information on the simultaneous model performance of other variables. Figure 18.5 shows the conditional quantile plot for hourly O3 predictions. This shows among other things that concentrations of O3 tend to be under-predicted for concentrations less than about 20 ppb. The Figure on the right shows the simultaneous model performance for wind speed and NOx for the same prediction intervals as shown in the conditional quantile plot. The plot on the right shows that for low concentrations of predicted O3 there is a tendency for NOx concentrations to be overestimated (NMB$\\(0.2 to 0.4) and wind speeds to be underestimated (NMB\\)-$0.2 to $-$0.3). One possible explanation for this behaviour is that the meteorological model tends to produce wind speeds that are too low, which would result in higher concentrations of NOx, which in turn would result in lower concentrations of O3. Note that it is possible to include more than one statistic, which would be plotted in a new panel e.g. statistic = c(\"NMB\", \"r\"). In essence the conditionalEval function provides more information on model performance that can help better diagnose potential problems. Clearly, there are many other ways in which the results can be analysed, which will depend on the data available. conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, var.obs = c(&quot;NOx.obs&quot;, &quot;ws.obs&quot;), var.mod = c(&quot;NOx.mod&quot;, &quot;ws.mod&quot;), statistic = &quot;NMB&quot;, var.names = c(&quot;nox&quot;, &quot;wind speed&quot;)) Figure 18.5: Conditional quantiles at for O3 concentrations (left plot). On the right is the model performance for wind speed and NOx predictions, which in this case is for the Normalised Mean Bias. A plot using temperature predictions shows that for most of the range in O3 predictions there is very little bias in temperature (although there is some negative bias in temperature for very low concentration O3 predictions): conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, var.obs = c(&quot;temp.obs&quot;, &quot;ws.obs&quot;), var.mod = c(&quot;temp.mod&quot;, &quot;ws.mod&quot;), statistic = &quot;NMB&quot;, var.names = c(&quot;temperature&quot;, &quot;wind speed&quot;)) Finally, (but not shown) it can be very useful to consider model performance in terms of air mass origin. In the example below, trajectories are imported, a cluster analysis undertaken and then evaluated using conditionalEval. ## import trajectories for 2006 traj &lt;- importTraj(&quot;london&quot;, 2006) ## carry out a cluster analysis cl &lt;- trajCluster(traj, method = &quot;Angle&quot;, n.cluster = 5) ## merge with orginal model eval data condDat &lt;- merge(condDat, cl, by = &quot;date&quot;) ## plot it conditionalEval(condDat, obs = &quot;O3.obs&quot;, mod = &quot;O3.mod&quot;, statistic = &quot;cluster&quot;, col.var = &quot;Set3&quot;) References "],
["sec-utility.html", "Section19 Utility functions 19.1 Selecting data by date 19.2 Making intervals — cutData 19.3 Selecting run lengths of values above a threshold — pollution episodes 19.4 Calculating rolling means 19.5 Aggregating data by different time intervals 19.6 Calculating percentiles 19.7 Correlation matrices", " Section19 Utility functions 19.1 Selecting data by date Selecting by date/time in R can be intimidating for new users—and time-consuming for all users. The selectByDate function aims to make this easier by allowing users to select data based on the British way of expressing date i.e. d/m/y. This function should be very useful in circumstances where it is necessary to select only part of a data frame. First load the packages we need. library(openair) library(tidyverse) ## select all of 1999 data.1999 &lt;- selectByDate(mydata, start = &quot;1/1/1999&quot;, end = &quot;31/12/1999&quot;) head(data.1999) ## # A tibble: 6 x 15 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1999-01-01 00:00:00 5.04 140 88 35 4 21 3.84 1.02 18 ## 2 1999-01-01 01:00:00 4.08 160 132 41 3 17 5.24 2.7 11 ## 3 1999-01-01 02:00:00 4.8 160 168 40 4 17 6.51 2.87 8 ## 4 1999-01-01 03:00:00 4.92 150 85 36 3 15 4.18 1.62 10 ## 5 1999-01-01 04:00:00 4.68 150 93 37 3 16 4.25 1.02 11 ## 6 1999-01-01 05:00:00 3.96 160 74 29 5 14 3.88 0.725 NA ## # … with 5 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt; tail(data.1999) ## # A tibble: 6 x 15 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1999-12-31 18:00:00 4.68 190 226 39 NA 29 5.46 2.38 23 ## 2 1999-12-31 19:00:00 3.96 180 202 37 NA 27 4.78 2.15 23 ## 3 1999-12-31 20:00:00 3.36 190 246 44 NA 30 5.88 2.45 23 ## 4 1999-12-31 21:00:00 3.72 220 231 35 NA 28 5.28 2.22 23 ## 5 1999-12-31 22:00:00 4.08 200 217 41 NA 31 4.79 2.17 26 ## 6 1999-12-31 23:00:00 3.24 200 181 37 NA 28 3.48 1.78 22 ## # … with 5 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt; ## easier way data.1999 &lt;- selectByDate(mydata, year = 1999) ## more complex use: select weekdays between the hours of 7 am to 7 pm sub.data &lt;- selectByDate(mydata, day = &quot;weekday&quot;, hour = 7:19) ## select weekends between the hours of 7 am to 7 pm in winter (Dec, Jan, Feb) sub.data &lt;- selectByDate(mydata, day = &quot;weekend&quot;, hour = 7:19, month = c(&quot;dec&quot;, &quot;jan&quot;, &quot;feb&quot;)) The function can be used directly in other functions. For example, to make a polar plot using year 2000 data: polarPlot(selectByDate(mydata, year = 2000), pollutant = &quot;so2&quot;) 19.2 Making intervals — cutData The cutData function is a utility function that is called by most other functions but is useful in its own right. Its main use is to partition data in many ways, many of which are built-in to openair Note that all the date-based types e.g. month/year are derived from a column date. If a user already has a column with a name of one of the date-based types it will not be used. For example, to cut data into seasons: mydata &lt;- cutData(mydata, type = &quot;season&quot;) head(mydata) ## # A tibble: 6 x 16 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1998-01-01 00:00:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 2 1998-01-01 01:00:00 2.16 230 NA NA NA 37 NA NA NA ## 3 1998-01-01 02:00:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 4 1998-01-01 03:00:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 5 1998-01-01 04:00:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 6 1998-01-01 05:00:00 3 190 264 42 0 16 5.50 3.05 NA ## # … with 6 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt;, season &lt;ord&gt; This adds a new field season that is split into four seasons. There is an option hemisphere that can be used to use southern hemisphere seasons when set as hemisphere = \"southern\". The type can also be another field in a data frame e.g. mydata &lt;- cutData(mydata, type = &quot;pm10&quot;) head(mydata) ## # A tibble: 6 x 16 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1998-01-01 00:00:00 0.6 280 285 39 1 pm10… 4.72 3.37 NA ## 2 1998-01-01 01:00:00 2.16 230 NA NA NA pm10… NA NA NA ## 3 1998-01-01 02:00:00 2.76 190 NA NA 3 pm10… 6.83 9.60 NA ## 4 1998-01-01 03:00:00 2.16 170 493 52 3 pm10… 7.66 10.2 NA ## 5 1998-01-01 04:00:00 2.4 180 468 78 2 pm10… 8.07 8.91 NA ## 6 1998-01-01 05:00:00 3 190 264 42 0 pm10… 5.50 3.05 NA ## # … with 6 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt;, season &lt;ord&gt; data(mydata) ## re-load mydata fresh This divides PM10 concentrations into four quantiles — roughly equal numbers of PM10 concentrations in four levels. Most of the time users do not have to call directly because most functions have a option that is used to call cutData directly e.g. polarPlot(mydata, pollutant = &quot;so2&quot;, type = &quot;season&quot;) However, it can be useful to call cutData before supplying the data to a function in a few cases. First, if one wants to set seasons to the southern hemisphere as above. Second, it is possible to override the division of a numeric variable into four quantiles by using the option n.levels. More details can be found in the help file. 19.3 Selecting run lengths of values above a threshold — pollution episodes A seemingly easy thing to do that has relevance to air pollution episodes is to select run lengths of contiguous values of a pollutant above a certain threshold. For example, one might be interested in selecting O3 concentrations where there are at least 8 consecutive hours above 90~ppb. In other words, a selection that combines both a threshold and persistence. These periods can be very important from a health perspective and it can be useful to study the conditions under which they occur. But how do you select such periods easily? The selectRunning utility function has been written to do this. It could be useful for all sorts of situations e.g. Selecting hours when primary pollutant concentrations are persistently high — and then applying other openair functions to analyse the data in more depth. In the study of particle suspension or deposition etc. it might be useful to select hours when wind speeds remain high or rainfall persists for several hours to see how these conditions affect particle concentrations. It could be useful in health impact studies to select blocks of data where pollutant concentrations remain above a certain threshold. As an example we are going to consider O3 concentrations at a semi-rural site in south-west London (Teddington). The data can be downloaded as follows: ted &lt;- importKCL(site = &quot;td0&quot;, year = 2005:2009, met = TRUE) ## NOTE - mass units are used ## ug/m3 for NOx, NO2, SO2, O3; mg/m3 for CO ## PM10_raw is raw data multiplied by 1.3 ## see how many rows there are nrow(ted) ## [1] 43824 We are going to contrast two polar plots of O3 concentration. The first uses all hours in the data set, and the second uses a subset of hours. The subset of hours is defined by O3 concentrations above 90~ppb for periods of at least 8-hours i.e. what might be considered as ozone episode conditions. episode &lt;- selectRunning(ted, pollutant = &quot;o3&quot;, threshold = 90, run.len = 8) ## see how many rows there are nrow(episode) ## [1] 1399 Now we are going to produce two bivariate polar plots shown in Figure 19.1. polarPlot(ted, pollutant = &quot;o3&quot;, min.bin = 2) polarPlot(episode, pollutant = &quot;o3&quot;, min.bin = 2) Figure 19.1: Example of using the selectRunning function to select episode hours to produce bivariate polar plots of O3 concentration. The results are shown in Figure 19.1. The polar plot for all data (left plot of Figure 19.1) shows that the highest O3 concentrations tend to occur for high wind speed conditions from almost every direction. Lower concentrations are observed for low wind speeds because concentrations of NOx are higher, resulting in O3 destruction. By contrast, a polar plot of the episode conditions (right plot of Figure 19.1) is very different. In this case there is a clear set of conditions where these criteria are met i.e. lengths of at least 8-hours where the O3 concentration is at least 90~ppb. It is clear the highest concentrations are dominated by south-easterly conditions i.e. corresponding to easterly flow from continental Europe where there has been time to the O3 chemistry to take place. Another interesting test plot is to consider NOx concentrations at Marylebone Road — see Figure 7.1, which shows that high concentrations are dominated by a swathe of south-westerly wind conditions (even for high wind speeds). However, if a selection is made of episode conditions (defined here as NOx concentrations &gt;500 ppb for at least 5-hours), then it can be seen that it is actually the low wind speed conditions that dominate. These conditions correspond to low in-canyon wind speeds and low wind speeds across London, which tend to elevate local and background NOx concentrations. Even though high concentrations of NOx are observed at high wind speeds, it does not seem that these conditions are as important for episode conditions. Users can run the code below to verify these observations. episode &lt;- selectRunning(mydata, pollutant = &quot;nox&quot;, threshold = 800, run.len = 5) polarPlot(episode, pollutant = &quot;nox&quot;, min.bin = 2) 19.4 Calculating rolling means Some air pollution statistics such as for O3 and particulate matter are expressed as rolling means and it is useful to be able to calculate these. It can also be useful to help smooth-out data for clearer plotting. The rollingMean function makes these calculations. One detail that can be important is that for some statistics a mean is only considered valid if there are a sufficient number of valid readings over the averaging period. Often there is a requirement for at least 75% data capture. For example, with an averaging period of 8 hours and a data capture threshold of 75%, at least 6 hours are required to calculate the mean. The function is called as follows; in this case to calculate 8-hour rolling mean concentrations of O3. mydata &lt;- rollingMean(mydata, pollutant = &quot;o3&quot;, hours = 8, new.name = &quot;rollingo3&quot;, data.thresh = 75) tail(mydata) ## # A tibble: 6 x 16 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2005-06-23 07:00:00 1.5 250 404 156 4 49 NA 1.81 28 ## 2 2005-06-23 08:00:00 1.5 260 388 145 6 48 NA 1.64 26 ## 3 2005-06-23 09:00:00 1.5 210 404 168 7 58 NA 1.29 34 ## 4 2005-06-23 10:00:00 2.6 240 387 175 10 55 NA 1.29 34 ## 5 2005-06-23 11:00:00 3.1 220 312 125 15 52 NA 1.29 33 ## 6 2005-06-23 12:00:00 3.1 220 287 119 17 55 NA 1.29 35 ## # … with 6 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, split.by &lt;ord&gt;, ## # feature &lt;chr&gt;, rollingo3 &lt;dbl&gt; Note that calculating rolling means shortens the length of the data set. In the case of O3, no calculations are made for the last 7 hours. Type ?rollingMean into R for more details. Note that the function currently only works with a single site. 19.5 Aggregating data by different time intervals Aggregating data by different averaging periods is a common and important task. There are many reasons for aggregating data in this way: Data sets may have different averaging periods and there is a need to combine them. For example, the task of combining an hourly air quality data set with a 15-minute average meteorological data set. The need here would be to aggregate the 15-minute data to 1-hour before merging. It is extremely useful to consider data with different averaging times straightforwardly. Plotting a very long time series of hourly or higher resolution data can hide the main features and it would be useful to apply a specific (but flexible) averaging period to the data for plotting. Those who make measurements during field campaigns (particularly for academic research) may have many instruments with a range of different time resolutions. It can be useful to re-calculate time series with a common averaging period; or maybe help reduce noise. It is useful to calculate statistics other than means when aggregating e.g. percentile values, maximums etc. For statistical analysis there can be short-term autocorrelation present. Being able to choose a longer averaging period is sometimes a useful strategy for minimising autocorrelation. In aggregating data in this way, there are a couple of other issues that can be useful to deal with at the same time. First, the calculation of proper vector-averaged wind direction is essential. Second, sometimes it is useful to set a minimum number of data points that must be present before the averaging is done. For example, in calculating monthly averages, it may be unwise to not account for data capture if some months only have a few valid points. When a data capture threshold is set through data.thresh it is necessary for timeAverage to know what the original time interval of the input time series is. The function will try and calculate this interval based on the most common time gap (and will print the assumed time gap to the screen). This works fine most of the time but there are occasions where it may not e.g. when very few data exist in a data frame. In this case the user can explicitly specify the interval through interval in the same format as avg.time e.g. interval = \"month\". It may also be useful to set start.date and end.date if the time series do not span the entire period of interest. For example, if a time series ended in October and annual means are required, setting end.date to the end of the year will ensure that the whole period is covered and that data.thresh is correctly calculated. The same also goes for a time series that starts later in the year where start.date should be set to the beginning of the year. All these issues are (hopefully) dealt with by the timeAverage function. The options are shown below, but as ever it is best to check the help that comes with the openair package. To calculate daily means from hourly (or higher resolution) data: daily &lt;- timeAverage(mydata, avg.time = &quot;day&quot;) daily ## # A tibble: 2,731 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998-01-01 00:00:00 6.84 188. 154. 39.4 6.87 18.2 3.15 2.70 NaN ## 2 1998-01-02 00:00:00 7.07 223. 132. 39.5 6.48 27.8 3.94 1.77 NaN ## 3 1998-01-03 00:00:00 11.0 226. 120. 38.0 8.41 20.2 3.20 1.74 NaN ## 4 1998-01-04 00:00:00 11.5 223. 105. 35.3 9.61 21.0 2.96 1.62 NaN ## 5 1998-01-05 00:00:00 6.61 237. 175. 46.0 4.96 24.2 4.52 2.13 NaN ## 6 1998-01-06 00:00:00 4.38 197. 214. 45.3 1.35 34.6 5.70 2.53 NaN ## 7 1998-01-07 00:00:00 7.61 219. 193. 44.9 4.42 31.0 5.67 2.48 NaN ## 8 1998-01-08 00:00:00 8.58 216. 161. 43.1 4.96 36 4.68 2.10 NaN ## 9 1998-01-09 00:00:00 6.7 206. 163. 38 3.62 38.0 5.13 2.36 NaN ## 10 1998-01-10 00:00:00 2.98 167. 219. 44.9 0.375 37.0 4.91 2.23 NaN ## # … with 2,721 more rows, and 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ## # ratio &lt;dbl&gt;, rollingo3 &lt;dbl&gt; Monthly 95th percentile values: monthly &lt;- timeAverage(mydata, avg.time = &quot;month&quot;, statistic = &quot;percentile&quot;, percentile = 95) monthly ## # A tibble: 90 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1998-01-01 00:00:00 11.2 45. 371. 68.6 14 53 11.1 3.99 NA ## 2 1998-02-01 00:00:00 8.16 16.7 524. 92 7 68.9 17.5 5.63 NA ## 3 1998-03-01 00:00:00 10.6 37.6 417. 85 15 61 18.4 4.85 NA ## 4 1998-04-01 00:00:00 8.16 44.4 384 81.5 20 52 14.6 4.17 NA ## 5 1998-05-01 00:00:00 7.56 40.6 300 80 25 61 12.7 3.55 40 ## 6 1998-06-01 00:00:00 8.47 50.7 377 74.2 15 53 12.2 4.28 33.9 ## 7 1998-07-01 00:00:00 9.22 36.7 386. 80.0 NA 52.4 13.9 4.52 32 ## 8 1998-08-01 00:00:00 7.92 48.4 337. 87.0 16 58.2 13.0 3.78 38 ## 9 1998-09-01 00:00:00 6 66.7 334. 81.3 14 64 18.2 4.25 47 ## 10 1998-10-01 00:00:00 12 33.9 439. 84 15.1 54 12.0 4.81 33 ## # … with 80 more rows, and 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, ## # rollingo3 &lt;dbl&gt; 2-week averages but only calculate if at least 75% of the data are available: twoweek &lt;- timeAverage(mydata, avg.time = &quot;2 week&quot;, data.thresh = 75) twoweek ## # A tibble: 196 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1997-12-29 00:00:00 6.98 212. 167. 41.4 4.63 29.3 4.47 2.17 NA ## 2 1998-01-12 00:00:00 4.91 221. 173. 42.1 4.70 28.8 5.07 1.86 NA ## 3 1998-01-26 00:00:00 2.78 242. 233. 51.4 2.30 34.9 8.07 2.45 NA ## 4 1998-02-09 00:00:00 4.43 215. 276. 57.1 2.63 43.7 8.98 2.94 NA ## 5 1998-02-23 00:00:00 6.89 237. 248. 56.7 4.99 28.8 9.79 2.57 NA ## 6 1998-03-09 00:00:00 2.97 288. 160. 44.8 5.64 32.7 8.65 1.62 NA ## 7 1998-03-23 00:00:00 4.87 192. 224. 53.6 5.52 35.9 10.2 2.34 NA ## 8 1998-04-06 00:00:00 3.24 294. 144. 43.4 10.1 23.8 5.48 1.40 NA ## 9 1998-04-20 00:00:00 4.38 195. 177. 47.6 10.5 31.4 5.54 1.73 NA ## 10 1998-05-04 00:00:00 3.97 285. 134. 45.5 10.2 38.6 5.49 1.41 24.6 ## # … with 186 more rows, and 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ## # ratio &lt;dbl&gt;, rollingo3 &lt;dbl&gt; Note that timeAverage has a type option to allow for the splitting of variables by a grouping variable. The most common use for type is when data are available for different sites and the averaging needs to be done on a per site basis. First, retaining by site averages: # import some data for two sites dat &lt;- importAURN(c(&quot;kc1&quot;, &quot;my1&quot;), year = 2011:2013) # annual averages by site timeAverage(dat, avg.time = &quot;year&quot;, type = &quot;site&quot;) ## # A tibble: 6 x 17 ## # Groups: site [2] ## site date co nox no2 no o3 so2 pm10 pm2.5 ## &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lond… 2011-01-01 00:00:00 0.656 306. 97.2 137. 18.5 6.86 38.4 24.5 ## 2 Lond… 2012-01-01 00:00:00 0.589 313. 94.0 143. 15.0 8.13 30.8 21.5 ## 3 Lond… 2013-01-01 00:00:00 0.506 281. 84.7 128. 17.7 5.98 29.1 20.1 ## 4 Lond… 2011-01-01 00:00:00 0.225 53.8 36.1 11.6 39.4 2.06 23.7 16.3 ## 5 Lond… 2012-01-01 00:00:00 0.266 57.4 36.7 13.3 38.5 2.03 20.2 14.6 ## 6 Lond… 2013-01-01 00:00:00 0.250 57.9 36.9 13.7 38.4 2.01 23.1 14.7 ## # … with 7 more variables: v10 &lt;dbl&gt;, v2.5 &lt;dbl&gt;, nv10 &lt;dbl&gt;, nv2.5 &lt;dbl&gt;, ## # ws &lt;dbl&gt;, wd &lt;dbl&gt;, air_temp &lt;dbl&gt; Retain site name and site code: # can also retain site code timeAverage(dat, avg.time = &quot;year&quot;, type = c(&quot;site&quot;, &quot;code&quot;)) ## # A tibble: 6 x 18 ## # Groups: site, code [2] ## site code date co nox no2 no o3 so2 pm10 ## &lt;fct&gt; &lt;fct&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lond… MY1 2011-01-01 00:00:00 0.656 306. 97.2 137. 18.5 6.86 38.4 ## 2 Lond… MY1 2012-01-01 00:00:00 0.589 313. 94.0 143. 15.0 8.13 30.8 ## 3 Lond… MY1 2013-01-01 00:00:00 0.506 281. 84.7 128. 17.7 5.98 29.1 ## 4 Lond… KC1 2011-01-01 00:00:00 0.225 53.8 36.1 11.6 39.4 2.06 23.7 ## 5 Lond… KC1 2012-01-01 00:00:00 0.266 57.4 36.7 13.3 38.5 2.03 20.2 ## 6 Lond… KC1 2013-01-01 00:00:00 0.250 57.9 36.9 13.7 38.4 2.01 23.1 ## # … with 8 more variables: pm2.5 &lt;dbl&gt;, v10 &lt;dbl&gt;, v2.5 &lt;dbl&gt;, nv10 &lt;dbl&gt;, ## # nv2.5 &lt;dbl&gt;, ws &lt;dbl&gt;, wd &lt;dbl&gt;, air_temp &lt;dbl&gt; Average all data across sites (drops site and code): timeAverage(dat, avg.time = &quot;year&quot;) ## # A tibble: 3 x 16 ## date co nox no2 no o3 so2 pm10 pm2.5 v10 ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2011-01-01 00:00:00 0.439 181. 67.1 74.9 31.5 4.31 31.4 20.5 5.40 ## 2 2012-01-01 00:00:00 0.424 182. 64.7 76.7 26.9 5.08 25.6 18.1 4.23 ## 3 2013-01-01 00:00:00 0.378 169. 60.7 70.7 28.0 3.79 26.8 17.4 4.29 ## # … with 6 more variables: v2.5 &lt;dbl&gt;, nv10 &lt;dbl&gt;, nv2.5 &lt;dbl&gt;, ws &lt;dbl&gt;, ## # wd &lt;dbl&gt;, air_temp &lt;dbl&gt; timeAverage also works the other way in that it can be used to derive higher temporal resolution data e.g. hourly from daily data or 15-minute from hourly data. An example of usage would be the combining of daily mean particle data with hourly meteorological data. There are two ways these two data sets can be combined: either average the meteorological data to daily means or calculate hourly means from the particle data. The timeAverage function when used to ‘expand’ data in this way will repeat the original values the number of times required to fill the new time scale. In the example below we calculate 15-minute data from hourly data. As it can be seen, the first line is repeated four times and so on. data15 &lt;- timeAverage(mydata, avg.time = &quot;15 min&quot;, fill = TRUE) head(data15, 20) ## # A tibble: 20 x 14 ## date ws wd nox no2 o3 pm10 so2 co pm25 ## &lt;dttm&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1998-01-01 00:00:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 2 1998-01-01 00:15:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 3 1998-01-01 00:30:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 4 1998-01-01 00:45:00 0.6 280 285 39 1 29 4.72 3.37 NA ## 5 1998-01-01 01:00:00 2.16 230 NA NA NA 37 NA NA NA ## 6 1998-01-01 01:15:00 2.16 230 NA NA NA 37 NA NA NA ## 7 1998-01-01 01:30:00 2.16 230 NA NA NA 37 NA NA NA ## 8 1998-01-01 01:45:00 2.16 230 NA NA NA 37 NA NA NA ## 9 1998-01-01 02:00:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 10 1998-01-01 02:15:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 11 1998-01-01 02:30:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 12 1998-01-01 02:45:00 2.76 190 NA NA 3 34 6.83 9.60 NA ## 13 1998-01-01 03:00:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 14 1998-01-01 03:15:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 15 1998-01-01 03:30:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 16 1998-01-01 03:45:00 2.16 170 493 52 3 35 7.66 10.2 NA ## 17 1998-01-01 04:00:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 18 1998-01-01 04:15:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 19 1998-01-01 04:30:00 2.4 180 468 78 2 34 8.07 8.91 NA ## 20 1998-01-01 04:45:00 2.4 180 468 78 2 34 8.07 8.91 NA ## # … with 4 more variables: ws2 &lt;dbl&gt;, wd2 &lt;dbl&gt;, ratio &lt;dbl&gt;, rollingo3 &lt;dbl&gt; The timePlot can apply this function directly to make it very easy to plot data with different averaging times and statistics. 19.6 Calculating percentiles calcPercentile makes it straightforward to calculate percentiles for a single pollutant. It can take account of different averaging periods, data capture thresholds — see Section 19.5 for more details. For example, to calculate the 25, 50, 75 and 95th percentiles of O3 concentration by year: calcPercentile(mydata, pollutant = &quot;o3&quot;, percentile = c(25, 50, 75, 95), avg.time = &quot;year&quot;) ## date percentile.25 percentile.50 percentile.75 percentile.95 ## 1 1998-01-01 2 4 7 16 ## 2 1999-01-01 2 4 9 21 ## 3 2000-01-01 2 4 9 22 ## 4 2001-01-01 2 4 10 24 ## 5 2002-01-01 2 4 10 24 ## 6 2003-01-01 2 4 11 24 ## 7 2004-01-01 2 5 11 23 ## 8 2005-01-01 3 7 16 28 19.7 Correlation matrices Understanding how different variables are related to one another is always important. However, it can be difficult to easily develop an understanding of the relationships when many different variables are present. One of the useful techniques used is to plot a correlation matrix, which provides the correlation between all pairs of data. The basic idea of a correlation matrix has been extended to help visualise relationships between variables by Friendly (2002) and Sarkar (2007). The corPlot shows the correlation coded in three ways: by shape (ellipses), colour and the numeric value. The ellipses can be thought of as visual representations of scatter plot. With a perfect positive correlation a line at 45 degrees positive slope is drawn. For zero correlation the shape becomes a circle — imagine a ‘fuzz’ of points with no relationship between them. With many variables it can be difficult to see relationships between variables i.e. which variables tend to behave most like one another. For this reason hierarchical clustering is applied to the correlation matrices to group variables that are most similar to one another (if cluster = TRUE.) It is also possible to use the openair type option to condition the data in many flexible ways, although this may become difficult to visualise with too many panels. An example of the corPlot function is shown in Figure 19.2. In this Figure it can be seen the highest correlation coefficient is between PM10 and PM2.5 (r = 0. 84) and that the correlations between SO2, NO2 and NOx are also high. O3 has a negative correlation with most pollutants, which is expected due to the reaction between NO and O3. It is not that apparent in Figure 19.2 that the order the variables appear is due to their similarity with one another, through hierarchical cluster analysis. In this case we have chosen to also plot a dendrogram that appears on the right of the plot. Dendrograms provide additional information to help with visualising how groups of variables are related to one another. Note that dendrograms can only be plotted for type = \"default\" i.e. for a single panel plot. corPlot(mydata, dendrogram = TRUE) Figure 19.2: Example of a correlation matrix showing the relationships between variables. Note also that the corPlot accepts a type option, so it possible to condition the data in many flexible ways, although this may become difficult to visualise with too many panels. For example: corPlot(mydata, type = &quot;season&quot;) When there are a very large number of variables present, the corPlot is a very effective way of quickly gaining an idea of how variables are related. As an example (not plotted) it is useful to consider the hydrocarbons measured at Marylebone Road. There is a lot of information in the hydrocarbon plot (about 40 species), but due to the hierarchical clustering it is possible to see that isoprene, ethane and propane behave differently to most of the other hydrocarbons. This is because they have different (non-vehicle exhaust) origins. Ethane and propane results from natural gas leakage whereas isoprene is biogenic in origin (although some is from vehicle exhaust too). It is also worth considering how the relationships change between the species over the years as hydrocarbon emissions are increasingly controlled, or maybe the difference between summer and winter blends of fuels and so on. hc &lt;- importAURN(site = &quot;my1&quot;, year = 2005, hc = TRUE) ## now it is possible to see the hydrocarbons that behave most ## similarly to one another corPlot(hc) References "],
["references.html", "References", " References Applequist, Scott. 2012. “Wind Rose Bias Correction.” Journal of Applied Meteorology and Climatology 51 (7): 1305–9. Ara Begum, Bilkis, Eugene Kim, Cheol-Heon Jeong, Doh-Won Lee, and Philip K. Hopke. 2005. “Evaluation of the potential source contribution function using the 2002 Quebec forest fire episode.” Atmospheric Environment 39 (20): 3719–24. https://doi.org/10.1016/j.atmosenv.2005.03.008. Ashbaugh, Lowell L., William C. Malm, and Willy Z. Sadeh. 1985. “A residence time probability analysis of sulfur concentrations at grand Canyon National Park.” Atmospheric Environment (1967) 19 (8): 1263–70. https://doi.org/10.1016/0004-6981(85)90256-2. Carslaw, David. 2020. Worldmet: Import Surface Meteorological Data from Noaa Integrated Surface Database (Isd). http://github.com/davidcarslaw/worldmet. Carslaw, D. C., and S. D. Beevers. 2013. “Characterising and Understanding Emission Sources Using Bivariate Polar Plots and K-Means Clustering.” Environmental Modelling &amp; Software 40 (0): 325–29. https://doi.org/10.1016/j.envsoft.2012.09.005. Carslaw, D. C., S. D. Beevers, K. Ropkins, and M. C. Bell. 2006. “Detecting and Quantifying Aircraft and Other on-Airport Contributions to Ambient Nitrogen Oxides in the Vicinity of a Large International Airport.” Atmospheric Environment 40 (28): 5424–34. Carslaw, D. C., S. D. Beevers, and J. E. Tate. 2007. “Modelling and Assessing Trends in Traffic-Related Emissions Using a Generalised Additive Modelling Approach.” Atmospheric Environment 41 (26): 5289–99. COMEAP. 2011. “Review of the Uk Air Quality Index: A Report by the Committee on the Medical Effects of Air Pollutants.” http://comeap.org.uk/documents/reports/130-review-of-the-uk-air-quality-index.html. Droppo, James G, and Bruce A Napier. 2008. “Wind Direction Bias in Generating Wind Roses and Conducting Sector-Based Air Dispersion Modeling.” Journal of the Air &amp; Waste Management Association 58 (7): 913–18. Fleming, Z. L., P. S. Monks, and A. J. Manning. 2012. “Review: Untangling the influence of air-mass history in interpreting observed atmospheric composition.” Atmospheric Research 104-105: 1–39. https://doi.org/10.1016/j.atmosres.2011.09.009. Friendly, M. 2002. “Corrgrams: Exploratory Displays for Correlation Matrices.” The American Statistician 56 (4): 316–25. Grange, Stuart K, Alastair C Lewis, and David C Carslaw. 2016. “Source Apportionment Advances Using Polar Plots of Bivariate Correlation and Regression Statistics.” Atmospheric Environment 145: 128–34. Hastie, T. J., and R. J. Tibshirani. 1990. Generalized Additive Models. London: Chapman; Hall. Henry, Ronald, Gary A. Norris, Ram Vedantham, and Jay R. Turner. 2009. “Source Region Identification Using Kernel Smoothing.” Article. Environmental Science &amp; Technology 43 (11): 4090–7. https://doi.org/{10.1021/es8011723}. Hirsch, R. M., J. R. Slack, and R. A. Smith. 1982. “Techniques of Trend Analysis for Monthly Water-Quality Data.” Water Resources Research 18 (1): 107–21. Hsu, Ying-Kuang, Thomas M. Holsen, and Philip K. Hopke. 2003. “Comparison of hybrid receptor models to locate PCB sources in Chicago.” Atmospheric Environment 37 (4): 545–62. https://doi.org/10.1016/S1352-2310(02)00886-5. Kunsch, H. R. 1989. “The Jackknife and the Bootstrap for General Stationary Observations.” Annals of Statistics 17 (3): 1217–41. Lupu, Alexandru, and Willy Maenhaut. 2002. “Application and comparison of two statistical trajectory techniques for identification of source regions of atmospheric aerosol species.” Atmospheric Environment 36: 5607–18. Pekney, Natalie J., Cliff I. Davidson, Liming Zhou, and Philip K. Hopke. 2006. “Application of PSCF and CPF to PMF-Modeled Sources of PM 2.5 in Pittsburgh.” Aerosol Science and Technology 40 (10): 952–61. https://doi.org/10.1080/02786820500543324. Sarkar, Deepayan. 2007. Lattice Multivariate Data Visualization with R. New York: Springer. Seibert, P, H Kromp-Kolb, U Baltensperger, and DT Jost. 1994. “Trajectory Analysis of High-Alpine Air Pollution Data.” NATO Challenges of Modern Society 18: 595–95. Sen, P. K. 1968. “Estimates of Regression Coefficient Based on Kendall’s Tau.” Journal of the American Statistical Association 63(324): 1379–89. Theil, H. 1950. “A Rank Invariant Method of Linear and Polynomial Regression Analysis, I, Ii, Iii.” Proceedings of the Koninklijke Nederlandse Akademie Wetenschappen, Series A – Mathematical Sciences 53: 386–92, 521–25, 1397–1412. Uria-Tellaetxe, I, and D. C. Carslaw. 2014. “Conditional Bivariate Probability Function for Source Identification.” Environmental Modelling &amp; Software 59: 1–9. https://doi.org/10.1016/j.envsoft.2014.05.002. Westmoreland, E. J., N Carslaw, D. C. Carslaw, A. Gillah, and E. Bates. 2007. “Analysis of Air Quality Within a Street Canyon Using Statistical and Dispersion Modelling Techniques.” Atmospheric Environment 41 (39): 9195–9205. Wilcox, Rand R. 2010. Fundamentals of Modern Statistical Methods: Substantially Improving Power and Accuracy. 2nd ed. Springer New York. http://www.springerlink.com/content/978-1-4419-5524-1. Wilks, Daniel S. 2005. Statistical Methods in the Atmospheric Sciences, Volume 91, Second Edition (International Geophysics). 2nd ed. Hardcover; Academic Press. Wood, S. N. 2006. Generalized Additive Models: An Introduction with R. Chapman; Hall/CRC. Yu, K. N., Y. P. Cheung, T. Cheung, and R. C. Henry. 2004. “Identifying the Impact of Large Urban Airports on Local Air Quality by Nonparametric Regression.” Atmospheric Environment 38 (27): 4501–7. "]
]
